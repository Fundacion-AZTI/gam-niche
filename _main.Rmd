--- 
title: "Species distribution models (SDM)"
author: "AZTI"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography:
- references.bib
description: |
  This is a short tutorial about constructing species distribution models in R using shape-constrained generalized additive models.  
link-citations: yes
github-repo: Fundacion-AZTI/SDM
---

# About {-}

This is a short tutorial for constructing species distribution models in R using shape-constrained generalized additive models. 

The code is available in [AZTI's github repository](https://github.com/Fundacion-AZTI/SDM) repository and the book is readily available [here](https://fundacion-azti.github.io/SDM/).

To cite this book, please use: 

BLA BLA BLA

<!--chapter:end:index.Rmd-->

# Introduction

Species distribution models (SDMs) are numerical tools that combine observations of species occurrence or abundance at known locations with information on the environmental and/or spatial characteristics of those locations [@elith_etal_2009]. They are also known as ecological niche models (ENM) or habitat suitability models or... 

A wide variety of methods have been used ... 

Reviews of SDM literature include ... 

One of the common problems is that, the fitted models do not agree with the ecological niche theory...  

This book provides a tutorial on how to use shape-constrained generalized additive models to build SDMs. It is organised following the key steps in good modeling practice of SDMs [@elith_etal_2009]. First, presence data of a selected species are downloaded from GBIF/OBIS datasets and pseudo-absence data are created. Then, environmental data are downloaded from public repositories and extracted at each of the presence/pseudo-absence data points. Based on this dataset, an exploratory analysis is conducted to help deciding on the best modelling approach. The model is fitted to the dataset and the quality of the fit and the realism of the fitted response function are evaluated. After selecting a threshold to transform the continuous probability predictions into binary responses, the model is validated using a k-fold approach. Finally, the predicted maps are generated for visualization.      


<!--chapter:end:01-intro.Rmd-->

# Libraries

Load libraries that will be used

```{r}
library("scam")
```

Note that all the libraries must be installed. If some library is not installed, run: 

```{r}
install.packages("scam")
```

<!--chapter:end:02-libraries.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Presence-absence data

Bla bla bla

## Download presence data

Download from GBIF OBIS. Mireia

```{r, eval=F}
# Script information ------------------------------------------------------

# Title: Download OBIS/GBIF occurrence data used in H2020 Mission Atlantic (No 862428) Project Task 3.4.
# Last modified by Mireia Valle (github profile: MireiaValle, email: mvalle@azti.es) based on original code for sourcing OBIS and GBIF from Guillem Chust (email: gchust@azti.es) and some adaptations from Eduardo Ramirez. 

# Load libraries 
---------------------------------
  
# Specific libraries to get the occurrence data
#install.packages("robis") # https://cran.r-project.org/web/packages/robis/robis.pdf
library(robis)
library (rgbif)

# Libraries for Spatial data 
library(rgdal)
library(sf) # shapes

# Library for plotting
library(ggplot2)

# Libraries for reading data
library(data.table)
library(dplyr)
library(tidyr)

# outliers
library(CoordinateCleaner)

# Library for reproducible workflow
library(here)


# STUDY AREA
---------------------------------

##NOTA interna: En el código de MISSION uso el shapefile de FAO que lo tengo descargado en el ordenador, hay que añadir la info para la descarga manual o el código para la descarga con R. Para MISSION elimino el Black Sea del área de estudio. 
  
# el enlace para la descarga del shapefile es el siguiente: 
 
# http://www.fao.org/fishery/geoserver/fifao/ows?service=WFS&request=GetFeature&version=1.0.0&typeName=fifao:FAO_AREAS_CWP&outputFormat=SHAPE-ZIP")

# Load FAO (spatial multipolygon)
FAO<- readOGR(here::here("data", "spatial", "FAO_AREAS.shp"))

#Selecting Atlantic FAO regions
FAO_Atl <- FAO[FAO$OCEAN=="Atlantic",]

#Plot Atlantic FAO regions
plot(FAO_Atl)

#remove Black Sea subarea

Black_Sea <- FAO_Atl[FAO_Atl$ID=="20",]

plot(Black_Sea)

## Find the 'difference', i.e. reverse of st_intersection

# transform to sf object
Black_Sea.sf <- st_as_sf(Black_Sea)

FAO_Atl.sf <- st_as_sf(FAO_Atl)

# remove the black see
FAO_Atl_no_black_sea <- st_difference(FAO_Atl.sf,Black_Sea.sf) %>%   dplyr::select (F_AREA)

#transform to spatial polygons dataframe
FAO_Atl_no_black_sea <- sf:::as_Spatial(FAO_Atl_no_black_sea)

plot(FAO_Atl_no_black_sea)

# DOWNLOAD DATA FROM OBIS AND GBIF DATASETS AND MERGE 
---------------------------------  
  
# en la reunión decidimos hacer pruebas con: Thunnus alalunga, Thunnus obesus, Xiphias gladius y Thunnus albacares
  
## Find data by scientific name in the datasets OBIS/GBIF
  
  # Get data from OBIS
  mydata.obis<-robis::occurrence(scientificname="Thunnus alalunga")

  # Get data from GBIF
  mydata.gbif<-occ_data(scientificName="Thunnus alalunga", hasCoordinate = TRUE, limit=100000)$data
  
  # Select columns of interest from downloaded data
  
  #check names in for obis data
  names(mydata.obis)
  
  #select columns of interest
  mydata.obis <-  mydata.obis %>%
                  dplyr::select("scientificName",
                   "decimalLongitude",
                   "decimalLatitude",
                   "date_year",
                   "month",
                   "day",
                   "eventDate",
                   "depth",
                   "bathymetry",
                   "occurrenceStatus",
                   "sst")
  
  # check names for gbif data
  names(mydata.gbif)
  
  #select columns of interest
  mydata.gbif <- mydata.gbif %>%
                  dplyr::select("acceptedScientificName",
                   "decimalLongitude",
                   "decimalLatitude",
                   "year",
                   "month",
                   "day",
                   "eventDate",
                   "depth")
  
  ## Add new field and rename some columns from mydata.gbif dataframe in order to have the same columns and be able to join both tables
  
  mydata.gbif <- mydata.gbif %>% 
    dplyr::rename(scientificName= "acceptedScientificName") %>% 
    dplyr::rename(date_year = "year") %>% 
    dplyr::mutate(bathymetry= NA) %>% 
    dplyr::mutate(occurrenceStatus=1) %>% 
    dplyr::mutate(sst= NA)

  ## Join data from OBIS and GBIF 
  mydata.fus<-rbind(mydata.obis,mydata.gbif)
  
  ## assign unique scientific name 
  mydata.fus <- mydata.fus %>% 
    dplyr::mutate(scientificName= paste(mydata.obis$scientificName[1]))
  
# CLEAN RAW DATA 
---------------------------------    
  
  #### give date format to eventDate and fill out month and date_year columns
  mydata.fus$eventDate <- as.Date(mydata.fus$eventDate)
  mydata.fus$date_year <- as.numeric(mydata.fus$date_year)
  mydata.fus$month <- as.numeric(mydata.fus$month)

  ### mutate occurrenceStatus column giving value of 1 to presences and 0 to absences
  mydata.fus <- mydata.fus %>% 
    mutate(occurrenceStatus = ifelse(occurrenceStatus== "NA", NA, occurrenceStatus)) %>%
    mutate(occurrenceStatus = ifelse(occurrenceStatus== "Present", 1, occurrenceStatus)) %>% 
    mutate(occurrenceStatus = ifelse(occurrenceStatus== "present", 1, occurrenceStatus)) %>% 
    mutate(occurrenceStatus = ifelse(occurrenceStatus== "Presente", 1, occurrenceStatus)) %>% 
    mutate(occurrenceStatus = ifelse(occurrenceStatus== "Presence", 1, occurrenceStatus)) %>% 
    mutate(occurrenceStatus = ifelse(occurrenceStatus== "P", 1, occurrenceStatus)) %>% 
    mutate(occurrenceStatus = ifelse(occurrenceStatus== "Q", 1, occurrenceStatus))

  ### Assign 1 value to all retrieved points 
  
  #NOTA INTERNA: replace_na me da error por eso asigno directamente 1 a todos los puntos
  mydata.fus <- mydata.fus  %>% 
    dplyr::mutate(occurrenceStatus = 1)

# REMOVING OUTLIERS
---------------------------------  

  # find outliers based on distance method
  out.dist <- cc_outl(x=mydata.fus,
                lon = "decimalLongitude", lat = "decimalLatitude",
                species = "scientificName",
                method="distance", tdi=1000, # distance method with tdi=1000km
                thinning=T, thinning_res=0.5,
                value="flagged") 

  # remove outliers from the data
  
  mydata.fus <- mydata.fus[out.dist, ]
  
# REMOVE DUPLICATES
---------------------------------  
date <- (cbind(mydata.fus$decimalLongitude,mydata.fus$decimalLatitude,mydata.fus$eventDate))

mydata.fus<-mydata.fus[!duplicated(date),]

# PREPARE DATA TO USE FAO ATLANTIC REGION MASK
---------------------------------    
    
  # Prepare coordinate format and projection to be able to use FAO zone masks
  dat <- data.frame(cbind(mydata.fus$decimalLongitude,mydata.fus$decimalLatitude))
  ptos<-as.data.table(dat,keep.columnnames=TRUE)
  
  coordinates(ptos) <- ~ X1 + X2
  
  proj4string(ptos) <-proj4string(FAO)
  
  ## Select only occurrences from FAO Atlantic
  match2<-data.frame(subset(mydata.fus,!is.na(over(ptos, FAO_Atl_no_black_sea)[,1])))
  
  ## Extract the FAO area of each point
  match3<-data.frame(subset(over(ptos, FAO_Atl_no_black_sea), !is.na(over(ptos, FAO_Atl_no_black_sea)[,1])))
  
  data_fus_Atl<-cbind(F_AREA=match3$F_AREA,match2)

# SAVE OCCURRENCE DATA
---------------------------------    

#NOTA INTERNA: quiero guardar el RData en la carpeta data/occurrences con el nombre de la especies sin espacios para no machar los RData en caso de que hagamos más de una especie pero no lo consigo. 

save(data_fus_Atl, file = here::here("data/occurrences/occ.RData"))

  
# PLOT OCCURRENCES MAP
---------------------------------   
ggplot() +
   geom_path(data = FAO_Atl_no_black_sea, 
             aes(x = long, y = lat, group = group),
             color = 'gray', size = .2) +
   geom_point(data=data_fus_Atl, aes(x=decimalLongitude, y=decimalLatitude,colour= occurrenceStatus)) 
  
```


## Create pseudo-absence data

Prevalence 50%

See code from ANICHO (mantaining some space around presences). Leire C.

Ref [@barbetmassin_etal_2012]

Copio aqui el codigo de anicho tal cual, luego lo limpiaré para este caso:

```{r, eval=F}
# Script information ------------------------------------------------------

# Title: Generate pseudo-absences for IM-18-ANICHO
# Last modified by Leire Ibaibarriaga (libaibarriaga@azti.es) and Leire Citores (lcitores@azti.es)

# Load libraries ----------------------------------------------------------

library(tidyverse)
library(ggplot2)
library(scales)
library(here)
library(ggridges)

library(maps)        # some basic country maps
library(mapdata)     # higher resolution maps
library(mapproj)
library(marmap)      # access global topography data
library(mapplots)    # ices rectangles
library(sf)
library(gridExtra)
library(lubridate)

# general settings for ggplot (black-white background, larger base_size)

theme_set(theme_bw(base_size = 16))

# Set directories ---------------------------------------------------------

# final data set created by Nerea Goikoetxea after combining logboook and VMS data are in:
# \\dok\nas\K\AZTIMAR\PROYECTOS\Funcionamiento de los ecosistemas marinos\IM-18-ANICHO\Cruce logbooks vs VMS\Data\df_2010_2019_DEF.csv
# note that these data do not have yet the environmental variables. We have to repeat the process including the pseudo-absences 
# because some dates were wrong in the previous file (anicho_landings_final_FECHASMAL.csv)

data.dir <- here("data")

# Data frame with environmental variables ---------------------------------

# read data-file

df <- read.csv(file.path(data.dir,"df_2010_2019_DEF.csv"), header=T, sep=";", dec=",")

dim(df) # 26336 rows and 11 columns
head(df)
tail(df)
summary(df)

# change the names

names(df) <- c("CODEUEBUQUE","FECHA_CAPT","DOY","WEEK","MONTH","YEAR","LAND","OK","PESO_ANE","LAT","LON")
  
# format

df$FECHA_CAPT <- as.Date(df$FECHA_CAPT, format="%Y-%m-%d")

# check duplicates in all the columns

dupli <- duplicated(df)
table(dupli) # there are 13 duplicated rows!!
df[dupli, ] # this shows only the rows that are duplicated (ie the second/third/... time they appear)

dupli <- duplicated(df) | duplicated(df, fromLast=T) # to see the duplicates and the first time they appear
table(dupli) # 26, ie each duplicated row appears twice 
df[dupli, ]

# LIC: tras hablar con Lucia parece que aunque parezcan replicados, son datos oficiales de SGPM y
# tendrian que ser correctos. Asi que los agrupo y sumo la variable PESO

df <- df %>% 
  group_by_at(vars(-PESO_ANE)) %>% 
  summarise(PESO_ANE=sum(PESO_ANE)) 
dim(df) # 26085 observations

# select only points from March to July

df <- subset(df, MONTH %in% c(3:7)) # 25036 observations
dim(df) # 25036 observations

# Depth, ICES statistical rectangles etc ----------------------------------


# read shapefile with ices divisions

ices.areas.shp <- st_read("C:/use/proyectos/IM-18-ANICHO/datos/ICES_shapefiles/ICES_areas")
st_crs(ices.areas.shp)
wgs<-"+proj=longlat +datum=WGS84 +ellps=WGS84"
ices.areas.shp <- st_transform(ices.areas.shp, wgs) 

ices.rect.shp <- st_read("C:/use/proyectos/IM-18-ANICHO/datos/ICES_shapefiles/ICES_rectangles")
st_crs(ices.rect.shp)
wgs<-"+proj=longlat +datum=WGS84 +ellps=WGS84"
ices.rect.shp <- st_transform(ices.rect.shp, wgs)

ices.rectareas.shp <- st_read("C:/use/proyectos/IM-18-ANICHO/datos/ICES_shapefiles/ICES_StatRec_mapto_ICES_Areas")
st_crs(ices.rectareas.shp)
wgs<-"+proj=longlat +datum=WGS84 +ellps=WGS84"
ices.rectareas.shp <- st_transform(ices.rectareas.shp, wgs)

# get bathymetry data

bathy <- getNOAA.bathy(lon1=-18,lon2=0,lat1=41,lat2=51, resolution = 1, 
                       keep=FALSE, antimeridian=FALSE)
class(bathy)
autoplot(bathy)
bathy.df <- fortify(bathy)
class(bathy.df)
str(bathy.df)

# add Depth from marmap according to Lon and Lat

idx <- which(!is.na(df$LON) & !is.na(df$LAT) & df$LON > -18 & df$LON < 0 & df$LAT >41 & df$LAT < 51)
df$DEPTH <- NA
df$DEPTH[idx] <- get.depth(bathy, df[idx,c("LON","LAT")], locator=F)$depth

# Maps --------------------------------------------------------------------

# basic map data

global <- map_data("worldHires")

# basic ggplot

p0 <- ggplot() + 
  geom_contour(data=bathy.df, aes(x,y,z=z), breaks=c(-100, -200), col="grey")+
  annotation_map(map=global, fill="grey")+
  geom_sf(data=fortify(ices.areas.shp[1]), fill=NA)+
  scale_x_continuous(minor_breaks = seq(-10, 0, 1), breaks = seq(-10, 0, 1))+   # ices rectangles
  scale_y_continuous(minor_breaks = seq(42, 50, 0.5), breaks=seq(42, 50, 1))+   # ices rectangles
  coord_sf(xlim=c(-10,0), ylim=c(42,50))+
  xlab("")+
  ylab("")
print(p0)

# EGSP: Transformation to UTM ---------------------------------------------

# function to find your UTM. Taken from Nerea Goikoetxea

lonlat2UTM = function(lonlat) {
  utm = (floor((lonlat[1] + 180) / 6) %% 60) + 1
  if(lonlat[2] > 0) {
    utm + 32600
  } else{
    utm + 32700
  }
}

EPSG_2_UTM <- lonlat2UTM(c(mean(df$LON), mean(df$LAT))) 
EPSG_2_UTM # 32630 --> UTM zone 30N; 
#           WGS84 Bounds: -6.0000, 0.0000, 0.0000, 84.0000
#           Projected Bounds: 166021.4431, 0.0000, 833978.5569, 9329005.1825

# Visualize areas for generating pseudo-absences ---------------------------

# check class
class(ices.rectareas.shp)

# select multipolygon object from the shapefile
aux1 <- ices.areas.shp[ices.areas.shp$Area_27 %in% c("8.b", "8.c"), ]

# create a polygon for intersection with ices areas, so that we can select from 6ÂºW to the east

aux2 <- st_sfc(st_polygon( list(rbind(c(-6, 40), c(-6, 50), c(1, 50), c(1,40), c(-6, 40)))))
aux2 <- st_set_crs(aux2, "+proj=longlat +datum=WGS84 +ellps=WGS84") 
wgs<-"+proj=longlat +datum=WGS84 +ellps=WGS84"
aux2 <- st_transform(aux2, wgs) 

# create a polygon for union with ices areas, so that we can add two rectangles from 4ÂºW to the east

aux3 <- st_sfc(st_polygon( list(rbind(c(-4, 44), c(-4, 46), c(-2, 46), c(-2,44), c(-4, 44)))))
# aux3 <- st_sfc(st_polygon( list(rbind(c(-4, 44.5), c(-4, 45.5), c(-2, 45.5), c(-2,44.5), c(-4, 44.5)))))
aux3 <- st_set_crs(aux3, "+proj=longlat +datum=WGS84 +ellps=WGS84") 
wgs<-"+proj=longlat +datum=WGS84 +ellps=WGS84"
aux3 <- st_transform(aux3, wgs) 

# transform the catch data points into sf and add the CRS

df.sf <- st_as_sf(df, coords=c("LON","LAT"))
df.sf <- st_set_crs(df.sf, "+proj=longlat +datum=WGS84 +ellps=WGS84") 
wgs<-"+proj=longlat +datum=WGS84 +ellps=WGS84"
df.sf <- st_transform(df.sf, wgs) 

# transform to UTMs (in m)

aux1.utm <- st_transform(aux1, EPSG_2_UTM)
aux2.utm <- st_transform(aux2, EPSG_2_UTM)
aux3.utm <- st_transform(aux3, EPSG_2_UTM)
df.sf.utm <- st_transform(df.sf, EPSG_2_UTM)

# convex hull of presence data
# df_buff <- st_convex_hull(st_union(df.sf.utm))
# plot(df_buff)

# create buffers of 10km (10000) around the points and join the resulting polygons

buffer <- st_buffer(df.sf.utm, dist=10000)
buffer <- st_union(buffer)
plot(buffer)

# intersect the ices divisions and the squares

aux <- st_intersection(x=st_union(st_union(aux1.utm), aux3.utm), y=aux2.utm)
plot(aux, col=2)
plot(buffer, add=T)

# intersect the result with the buffers around the catch data points
aux0 <- st_difference(aux, buffer)
plot(aux0, col=2)

# ggplot for all data

p <- p0 +
  geom_sf(data=aux0, fill="red", alpha=0.3)+
  coord_sf(xlim=c(-10,0), ylim=c(42,50))
ggsave(file.path("plots","pseudo",paste0("area_pseudo_all.png")), p, device="png")

# # randomly sample inside the new polygon
# 
# kk <-  st_sample(aux0, size=100, type="random")
# 
# # plot
# 
# p <- p0 +
#   geom_sf(data=aux0, col="red", alpha=0.3)+
#   geom_sf(data=fortify(kk))+
#   coord_sf(xlim=c(-10,0), ylim=c(42,50))
# print(p)
# 
# # extract coordinates as data.frame
# 
# st_coordinates(kk)

# loop to calculate area to generate pseudo-absences by year

for (yy in sort(unique(df$YEAR))){
  df.sub <- subset(df.sf.utm, YEAR==yy)
  if (nrow(df.sub)>0){
    buffer.sub <- st_buffer(df.sub, dist=10000)
    buffer.sub <- st_union(buffer.sub)
    aux0.sub <- st_difference(aux, buffer.sub)
    p <- p0 +
      geom_sf(data=aux0.sub, fill="red", alpha=0.3)+
      #geom_sf(data=df.sub)+
      coord_sf(xlim=c(-6,0), ylim=c(43,46))
    ggsave(file.path("plots","pseudo",paste0("area_pseudo_",yy,".png")), p, device="png")
  }
}

# loop to calculate area to generate pseudo-absences by month and year

for (yy in sort(unique(df$YEAR))){
  for (mm in seq(3,7,by=1)){
    df.sub <- subset(df.sf.utm, YEAR==yy & MONTH==mm)
    if (nrow(df.sub)>0){
      buffer.sub <- st_buffer(df.sub, dist=10000)
      buffer.sub <- st_union(buffer.sub)
      aux0.sub <- st_difference(aux, buffer.sub)
      p <- p0 +
        geom_sf(data=aux0.sub, fill="red", alpha=0.3)+
        #geom_sf(data=df.sub)+
        coord_sf(xlim=c(-6,0), ylim=c(43,46))
      ggsave(file.path("plots","pseudo",paste0("area_pseudo_",yy,"_",mm,".png")), p, device="png")
    }    
  }
}

# Remove points outside ices 8b, 8c or in land ----------------------------

# number of points within the study area

df.in <- st_intersects(df.sf.utm, aux, sparse=FALSE)
df$INSIDE <- df.in[,1]
mean(df$INSIDE) # 0.9743969 points inside the area of study

table(df$INSIDE)
table(df$DEPTH>0)
table(df$INSIDE, df$DEPTH>0)

p <- p0 +
  geom_point(data=subset(df, INSIDE==1 & DEPTH <=0), aes(x=LON, y=LAT), col="red", alpha=0.3)+
  coord_sf(xlim=c(-10,0), ylim=c(43,46))+
  ggtitle("Points inside area with Depth<=0")
  ggsave(file.path("plots","pseudo",paste0("points_inside_depthneg.png")), p, device="png")

p <- p0 +
  geom_point(data=subset(df, INSIDE==1 & DEPTH >0), aes(x=LON, y=LAT), col="red", alpha=0.3)+
  coord_sf(xlim=c(-10,0), ylim=c(43,46))+
  ggtitle("Points inside area with Depth>0")
  ggsave(file.path("plots","pseudo",paste0("points_inside_depthpos.png")), p, device="png")

p <- p0 +
  geom_point(data=subset(df, INSIDE==0 & DEPTH <= 0), aes(x=LON, y=LAT), col="red", alpha=0.3)+
  coord_sf(xlim=c(-10,0), ylim=c(43,46))+
  ggtitle("Points outside area with Depth<=0")
  ggsave(file.path("plots","pseudo",paste0("points_outside_depthneg.png")), p, device="png")

p <- p0 +
  geom_point(data=subset(df, INSIDE==0 & DEPTH > 0), aes(x=LON, y=LAT), col="red", alpha=0.3)+
  coord_sf(xlim=c(-10,0), ylim=c(43,46))+
#  coord_sf(xlim=c(-10,0), ylim=c(42,50))+
  ggtitle("Points outside area with Depth>0")
  ggsave(file.path("plots","pseudo",paste0("points_outside_depthpos.png")), p, device="png")

# So, we keep only the points INSIDE the area with DEPTH<0

df <- subset(df, INSIDE==1 & DEPTH <= 0)
dim(df) # 24147 observations

# remove the columns that are not going to be used

df$LAND <- df$OK <- df$INSIDE <- NULL

# Generate pseudo-absences -------------------------------------------------

# we will use the positive database to generate the pseudo-absences

dfpos <- subset(df, PESO_ANE>0)
nbpoints <- nrow(dfpos) # 23678 out of 24147 are positive observations (98%)

# we compute again the buffer but only for the positive data points

# transform the catch data points into sf and add the CRS

dfpos.sf <- st_as_sf(dfpos, coords=c("LON","LAT"))
dfpos.sf <- st_set_crs(dfpos.sf, "+proj=longlat +datum=WGS84 +ellps=WGS84") 
wgs<-"+proj=longlat +datum=WGS84 +ellps=WGS84"
dfpos.sf <- st_transform(dfpos.sf, wgs) 
dfpos.sf.utm <- st_transform(dfpos.sf, EPSG_2_UTM) # transform to UTMs (in m)

# Generate the pseudo-absence data frame

pseudo <- matrix(data=NA, nrow=nbpoints, ncol=10)
pseudo <- data.frame(pseudo)
names(pseudo) <- c("CODEUEBUQUE","FECHA_CAPT","DOY","WEEK","MONTH","YEAR","LAT","LON","PESO_ANE","DEPTH")

# set the seed

set.seed(1)

# sample the date

pseudo$FECHA_CAPT <- sample(x=dfpos$FECHA_CAPT, size=nbpoints, replace = TRUE)
pseudo$FECHA_CAPT <- as.Date(pseudo$FECHA_CAPT, format="%Y-%m-%d")
pseudo$DOY <- yday(pseudo$FECHA_CAPT)
pseudo$WEEK <- week(pseudo$FECHA_CAPT)
pseudo$MONTH <- month(pseudo$FECHA_CAPT)
pseudo$YEAR <- year(pseudo$FECHA_CAPT)

# loop by month and year

for (yy in sort(unique(dfpos$YEAR))){
  for (mm in sort(unique(dfpos$MONTH))){
    idx <- which(pseudo$YEAR==yy & pseudo$MONTH==mm) 
    if (length(idx)>0){
      df.sub <- subset(dfpos.sf.utm, YEAR==yy & MONTH==mm)
      buffer.sub <- st_buffer(df.sub, dist=10000)
      buffer.sub <- st_union(buffer.sub)
      aux0.sub <- st_difference(aux, buffer.sub)
      rp.sf <- st_sample(aux0.sub, size=length(idx), type="random") # randomly sample points
      rp.sf <- st_transform(rp.sf, 4326)
      rp <- as.data.frame(st_coordinates(rp.sf)) # transform to lat&lon and extract coordinates as data.frame
      pseudo$LON[idx] <- rp$X
      pseudo$LAT[idx] <- rp$Y
      p <- p0 +
        geom_sf(data=aux0.sub, fill=2, alpha=0.3)+
        geom_sf(data=df.sub, col=4, alpha=0.3)+
        geom_sf(data=rp.sf, col=1, shape=4)+
        coord_sf(xlim=c(-6,0), ylim=c(43,46))+
        ggtitle(paste("ANE",yy,mm)) 
      ggsave(file.path("plots","pseudo",paste0("pseudo_",yy,"_",mm,".png")), p, device="png")
    }  
  }
}


# complete the rest of columns

pseudo$CODEUEBUQUE <- "ESPxxxxxxxxx" # generic name to distinguish the pseudo-absence data
pseudo$PESO_ANE <- 0
pseudo$DEPTH <- get.depth(bathy, pseudo[ ,c("LON","LAT")], locator=F)$depth

# there might be still some locations with Depth>0 
summary(pseudo$DEPTH)
sum(pseudo$DEPTH>0)  # 197 obs
mean(pseudo$DEPTH>0) # 0.008316447 very small proportion

# transform to sf object (lat&lon) to plot

pseudo.sf <- st_as_sf(pseudo, coords=c("LON","LAT"))
pseudo.sf <- st_set_crs(pseudo.sf, "+proj=longlat +datum=WGS84 +ellps=WGS84") 
wgs<-"+proj=longlat +datum=WGS84 +ellps=WGS84"
pseudo.sf <- st_transform(pseudo.sf, wgs) 

p <- p0 +
  geom_sf(data=pseudo.sf, aes(col=(DEPTH>0)), col="red", alpha=0.3)+
  coord_sf(xlim=c(-6,0), ylim=c(42,46))
  #  coord_sf(xlim=c(-10,0), ylim=c(42,50))+
print(p)

# plot all pseudo-absences

p <- p0 +
  geom_sf(data=aux0, fill=2, alpha=0.3)+
  geom_sf(data=dfpos.sf, col=4, alpha=0.3)+
  geom_sf(data=pseudo.sf, col=1, shape=4, alpha=0.3)+
  coord_sf(xlim=c(-6,0), ylim=c(43,46))
print(p)
ggsave(file.path("plots","pseudo","pseudo_all.png"), p, device="png")

# plot pseudo-absences by year

p <- p0 +
  geom_sf(data=aux0, fill=2, alpha=0.3)+
  geom_sf(data=dfpos.sf, col=4, alpha=0.3)+
  geom_sf(data=pseudo.sf, col=1, shape=4, alpha=0.3)+
  coord_sf(xlim=c(-6,0), ylim=c(43,46))+
  facet_wrap(~YEAR)
print(p)
ggsave(file.path("plots","pseudo","pseudo_all_by_year.png"), p, device="png")


# Save the final dataset including the pseudo-absences --------------------

head(df)
head(pseudo)

# Join the two data sets and save the final dataset

dat <- rbind(df, pseudo)
write.table(dat, file=file.path("data","dfpseudo_ane_2010_2019.csv"), row.names=F, sep=";", dec=".")

# End of script -----------------------------------------------------------



```



<!--chapter:end:03-data_pa.Rmd-->

# Environmental data

Bla bla bla

## Download from public repositories

Download from Bio-oracle. 

```{r, eval=F}
### Download Environmental data from Bio-oracle

# for mapping ...
library(maptools)
library(rgrass7)
library(raster)
library(sp)
library(ggplot2)
library(maps)

# libraries for bio-oracle
library(rgdal)
library(sdmpredictors) 
library(leaflet)

# http://bio-oracle.org/code.php
install.packages("sdmpredictors")
#install.packages("leaflet")

# Load package 
library(sdmpredictors)

# Species Data example from GBIF
mydata <- occ_data(scientificName = "Anisakis", hasCoordinate = TRUE)$data  # 02/10/2021 416 obs x 150 var
mydata.gbif <- subset(mydata, select=c(acceptedScientificName,genus,specificEpithet,decimalLatitude,decimalLongitude,year,month,day))
mydata.gbif.ll <- cbind(mydata.gbif$decimalLongitude, mydata.gbif$decimalLatitude)
mydata.gbif.ll <- as.data.frame(mydata.gbif.ll)
names(mydata.gbif.ll) <- c("Lon", "Lat")

# Explore datasets in the package 
list_datasets()
list_layers("Bio-ORACLE")
mytab <- list_layers("Bio-ORACLE")

## Download specific layers to the current directory 

myBioracle.layers <- load_layers(c("BO_chlomean", "BO_damean", "BO_salinity", "BO_sstmean", "BO_sstrange", "BO_bathymean")) 
#save(myBioracle.layers, file="myBioracle.layers.Rdata")
#load(file="myBioracle.layers.Rdata") # creo que no funciona
myBioracle.layers

# Check layer statistics 
layer_stats("myBioracle.layers") 

# Crop raster to fit the North East Atlantic window for estimating SST range
# lat: 36.49 to 67.00  
# lon: -16.000 to 9.000
my.NEatlantic.ext <- extent(-100, 45, -90, 90) # xmin, xmax, ymin, ymax
myBioracle.layers.cropNEatlantic <- crop(myBioracle.layers, my.NEatlantic.ext) 
my.colors = colorRampPalette(c("#5E85B8","#EDF0C0","#C13127")) 
plot(myBioracle.layers.cropNEatlantic,col=my.colors(1000),axes=F, box=F)
summary(myBioracle.layers.cropNEatlantic)

# Generate a nice color ramp and plot the map 
my.colors = colorRampPalette(c("#5E85B8","#EDF0C0","#C13127")) 

plot(myBioracle.layers.cropNEatlantic,col=my.colors(1000),axes=F, box=F)

image(log(myBioracle.layers.cropNEatlantic$BO_chlomean),col=my.colors(1000),axes=T, ylab=NA,xlab=NA, main="Chl (log scale)", cex.main=2)
map("world",add=T, fill=T, col="white",lwd=0.1); box()

image(log(myBioracle.layers.cropNEatlantic$BO_damean),col=my.colors(1000),axes=T, ylab=NA,xlab=NA,main="Diffuse Attenuation", cex.main=2)
map("world",add=T, fill=T, col="white",lwd=0.1); box()

image(myBioracle.layers.cropNEatlantic$BO_salinity,col=my.colors(1000),axes=T, ylab=NA,xlab=NA,main="Salinity", cex.main=2)
map("world",add=T, fill=T, col="white",lwd=0.1); box()

image(myBioracle.layers.cropNEatlantic$BO_sstmean,col=my.colors(1000),axes=T, ylab=NA,xlab=NA,main="SST mean", cex.main=2)
map("world",add=T, fill=T, col="white",lwd=0.1); box()

image(myBioracle.layers.cropNEatlantic$BO_sstrange,col=my.colors(1000),axes=T, ylab=NA,xlab=NA,main="SST range", cex.main=2)
map("world",add=T, fill=T, col="white",lwd=0.1); box()

image(myBioracle.layers.cropNEatlantic$BO_bathymean,col=my.colors(1000),axes=T, ylab=NA,xlab=NA,main="Depth", cex.main=2)
map("world",add=T, fill=T, col="white",lwd=0.1); box()


### Extract environmental values from layers 

mydata1.env <- raster::extract(x=myBioracle.layers,y=mydata.gbif.ll, df=T) # sin buffer (se extrae solo los puntos)

# con Bilinear extraigo los sites de los 4 nearest cells 
mydata1.env.bil <- raster::extract(x=myBioracle.layers,y=mydata.gbif.ll, method="bilinear", na.rm=TRUE, df=T) # 4 nearest cells, se consigue valor 29 sites más (se siguen perdiendo 64, de los cuales algunos son errores, es decir, no costeros)

mydata.all <- cbind(mydata.gbif.ll, mydata1.env.bil)

summary(mydata.all) # 448 sites : 84 are NAs


```

We can also download the bathymetry from the `marmap` library

```{r, eval=F}
library(marmap)

bathy <- getNOAA.bathy(lon1=-11,lon2=0,lat1=41,lat2=51, resolution = 1, 
                       keep=FALSE, antimeridian=FALSE)
bathy.df <- fortify(bathy) # LI: maybe this is not needed here

df$Depth <- get.depth(bathy, df[,c("Lon","Lat")], locator=F)$depth

```

## Operations with rasters (maybe not needed)

We can complete this a bit more later on, though not necessary right now

for example, given a raster, we can calculate gradients in the vertical or depth at which the max is found

```{r, eval=F}

# Auxiliary functions ------------------------------------------------------

# Taken from: 
# https://gis.stackexchange.com/questions/114723/subset-netcdf-based-on-last-valid-variable-by-level

# deepestValid: function to that, for each cell,  
# - returns NA in case all depth level values are NA
# - returns the value of the last available depth level in case no NA occur
# - else returns the value of the last non-NA depth level

deepestValid <- function(x) {
  na <- is.na(x)
  if (all(na)) {
    return(NA)
  } else if (all(!na)) {
    return(x[length(x)])
  } else {
    first_na <- which(na)[1]
    last_valid <- first_na - 1
    return(x[last_valid])
  }
}

# read the data files using the library raster

b1 <- brick(file.path(hdata.dir, paste0("thetao_",dd, " 12:00:00.nc")))
b2 <- brick(file.path(hdata.dir, paste0("so_",dd, " 12:00:00.nc")))
b3 <- brick(file.path(hdata.dir, paste0("mlotst_",dd, " 12:00:00.nc")))
b4 <- brick(file.path(hdata.dir, paste0("chl_",dd, " 12:00:00.nc")))
b5 <- brick(file.path(hdata.dir, paste0("o2_",dd, " 12:00:00.nc"))) 

# temperature at surface (exactly at 0.49m)
# see names(b1)

bb1a <- b1[[1]] 
names(bb1a) <- "TEMP0m"  

# temperature at 100m (layer 22, which is at 92.33m) or at the deepest available 
# see names(b1)

bb1b <- calc(subset(b1, 1:22), fun=deepestValid)
names(bb1b) <- "TEMP100m" 

# salinity at surface (exactly at 0.49m)
# see names(b2)

bb2 <- b2[[1]] 
names(bb2) <- "so1"  

# logarithm of ocean mixed layer thickness (m)

bb3 <- b3[[1]]
bb3 <- log(bb3)
# or equivalently:
# bb3 <- calc(bb3, fun=function(x) {log(x)})
names(bb3) <- "logmlotst1"

# logarithm of the chlorophyll integrated until 100m (layer 22, which is at 92.33m) or at the deepest available 
# see names(b4)

b4 <- subset(b4, 1:22) # take only values in the first 22 layers corresponding up to depth 100m 
depth.lev <- as.numeric(as.character(sapply(names(b4), substring, first=2))) # extract depth values from layer names
depth.lev <- c(0, depth.lev)
w <- diff(depth.lev) 
bb4 <- calc(b4, function(x){sum(x*w, na.rm=T)}) # compute integrated value
bb4 <- log(bb4)
names(bb4) <- "logCHL100m"

# oxygen integrated until 100m (layer 22, which is at 92.33m) or at the deepest available 
# see names(b5)

b5 <- subset(b5, 1:22) # take only values in the first 22 layers corresponding up to depth 100m
depth.lev <- as.numeric(as.character(sapply(names(b5), substring, first=2))) # extract depth values from layer names
depth.lev <- c(0, depth.lev)
w <- diff(depth.lev)
bb5 <- calc(b5, function(x){sum(x*w, na.rm=T)}) # compute integrated value
names(bb5) <- "O100m"

# unique stack (because they all have the same extent and resolution)

b <- stack(bb1a, bb1b, bb2, bb3, bb4, bb5)

# Step 2: Add bathymetry --------------------------------------------------

# get the extent of the raster to extract the corresponding bathymetry

e <- extent(b)

# load bathymetry of the area (get wider limits)
# resolution = 5 (resolucion en minutos, corresponde a 1/12 º) 

# mybathy <- getNOAA.bathy(lon1=floor(e@xmin), lon2=ceiling(e@xmax), lat1=floor(e@ymin), lat2=ceiling(e@ymax), 
#                          resolution=1, keep=F) 
# save(mybathy, file="mybathy.RData")

load(file.path("mybathy.RData"))
     
# transform to data frame

pred.dat <- raster::as.data.frame(b, xy=T)

# include bathymetry into the prediction data frame

pred.dat$DEPTH <- get.depth(mybathy, pred.dat[ ,c("x","y")], locator=F)$depth
pred.dat$DEPTH[pred.dat$DEPTH>=0] <- NA
pred.dat$logDEPTH<-log(-pred.dat$DEPTH)

# transform the bathymetry to raster format 

b7 <- rasterFromXYZ(pred.dat[,c("x","y","logDEPTH")]) 
names(b7) <- "logDEPTH"

# include the bathymetry raster into the prediction stack

b <- stack(b, b7)


# Step 3: Create latitude, longitude and doy rasters ----------------------
# these are needed to predict according to the spatio-temporal model

b8 <- b9 <- b10 <- b[[1]] # create rasters with same structure

b8[] <- coordinates(b8)[,1] #longitude  
names(b8) <- "LON"

b9[] <- coordinates(b8)[,2] #latitude  
names(b9) <- "LAT"

b10[] <- doy # day of the year
names(b10) <- "DOY"

# include longitude, latitude and day of the year into the prediction stack, 
# they are needed for the spatio-temporal model prediction

b <- stack(b, b8, b9, b10)

```


<!--chapter:end:04-data_env.Rmd-->

# Prepare final dataset

Bla bla bla

## Extract environmental data associated to presence-absence data

## Exploratory plots

<!--chapter:end:05-data_final.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Shape Constrained-Generalized Additive Models

One citation is [@citores_etal_2020]

Mention there is an alternative using `mboost` that won't be further developed here.  

### Loading required libraries

```{r, eval=F}
library(scam)
#library(mgcv)
library(plotmo)
library(rgdal)
library(ggplot2)
library(dplyr)
library(fields) #imageplot
library(maps) #map world
library(raster)
library(RColorBrewer)#color palette

#SDMTools - install RTools and follow: https://cran.r-project.org/bin/windows/Rtools/
#install.packages("remotes")
#remotes::install_version("SDMTools", "1.1-221.2")
library(SDMTools)
library(dismo)
library(stringr)
```


## Model fit

```{r, eval=F}
#NOTA INTERNA: aquí hay que cargar el RData de presencias y pseudoausencias que creemos en 05_data_final de momento he copido los datos de PA de la anchoa para poder correr el código. Habrá que cambiar status por el nomnbre de la columna de presencia-pseudoausencia y el nombre de las variables, en este caso lo hacemos para la anchoa con todas las variables que tenemos: 

#Load presence-absence dataset of the species

load(here::here ("data", "outputs_for_modelling", "PA.Rdata"))
data <- SP11 #NOTA INTERNA: así se llama el RData de la anchoa


raw_model <- scam (status ~  s(temp.v, k=8,bs="cv", m=2)+ s(sal.v, k=8,bs="cv", m=2) + s(nit.v, k=8,bs="cv", m=2) + s(npp.v, k=8,bs="cv", m=2)+ s(dist_seabed.v, k=8,bs="cv", m=2) + s(mld_relative_position.v, k=8,bs="cv", m=2), family=binomial(link="logit"), data=data)


#Check raw model which contains all variables
summary(raw_model)
scam.check(raw_model)
plot(raw_model)
plotmo(raw_model,level = 0.95, pt.col=8)
```


## Model selection

# Source the function for forward selection of the variables developed by libaibarriaga criterium: AIC and pvalue

```{r, eval=F}
source(here::here ("function","function_scam_selection_optimized.R"))
```

## preparing data
```{r, eval=F}
# selecting the variables to be used in the model 

#NOTA INTERNA: añadir el nombre de las variables que tenemos
vars <- c("temp.v",
          "sal.v",
          "nit.v",
          "dist_seabed.v",
          "mld_relative_position.v",
          "npp.v")
```

#Code to build SC-GAM model default sp= NULL, k = 8 and plevel  = 0.05
```{r, eval=F}

  # apply the model selection function (no limits in the number of terms)
  model_SCGAM <- try(modsel.scam(basef="status ~ 1", vars=vars, dat=data), silent=T) # by default aic.tol=2

#NOTA INTERNA: ¿queremos añadir el if else y asignar un valor de sp en caso de que sp NULL de error?

```

# check results
```{r, eval=F}
model_SCGAM$svars
sapply(model_SCGAM$smod, AIC)
sapply(model_SCGAM$smod, function(x) summary(x)$dev.expl)
lapply(model_SCGAM$smod, formula)
lapply(model_SCGAM$smod, summary)

# see specifically the summary and the plots of the final model after the selection
# note that it is the last element of the list of models


summary(selected_model)
scam.check(selected_model)
plot(selected_model)
plotmo(selected_model,level = 0.95, pt.col=8)

#Select the last model of the list which is the best model according to AIC and plevel criteria
selected_model <- model_SCGAM$smod[[length(model_SCGAM$smod)]]
selected_model$sp

# Save the model

save(selected_model, file = here::here("models/selected_model.RData"))

```

<!--chapter:end:06-model_fit.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---


# Model validation

Bla bla 


### Loading required libraries

```{r, eval=F}
library(scam)
#library(mgcv)
library(plotmo)
library(rgdal)
library(ggplot2)
library(dplyr)
library(fields) #imageplot
library(maps) #map world
library(raster)
library(RColorBrewer)#color palette
library(tidyverse)
library(R.utils) #loadToEnv

#SDMTools - install RTools and follow: https://cran.r-project.org/bin/windows/Rtools/
#install.packages("remotes")
#remotes::install_version("SDMTools", "1.1-221.2")
library(SDMTools)
library(dismo)

#plots
library(ggplot2)
library(ggpubr)
library(hrbrthemes) # theme_ipsum
```

## Optimum threshold

```{r, eval=F}
#NOTA INTERNA: aquí hay que cargar el RData del modelo, cargo el que se crea con el 06-model_fit para poder correr el código

#Load SC-GAM model

load(here::here ("models", "selected_model.Rdata"))

#Load species PA data

load(here::here ("data", "outputs_for_modelling", "PA.Rdata"))
data <- SP11 #NOTA INTERNA: así se llama el RData de la anchoa

  # Predict 
  scgam.pred <- predict(selected_model, newdata=data, type="response")
  
  # Add the prediction to the table
  data$scgam.pred <- as.vector(scgam.pred)
  head(data)

  ## Optimizing the threshold probability
  obs <- data$status
  predSCGAM_P <- data$scgam.pred
  
  # threshold optimizing
  myoptim <- optim.thresh (obs,predSCGAM_P)
  myoptim

```


## k-fold validation

## VALIDATION  --------------------
# https://rpubs.com/mlibxmda/SDMPartOne

Create a df to save validation results
```{r, eval=F}
validation_summary <- matrix(NA, ncol = 6, nrow = 1)
```


```{r, eval=F}
  # select the threshold that maximizes the sum of sensitivity and specificity (Jimenez-Valverde and Lobo, 2017)
  myThreshold <- as.numeric((myoptim[["max.sensitivity+specificity"]])) 
  
  # In case, the result is a range (instead of a single value), we select the mean value of the range 

  if(length(myThreshold)>1){
    print(paste("Note that the threshold is the mean of the range:", myThreshold))
    myThreshold <- mean (myThreshold) 
  } 
  
  # accuracy values with all observations
  accuracy (obs, predSCGAM_P, threshold=myThreshold)
  
  # create confusion matrix with all observations
  confusion.matrix(obs, predSCGAM_P, threshold=myThreshold)
  
  # isolate the formula  
  v_summary <- summary(selected_model)
  
  formula <- v_summary[["formula"]]
  
  # Generating K Groups of presences for k-fold cross-validation
  
  # cross-validation using k-fold
  k <- 5 # Number of groups
  
  # Separate Presences and absences into 2 different objects
  Pres   <- subset(data, status==1)
  Aus  <- subset(data, status==0)

  # separate the presences and the absences into the k groups
  # LI: I think we could alternatively do it in one go (i.e. without separating presences and absences) using the argument status: kfold(data, k, by=data$status)
  
  groups <- kfold(Pres, k)   
  head(groups)
  groups2 <- kfold(Aus, k)
  head (groups2)
  
  # initialise the confusion matrix and the accuracy table: 
  myCM <- NULL 
  myACC <- NULL

  # loop for each group k
  
  for (j in 1:k) {
    
    print(paste("Results k-fold validation: group ", j))
    
    # Preparation of Tabla of Training Sites
    presencias_Training <- Pres[groups != j,]
    ausencias_Training <- Aus[groups2 != j,]
    
    # Join the two tables
    puntos_Training <- rbind(presencias_Training, ausencias_Training)
    
    # Model fit
    sp <- unique(selected_model$sp)
    
    sp <- as.data.frame(sp)
    
    selected_model.sp.j <- scam (formula, family=binomial(link="logit"), 
                        data=puntos_Training, sp=c(sp$sp))
    summary(selected_model.sp.j)
    
    # Predict Model
    muestra_validacion <- Pres[groups == j,]     # valores de predict donde las observaciones son presencias
    muestra2_validacion <- Aus [groups2 == j,]   # valores de predict donde las observaciones son ausencias
    puntos_validacion<-rbind(muestra_validacion, muestra2_validacion)
    
    selected_model.sp.j.pred <- predict(selected_model.sp.j, newdata=puntos_validacion, type="response")
    puntos_validacion$Pred <- selected_model.sp.j.pred
    
    # Confussion matrix and accuracy table for fold j
    obs <- puntos_validacion$status
    predSCGAM <- puntos_validacion$Pred
    # print(confusion.matrix(obs, predSCGAM, threshold=myThreshold))
    # print(accuracy(obs, predSCGAM, threshold=myThreshold))
    myCM <- rbind(myCM, as.numeric(confusion.matrix(obs, predSCGAM, threshold=myThreshold))) # columns are: obs0pred0, obs0pred1, obs1pred0, obs1pred1
    myACC <- rbind(myACC, accuracy(obs, predSCGAM, threshold=myThreshold))
  } # end of loop for j
  
  # save threshold
  Threshold <- myThreshold
  
  # Mean values across k-folds
  mean_AUC <- mean(myACC$AUC)
  mean_Omision <- mean(myACC$omission.rate)
  mean_sensitivity <- mean(myACC$sensitivity)
  mean_specificity <- mean(myACC$specificity)
  mean_Prop.Corr <- mean(myACC$prop.correct)  
  
  # add mean values to the validation summary table for species
  validation_summary[1,]<-c(Threshold,
                            mean_AUC,
                            mean_Omision,
                            mean_sensitivity,
                            mean_specificity,
                            mean_Prop.Corr)
  
  validation_summary.df <- as.data.frame(validation_summary)

  names (validation_summary.df) <- c("Threshold", "mean_AUC", "mean_Omision", "mean_sensitivity","mean_specificity", "mean_Prop.Corr")

  head(validation_summary.df)
```


<!--chapter:end:07-model_validation.Rmd-->

# Prediction and maps

predict from fitted models and produce maps

<!--chapter:end:08-projections.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:09-references.Rmd-->

