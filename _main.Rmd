--- 
title: "Species distribution models (SDM)"
author: "AZTI"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography:
- references.bib
description: |
  This is a short tutorial about constructing species distribution models in R using shape-constrained generalized additive models.  
link-citations: yes
github-repo: Fundacion-AZTI/SDM
---

# About {-}

This is a short tutorial for constructing species distribution models in R using shape-constrained generalized additive models. 

The code is available in [AZTI's github repository](https://github.com/Fundacion-AZTI/SDM) repository and the book is readily available [here](https://fundacion-azti.github.io/SDM/).

To cite this book, please use: 

BLA BLA BLA

<!--chapter:end:index.Rmd-->

# Introduction

Species distribution models (SDMs) are numerical tools that combine observations of species occurrence or abundance at known locations with information on the environmental and/or spatial characteristics of those locations [@elith_etal_2009]. SDMs are widely used as a tool for understanding species spatial ecology and are also known as ecological niche models (ENM) or habitat suitability models.

According to ecological niche theory, species response curves are unimodal with respect to environmental gradients [@hutchinson_1957]. While a variety of statistical methods have been developed for species distribution modelling, a general problem with most of these habitat modelling approaches is that the estimated response curves can display biologically implausible shapes which do not respect ecological niche theory. This is because species response curves are fit statistically with any assumption or restriction, which sometimes do not respect the ecological niche theory. To better understand species response to environmental changes, SDMs should consider theoretical background such as the ecological niche theory and pursue the unimodality of the response curve with respect to environmental gradients.

This book provides a tutorial on how to use shape-constrained generalized additive models (SC-GAMs) to build SDMs under the ecological niche theory framework [@citores_etal_2020]. SC-GAMs impose monotonicity and concavity constraints in the linear predictor of the GAMs and avoid overfitting. SC-GAM is an effective alternative to fitting nonsymmetric parametric response curves, while retaining the unimodality constraint, required by ecological niche theory, for direct variables and limiting factors.

The book is organised following the key steps in good modelling practice of SDMs [@elith_etal_2009]. First, presence data of a selected species are downloaded from GBIF/OBIS global public datasets and pseudo-absence data are created. Then, environmental data are downloaded from public repositories and extracted at each of the presence/pseudo-absence data points. Based on this dataset, an exploratory analysis is conducted to help deciding on the best modelling approach. The model is fitted to the dataset and the quality of the fit and the realism of the fitted response function are evaluated. After selecting a threshold to transform the continuous probability predictions into binary responses, the model is validated using a k-fold approach. Finally, the predicted maps are generated for visualization.
   


<!--chapter:end:01-intro.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Libraries

Load libraries that will be used

```{r}
library("scam")
```

Note that all the libraries must be installed. If some library is not installed, run: 

```{r}
install.packages("scam")
```

#NOTA INTERNA: eta zerbait horrela idazten badugu: 

List of required packages to follow this tutorial
```{r}
requiredPackages <- c(
  #GENERAL USE LIBRARIES --------#
  "here", # Library for reproducible workflow
  "rstudioapi",  # Library for reproducible workflow
  "maptools", #plotting world map
  "ggplot2", #for plotting
  
  #Chapter 3 Presence-absence data --------#
  "robis", # Specific library to get the occurrence data
  "rgbif",# Specific library to get the occurrence data
  "CoordinateCleaner", #to remove outlier
  "rgdal", # to work with Spatial data
  "sf", # to work with spatial data (shapefiles)
  "data.table", #for reading data,
  "dplyr", #for reading data,
  "tidyr", #for reading data,
  
  #Chapter 4 Environmental data --------#
  "raster", #to work with spatial data
  
  #Chapter 5 Prepare final dataset --------#
  
  #Chapter 6 Shape Constrained-Generalized Additive Models --------#
  "scam" #to build SC-GAMs models
  )

```

Function to install packages in case is need and load all required packages
```{r}
#Function to install the required packages that are not in your system and load all the required packages
install_load_function <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg))
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}

install_load_function(requiredPackages)
```


<!--chapter:end:02-libraries.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Presence-absence data

In this chapter we first, download occurrence data from global open-access datasets such as Global Biodiversity Information Facility (GBIF, https://www.gbif.org/) and Ocean Biodiversity Information System (OBIS, https://obis.org/); second, clean downloaded data reformating, renaming fields and removing outliers data; and lastly, we generate a set of pseudoabsence points along our study area. 

First we load list required libraries.

```{r, eval=T,message=FALSE,warning=FALSE}
requiredPackages <- c(
  #GENERAL USE LIBRARIES --------#
  "here", # Library for reproducible workflow
  "rstudioapi",  # Library for reproducible workflow
  "maptools", #plotting world map
  "ggplot2", #for plotting
  
  #Presence-absence data chapter--------#
  "robis", # Specific library to get the occurrence data
  "rgbif",# Specific library to get the occurrence data
  "CoordinateCleaner", #to remove outlier
  "rgdal", # to work with Spatial data
  "sf", # to work with spatial data (shapefiles)
  "data.table", #for reading data,
  "dplyr", #for reading data,
  "tidyr" #for reading data
  )
```

Function to install the required packages that are not in your system and load all the required packages
```{r, eval=F}
install_load_function <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg))
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}

install_load_function(requiredPackages)

```

Define some settings
```{r, eval=F}
# location of script
setwd(dirname(getSourceEditorContext()$path))

# general settings for ggplot (black-white background, larger base_size)
theme_set(theme_bw(base_size = 16))
```

## Download presence data

First we need to define our study area, in this case we select the Atlantic ocean based on the The Food and Agriculture Organization (FAO) Major Fishing Areas for Statistical Purposes and we remove Black sea subarea.   
```{r, eval=F}
# url where FAO shapfile is stored
url<-"https://www.fao.org/fishery/geoserver/area/ows?service=WFS&version=1.0.0&request=GetFeature&typeName=area%3AFAO_AREAS&maxFeatures=50&outputFormat=SHAPE-ZIP"

#download file
download.file(url,"data/spatial/FAO_AREAS.zip",mode="wb")

#unzip downloaded file
unzip("data/spatial/FAO_AREAS.zip",exdir="data/spatial")

# Load FAO (spatial multipolygon)
FAO<- st_read(file.path("data", "spatial", "FAO_AREAS.shp"))

#Select Atlantic Ocean FAO Area 
FAO_Atl <- FAO[FAO$OCEAN=="Atlantic",]

#Select Black sea subarea 
Black_Sea <- FAO_Atl[FAO_Atl$ID=="20",]

# transform to sf objects
FAO_Atl.sf <- st_as_sf(FAO_Atl)
Black_Sea.sf <- st_as_sf(Black_Sea)

# Remove Black sea using st_difference (reverse of st_intersection)
FAO_Atl_no_black_sea <- st_difference(FAO_Atl.sf,Black_Sea.sf) %>%   dplyr::select (F_AREA)

#transform to spatial polygons dataframe
study_area<- sf:::as_Spatial(FAO_Atl_no_black_sea)

plot(study_area)
```

Download data from OBIS and GBIF using scientific name. In this case we select Albacore tuna species (<em>Thunnus alalunga</em>).
```{r, eval=F}
# Get data from OBIS
mydata.obis<-robis::occurrence(scientificname="Thunnus alalunga")

# Get data from GBIF
mydata.gbif<-occ_data(scientificName="Thunnus alalunga", hasCoordinate = TRUE, limit=100000)$data
```

We now check the downloaded data and select the fields of interest
```{r, eval=F}
# check names for GBIF data
names(mydata.gbif)
  
#select columns of interest
mydata.gbif <- mydata.gbif %>%
                dplyr::select("acceptedScientificName",
                  "decimalLongitude",
                  "decimalLatitude",
                  "year",
                  "month",
                  "day",
                  "eventDate",
                  "depth")

#check names in for OBIS data
names(mydata.obis)

#select columns of interest
mydata.obis <-  mydata.obis %>%
                dplyr::select("scientificName",
                  "decimalLongitude",
                  "decimalLatitude",
                  "date_year",
                  "month",
                  "day",
                  "eventDate",
                  "depth",
                  "bathymetry",
                  "occurrenceStatus",
                  "sst")
  
```

Reformat the data addin a new field and renaming some columns from mydata.gbif dataframe in order to have the same columns and be able to join both tables

```{r, eval=F}
mydata.gbif <- mydata.gbif %>% 
    dplyr::rename(scientificName= "acceptedScientificName") %>% 
    dplyr::rename(date_year = "year") %>% 
    dplyr::mutate(bathymetry= NA) %>% 
    dplyr::mutate(occurrenceStatus=1) %>% 
    dplyr::mutate(sst= NA)

#Join data from OBIS and GBIF 
mydata.fus<-rbind(mydata.obis,mydata.gbif)
  
#assign unique scientific name 
  mydata.fus <- mydata.fus %>% 
    dplyr::mutate(scientificName= paste(mydata.obis$scientificName[1]))
```

Clean raw data
```{r, eval=F}
#give date format to eventDate and fill out month and date_year columns
  mydata.fus$eventDate <- as.Date(mydata.fus$eventDate)
  mydata.fus$date_year <- as.numeric(mydata.fus$date_year)
  mydata.fus$month <- as.numeric(mydata.fus$month)

#mutate occurrenceStatus column giving value of 1 to presences and 0 to absences
  
unique (mydata.fus$occurrenceStatus)

mydata.fus <- mydata.fus %>% 
    mutate(occurrenceStatus = ifelse(occurrenceStatus== "NA", NA, occurrenceStatus)) %>%
    mutate(occurrenceStatus = ifelse(occurrenceStatus== "Present", 1, occurrenceStatus)) %>% 
    mutate(occurrenceStatus = ifelse(occurrenceStatus== "present", 1, occurrenceStatus)) %>% 
    mutate(occurrenceStatus = ifelse(occurrenceStatus== "Presente", 1, occurrenceStatus)) %>% 
    mutate(occurrenceStatus = ifelse(occurrenceStatus== "Presence", 1, occurrenceStatus)) %>% 
    mutate(occurrenceStatus = ifelse(occurrenceStatus== "P", 1, occurrenceStatus)) %>% 
    mutate(occurrenceStatus = ifelse(occurrenceStatus== "Q", 1, occurrenceStatus))
```

Assign 1 value to all occurrences
```{r, eval=F}
mydata.fus <- mydata.fus  %>% 
    dplyr::mutate(occurrenceStatus = 1)
```

Remove outliers based on distance method (total distance= 1000 km)
```{r, eval=F}

out.dist <- cc_outl(x=mydata.fus,
                lon = "decimalLongitude", lat = "decimalLatitude",
                species = "scientificName",
                method="distance", tdi=1000, # distance method with tdi=1000km
                thinning=T, thinning_res=0.5,
                value="flagged") 

#remove outliers from the data
mydata.fus <- mydata.fus[out.dist, ]
```

Remove duplicates based on the date
```{r, eval=F}

date <- cbind(mydata.fus$decimalLongitude,mydata.fus$decimalLatitude,mydata.fus$eventDate)

mydata.fus<-mydata.fus[!duplicated(date),]
```

Prepare the data to 
```{r, eval=F}
# PREPARE DATA TO USE FAO ATLANTIC REGION MASK
---------------------------------    
    
  # Prepare coordinate format and projection to be able to use FAO zone masks
  dat <- data.frame(cbind(mydata.fus$decimalLongitude,mydata.fus$decimalLatitude))
  ptos<-as.data.table(dat,keep.columnnames=TRUE)
  
  coordinates(ptos) <- ~ X1 + X2
  
  proj4string(ptos) <-proj4string(study_area)
  
  ## Select only occurrences from FAO Atlantic
  match2<-data.frame(subset(mydata.fus,!is.na(over(ptos, study_area)[,1])))
  
  ## Extract the FAO area of each point
  match3<-data.frame(subset(over(ptos, study_area), !is.na(over(ptos, study_area)[,1])))
  
  # create data frame with area, name, lon, lat and year 
  df0<-cbind(F_AREA=match3$F_AREA,match2)[,c("F_AREA","scientificName","decimalLongitude","decimalLatitude","date_year","occurrenceStatus")]
  
  #rename some columns
  names(df0)[3:5]<-c("LON","LAT","YEAR")
  
  # add bathymetry from NOAA
  library(marmap)

  bathy <- getNOAA.bathy(lon1=-100,lon2=30,lat1=-41,lat2=55, resolution = 1, 
                       keep=FALSE, antimeridian=FALSE)

  df0$bathymetry <- get.depth(bathy, df0[,c("LON","LAT")], locator=F)$depth
```


```{r, eval=F}
# SAVE OCCURRENCE DATA
---------------------------------    

#NOTA INTERNA: quiero guardar el RData en la carpeta data/occurrences con el nombre de la especies sin espacios para no machar los RData en caso de que hagamos más de una especie pero no lo consigo. 

# lo dejo como esta, asi luego al cargar los datos no hay que poner el nombre de la especie y es mas sencillo

save(df0, file = "data/occurrences/occ.RData")
save(study_area, file = "data/spatial/total_area.RData")
```


```{r, eval=F}
# PLOT OCCURRENCES MAP
---------------------------------   
ggplot() +
   geom_path(data = study_area, 
             aes(x = long, y = lat, group = group),
             color = 'gray', size = .2) +
   geom_point(data=df0, aes(x=decimalLongitude, y=decimalLatitude,colour= occurrenceStatus)) 
  
```

## Create pseudo-absence data

Prevalence 50%

See code from ANICHO (mantaining some space around presences). Leire C.

Ref [@barbetmassin_etal_2012]

Copio aqui el codigo de anicho tal cual, luego lo limpiaré para este caso:

```{r, eval=F}
# Script information ------------------------------------------------------

# Title: Generate pseudo-absences for IM-18-ANICHO
# Last modified by Leire Ibaibarriaga (libaibarriaga@azti.es) and Leire Citores (lcitores@azti.es)

# Load libraries ----------------------------------------------------------

library(tidyverse)
library(ggplot2)
library(scales)
library(here)
library(ggridges)

library(maps)        # some basic country maps
library(mapdata)     # higher resolution maps
library(mapproj)
library(marmap)      # access global topography data
library(mapplots)    # ices rectangles
library(sf)
library(gridExtra)
library(lubridate)
library(raster)

# load data-files
#occurrence data
load("data/occurrences/occ.RData")
#map Atlantic without black sea
load("data/spatial/total_area.RData")

#remove points in land
df0<-subset(df0,bathymetry<0)

#use years from 2000 to 2014
df0<- subset(df0, YEAR<=2014 & YEAR>=2000)

# Remove points in south hemisphere (optional)
#prop.table(table(df0$LAT<=0)) #only 22% of obs in the south
#df0<-subset(df0,LAT>0)

# Remove south hemisphere area
#area <- crop(study_area, extent(-100, 100, 0, 100))
area <- study_area

#convert to spatial point data frame
df<-df0 ; coordinates(df)<- ~LON+LAT
crs(df)<-crs(area)

#convert to sf
area.sf<-st_union(st_as_sf(area))
df.sf<-st_as_sf(df)

ggplot(area.sf) + geom_sf() + geom_sf(data=st_union(df.sf))


dim(df) # 15079    5
head(df)
tail(df)
summary(df)
```


```{r, eval=F}
# Maps --------------------------------------------------------------------

# basic ggplot

global <- map_data("worldHires")

p0 <- ggplot() + 
  annotation_map(map=global, fill="grey")+
  geom_sf(data=area.sf,fill=NA)
print(p0)

# EGSP: Transformation to UTM ---------------------------------------------

# function to find your UTM. Taken from Nerea Goikoetxea

lonlat2UTM = function(lonlat) {
  utm = (floor((lonlat[1] + 180) / 6) %% 60) + 1
  if(lonlat[2] > 0) {
    utm + 32600
  } else{
    utm + 32700
  }
}

EPSG_2_UTM <- lonlat2UTM(c(mean(df$LON), mean(df$LAT))) 
EPSG_2_UTM # 32623

# transform to UTMs (in m)
aux <- st_transform(area.sf, EPSG_2_UTM)
df.sf.utm <- st_transform(df.sf, EPSG_2_UTM)

# number of points within the study area

df.in <- st_intersects(df.sf.utm, aux, sparse=FALSE)
df$INSIDE <- df.in[,1]
mean(df$INSIDE) # 100% points inside the area of study

# create buffers of 10km (10000) around the points and join the resulting polygons

buffer <- st_buffer(df.sf.utm, dist=100000)
buffer <- st_union(buffer)

p0 + geom_sf(data=buffer)

p <- p0 +
  geom_sf(data=buffer, fill="blue")+
  coord_sf(xlim=c(-90,-82), ylim=c(25,31))
print(p)

# intersect the result with the buffers around the catch data points
aux0 <- st_difference(aux, buffer)

# ggplot for all data

p <- p0 +
  geom_sf(data=aux0, fill=2)+
  coord_sf(xlim=c(-95,-82), ylim=c(25,31))
print(p)

p<-p0 + geom_sf(data=aux0,fill=2)

ggsave(file.path("plots","pseudo",paste0("area_pseudo_all.png")), p, device="png")
```


```{r, eval=F}
# Generate pseudo-absences -------------------------------------------------


# Generate the pseudo-absence data frame

pseudo <- matrix(data=NA, nrow=dim(df0)[1], ncol=dim(df0)[2])
pseudo <- data.frame(pseudo)
names(pseudo) <- names(df0)

# set the seed

set.seed(1)

##no loop

buffer.sub <- st_buffer(df.sf.utm, dist=100000)
buffer.sub <- st_union(buffer.sub)
aux0.sub <- st_difference(aux, buffer.sub)
rp.sf <- st_sample(aux0.sub, size=dim(df.sf.utm)[1], type="random") # randomly sample points
rp.sf <- st_transform(rp.sf, 4326)
rp <- as.data.frame(st_coordinates(rp.sf)) # transform to lat&lon and extract coordinates as data.frame
pseudo$LON <- rp$X
pseudo$LAT <- rp$Y
p <- p0 +
  geom_sf(data=aux0.sub, fill=2, alpha=0.3)+
  geom_sf(data=rp.sf, col=1, shape=4,size=0.5)+
  geom_sf(data=df.sf.utm, col=4, alpha=0.3)+
  ggtitle(paste(unique(df$scientificName),"all"))
ggsave(file.path("plots","pseudo",paste0("pseudo_","all",".png")), p, device="png")


p <- p0 +
  geom_sf(data=aux0.sub, fill=NA, alpha=0.3)+
  geom_sf(data=df.sf.utm, col=4, alpha=0.3,size=0.5)+
  geom_sf(data=rp.sf, col=1, shape=4,size=0.5)+
  coord_sf(xlim=c(-98,-79), ylim=c(15,31))+
  ggtitle(paste(unique(df$scientificName),"all"))
ggsave(file.path("plots","pseudo",paste0("pseudo_","all_zoom",".png")), p, device="png")


# complete the rest of columns

pseudo$scientificName <- df0$scientificName
pseudo$occurrenceStatus  <- 0
```


```{r, eval=F}
# Save the final dataset including the pseudo-absences --------------------

head(df0)
head(pseudo)

# Join the two data sets and save the final dataset

dat <- rbind(df0, pseudo)
save(list=c("dat"),file=file.path("data","outputs_for_modelling",file="PAdata.RData"))
save(list=c("area"),file=file.path("data","spatial",file="area.RData"))
write.table(dat, file=file.path("data","outputs_for_modelling","PAdata.csv"), row.names=F, sep=";", dec=".")

```




<!--chapter:end:03-data_pa.Rmd-->

# Environmental data

Bla bla bla

Loading required libraries

```{r, eval=F}
### Download Environmental data from Bio-oracle

# for mapping ...
library(maptools)
library(rgrass7)
library(raster)
library(sp)
library(ggplot2)
library(maps)
library(rgbif)
library(dataverse)
library(rstudioapi)

# libraries for bio-oracle
library(rgdal)
library(sdmpredictors) 
library(leaflet)

# http://bio-oracle.org/code.php
#install.packages("sdmpredictors")
#install.packages("leaflet")

# location of script
setwd(dirname(getSourceEditorContext()$path))
```

## Download from public repositories

Download from Bio-oracle. 

```{r, eval=F}
# Explore datasets in the package 
list_datasets()
list_layers("Bio-ORACLE")
mytab <- list_layers("Bio-ORACLE")

## Download specific layers to the current directory 

myBioracle.layers <- load_layers(c("BO2_chlomean_ss", "BO2_salinitymean_ss", "BO_damean" ,"BO_sstmean", "BO_bathymean")) 
save(myBioracle.layers, file="data/env/myBioracle.layers.Rdata")

#load area plygon
load("data/spatial/area.RData")

# transform object to select only data in the area of interest

env_dataframe <- as.data.frame(myBioracle.layers, xy=TRUE)
ptos<-env_dataframe[,c("x","y")]

coordinates(ptos) <- ~ x + y
proj4string(ptos) <-proj4string(area)
  
## Select only occurrences in the defined previously selected area
env_dataframe<-data.frame(subset(env_dataframe,!is.na(over(ptos, area)[,1])))

#save the data frame
save(env_dataframe,file="data/env/env_dataframe.Rdata")


```


<!--chapter:end:04-data_env.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Prepare final dataset

Bla bla bla

Loading required libraries

```{r, eval=F}
library(rgdal) # to read shp
library(raster) # to work with rasters
library (here) # Libraries for reproducible workflow
library (sp)     # to perform raster
library(HH) #to calculate for VIF (Variance Inflation Factor)
library(ggplot2)# plots
library(gridExtra)# multiple plots
library(dplyr)
library(RColorBrewer)#plotting colors
library(fields)#ploting images
library(marmap)#ploting world
library(maptools)#mapping
library(rstudioapi)
```

## Extract environmental data associated to presence-absence data

```{r, eval=F}

# location of script
setwd(dirname(getSourceEditorContext()$path))

#load data
load("data/outputs_for_modelling/PAdata.RData")
load(file="data/env/myBioracle.layers.Rdata") 


### Extract environmental values from layers 

# con Bilinear extraigo los sites de los 4 nearest cells 
mydata1.env.bil <- raster::extract(x=myBioracle.layers,y=dat[,c("LON","LAT")], method="bilinear", na.rm=TRUE, df=T) # 4 nearest cells, se consigue valor 29 sites más (se siguen perdiendo 64, de los cuales algunos son errores, es decir, no costeros)

# merge with pa data
data <- cbind(dat, mydata1.env.bil)

# remove positive bathymetries

data<-subset(data,BO_bathymean<0)

summary(data) # 29435 sites : 4 are NAs

#save data frame

save(list="data",file=file.path("data","outputs_for_modelling",file="PAdata_with_env.RData"))

```

## Variable correlation: Analysis of Variance Inflation Factor (VIF) 

<!-- #Nota interna: copio el script de analísis VIF para las variables superficiales del proyecto MISSION -->

<!-- ### Script information -->

With this script we assess the correlation between environmental variables based on Variance Inflation Factor (VIF) by depth set 

Project: IM-20-MISSION

VIF is a measure of the amount of multicollinearity in regression analysis

VIF equal to 1 = variables are not correlated
VIF between 1 and 5 = variables are moderately correlated 
VIF greater than 5 = variables are highly correlated

If you found high multicollinearity between two variables, you don’t need both of them in your model.

```{r, eval=F}

# select variables for vif calculation

v_table <- data %>% 
  dplyr::select (BO2_salinitymean_ss,BO_sstmean,BO2_chlomean_ss,BO_damean)

##### get vif results

resultado.vif <- vif(v_table)
sort(resultado.vif)

#remove highest

v_table <- v_table %>% 
  dplyr::select (-BO_damean)

##### get new vif results

resultado.vif <- vif(v_table)
sort(resultado.vif)


#save data frame without correlated variable

data <- data %>% dplyr::select (-BO_damean)
save(list="data",file=file.path("data","outputs_for_modelling",file="PAdata_with_env.RData"))

```

## Exploratory plots

```{r, eval=F}
####Leo archivos de especies de peces SP

load(file.path ("data", "outputs_for_modelling", "PAdata_with_env.Rdata"))

occ <- data %>% 
  dplyr::filter(data$occurrenceStatus == 1)

abs <- data %>% 
  dplyr::filter(data$occurrenceStatus == 0)

#NOTA INTERNA: El número de ausencias es mayor que las presencias, creo que tiene que ver con haber elimanado los datos de batimetrías menores que 0
  
##Salvo el pdf
pdf(file=file.path("results", "factsheets_env.pdf"), paper="a4", width=, height=12,compress=T)


  par(mfrow=c(3,3)) 
  
  names (data)

  #### 1 BO_bathymean: En plot1 una distribucion de densidad de presencias y ausencias vs bathymetry
  
  dens_occ<-density(occ$BO_bathymean,na.rm=T)
  dens_abs<-density(abs$BO_bathymean,na.rm=T)

  plot(dens_occ,xlab="BO_bathymean", ylab="dens", main = "",col="red")
  lines(dens_abs,xlab="BO_bathymean", ylab="dens", main = "",col="blue")

  legend("topleft",legend=c("Occurrences", "Pseudoabsences"),
         col=c("red", "blue"),pch=0.1, cex=0.8,box.lty=0,xpd=TRUE)
  
  ### NOTA INTERNA: hacer los plots para el resto de variables

  #2. "BO_chlomean"  
  dens_occ<-density(occ$BO2_chlomean_ss,na.rm=T)
  dens_abs<-density(abs$BO2_chlomean_ss,na.rm=T)

  plot(dens_occ,xlab="BO2_chlomean_ss", ylab="dens", main = "",col="red")
  lines(dens_abs,xlab="BO2_chlomean_ss", ylab="dens", main = "",col="blue")
  
  #NOTA INTERNA: HAY QUE JUGAR CON LAS ESCALAS

  legend("topright",legend=c("Occurrences", "Pseudoabsences"),
         col=c("red", "blue"),pch=0.1, cex=0.8,box.lty=0,xpd=TRUE)  
  #3. "BO_damean"      
  
  #REPETIR EL DENSITY PLOT DE ARRIBA CAMBIANDO LA VARIABLE
  
  #4. "BO_salinity" 
  
   #REPETIR EL DENSITY PLOT DE ARRIBA CAMBIANDO LA VARIABLE
  
  #5. "BO_sstmean" 
  
   #REPETIR EL DENSITY PLOT DE ARRIBA CAMBIANDO LA VARIABLE
  
  #6. "BO_sstrange" 
  
   #REPETIR EL DENSITY PLOT DE ARRIBA CAMBIANDO LA VARIABLE

 dev.off()
```


<!--chapter:end:05-data_final.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Shape Constrained-Generalized Additive Models


In order to fit SDM in agreement with the ecological niche theory, the proposed shape-constrained GAMs in [@citores_etal_2020] are fitted in this section. SC-GAMs are based on generalized additive models, allowing us to impose shape-constraints to the linear predictor function. The R package SCAM implements the general framework developed by Pya (2014) using shape-constrained P-splines. Monotonicity and concavity/convexity constraints can be imposed on the sign of the first and/or the second derivatives of the smooth terms. For fitting species distribution models in agreement with the ecological niche theory, we imposed concavity  constraints ($f''(x) \le 0$), so that the response can presents at most a single mode.  

Alternatively, the R package `mboost` fits SC-GAMs using boosting methodos. This alternative won't be further developed here.

First we load all required libraries.

```{r, eval=T,message=FALSE,warning=FALSE}
library(scam)
library(plotmo)
library(rgdal)
library(ggplot2)
library(dplyr)
library(fields)
library(maps) 
library(raster)
library(RColorBrewer)
library(SDMTools)
library(dismo)
library(stringr)
library(rstudioapi)
```


## Model fit

We set the working directory to the folder where the current script is located and we load the dataset (PAdata_with_env.Rdata) containing the presence-absence data toghether with the environmental data.

```{r, eval=T}
setwd(dirname(getSourceEditorContext()$path))

load(file.path ("data", "outputs_for_modelling", "PAdata_with_env.Rdata"))
```

To fit a logistic regression model in the SC-GAMs framework, we use the scam function, where we set the binomial family with the logit link function. Our response variable is the presence-absence data and the the selected three explanatory variables are the SST, chlorophyll and salinity. Each variable is included in the model through an spline function where the concavity constraint is set using bs="cv". The details about this option can be found in the section "Constructor for concave P-splines in SCAMs" of the SCAM manual (https://cran.r-project.org/web/packages/scam/scam.pdf). The number of knots (k) is fixed at 8 in this example for a good balance between flexibility and computation time.

**UNIVARIATE MODELS**

Before fitting the model with the selected three environmental variables, we can fit univariate model as follows.

We fit the univariate model for SST, we print the summary of the model fit, and look at the fitted curve in the response scale.

```{r, eval=T,message=FALSE,warning=FALSE}
model_sst <- scam (occurrenceStatus ~  s(BO_sstmean, k=8,bs="cv"), family=binomial(link="logit"), data=data)
summary(model_sst)
plotmo(model_sst,level = 0.95, pt.col=8)
```

We repeat the same for the rest of the variables.

```{r, eval=T,message=FALSE,warning=FALSE}
model_chl <- scam (occurrenceStatus ~  s(BO2_chlomean_ss, k=8,bs="cv"), family=binomial(link="logit"), data=data)
summary(model_chl)
plotmo(model_chl,level = 0.95, pt.col=8)
```


Due to convergence issues, sometimes it is necessary to fix the smoothing parameter (sp) at $10^{-5}$, as here when introducing salinity as an explanatory variable. If no value is provided, the smoothing parameter is estimated within the model.

```{r, eval=T,message=FALSE,warning=FALSE}
model_sal <- scam (occurrenceStatus ~  s(BO2_salinitymean_ss, k=8,bs="cv"), family=binomial(link="logit"), data=data,sp=0.00001)
summary(model_sal)
plotmo(model_sal,level = 0.95, pt.col=8)
```


**MODEL WITH ALL VARIABLES**

Now we fit the model including the three selected variables.

```{r, eval=T,message=FALSE,warning=FALSE}
model <- scam (occurrenceStatus ~  s(BO_sstmean, k=8,bs="cv")+ s(BO2_salinitymean_ss, k=8,bs="cv")+s(BO2_chlomean_ss, k=8,bs="cv"), family=binomial(link="logit"), data=data,sp=rep(0.00001,3))
summary(model)
plotmo(model,level = 0.95, pt.col=8)
```

We can see in the summary of the fit, that all included variables are statistically significant (with p<0.05) and present a unimodal response curve.

For a more detailed check of the fitting, we can use the scam.check function:

```{r, eval=T,message=FALSE,warning=FALSE}
scam.check(model)
```

Finally, we save the model.

```{r, eval=T,message=FALSE,warning=FALSE}
selected_model<-model
save(list=c("selected_model"), file = ("models/model.RData"))
```


## Model selection

When several expalanotry variables are available, a variable selection process can be carried out. Here we provide a function to perform forward variable selection (modsel.scam) based on the significance of variables and AIC values of the fits. 

```{r, eval=T,message=FALSE,warning=FALSE}
source("function/function_scam_selection_optimized.R")
```

We save the names of the variables we want to introduce for the variable selection process as a vector:

```{r, eval=T,message=FALSE,warning=FALSE}
vars <- c("BO_sstmean",
          "BO2_salinitymean_ss",
          "BO2_chlomean_ss")
```

The default AIC tolerance is 2 and there is not a limit on selected terms in this example. These options can be modified through aic.tol and vmax arguments in the function. The number of knots and the sp can be also modified.

```{r, eval=T,message=FALSE,warning=FALSE}
  model_SCGAM <- try(modsel.scam(basef="occurrenceStatus ~ 1", vars=vars, dat=data,sp=0.00001), silent=T)
```

We check results of the selected model:

```{r, eval=F}
model_SCGAM$svars
sapply(model_SCGAM$smod, AIC)
sapply(model_SCGAM$smod, function(x) summary(x)$dev.expl)
lapply(model_SCGAM$smod, formula)
lapply(model_SCGAM$smod, summary)

#Select the last model of the list which is the best model according to AIC and plevel criteria
selected_model <- model_SCGAM$smod[[length(model_SCGAM$smod)]]
selected_model$sp

# see specifically the summary and the plots of the final model after the selection
# note that it is the last element of the list of models
summary(selected_model)
scam.check(selected_model)
plot(selected_model)
plotmo(selected_model,level = 0.95, pt.col=8)

# Save the model

#save(selected_model, file = "models/selected_model.RData")

```

<!--chapter:end:06-model_fit.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---


# Model validation

Bla bla 


Loading required libraries

```{r, eval=F}
library(scam)
#library(mgcv)
library(plotmo)
library(rgdal)
library(ggplot2)
library(dplyr)
library(fields) #imageplot
library(maps) #map world
library(raster)
library(RColorBrewer)#color palette
library(tidyverse)
library(R.utils) #loadToEnv

#SDMTools - install RTools and follow: https://cran.r-project.org/bin/windows/Rtools/
#install.packages("remotes")
#remotes::install_version("SDMTools", "1.1-221.2")
library(SDMTools)
library(dismo)

#plots
library(ggplot2)
library(ggpubr)
library(hrbrthemes) # theme_ipsum
library(rstudioapi)
```

## Optimum threshold

```{r, eval=F}

# location of script
setwd(dirname(getSourceEditorContext()$path))

#NOTA INTERNA: aquí hay que cargar el RData del modelo, cargo el que se crea con el 06-model_fit para poder correr el código

#Load SC-GAM model

load(file.path ("models", "model.Rdata"))

#Load species PA data

#load(here::here ("data", "outputs_for_modelling", "PAdata_with_env.Rdata"))
#data <- dat.all #NOTA INTERNA: así se llama el RData 
data<-selected_model$model

  # Predict 
  scgam.pred <- predict(selected_model, newdata=data, type="response")
  
  # Add the prediction to the table
  data$scgam.pred <- as.vector(scgam.pred)
  head(data)

  ## Optimizing the threshold probability
  obs <- data$occurrenceStatus
  predSCGAM_P <- data$scgam.pred
  
  # threshold optimizing
  myoptim <- optim.thresh (obs,predSCGAM_P)
  myoptim

```


## k-fold validation

VALIDATION: https://rpubs.com/mlibxmda/SDMPartOne

Create a df to save validation results
```{r, eval=F}
validation_summary <- matrix(NA, ncol = 6, nrow = 1)
```


```{r, eval=F}
  # select the threshold that maximizes the sum of sensitivity and specificity (Jimenez-Valverde and Lobo, 2017)
  myThreshold <- as.numeric((myoptim[["max.sensitivity+specificity"]])) 
  
  # In case, the result is a range (instead of a single value), we select the mean value of the range 

  if(length(myThreshold)>1){
    print(paste("Note that the threshold is the mean of the range:", myThreshold))
    myThreshold <- mean (myThreshold) 
  } 
  
  # accuracy values with all observations
  accuracy (obs, predSCGAM_P, threshold=myThreshold)
  
  # create confusion matrix with all observations
  confusion.matrix(obs, predSCGAM_P, threshold=myThreshold)
  
  # isolate the formula  
  v_summary <- summary(selected_model)
  
  formula <- v_summary[["formula"]]
  
  # Generating K Groups of presences for k-fold cross-validation
  
  # cross-validation using k-fold
  k <- 5 # Number of groups

  #generate groups
  groups<-kfold(data, k, by=data$status)
  
  
  # initialise the confusion matrix and the accuracy table: 
  myCM <- NULL 
  myACC <- NULL

  # loop for each group k
  
  for (j in 1:k) {
    
    print(paste("Results k-fold validation: group ", j))
    
    # Preparation of Training Sites

    puntos_Training <- data[groups != j,]
    
    # Model fit
    sp <- selected_model$sp
    
    selected_model.sp.j <- scam (formula, family=binomial(link="logit"), 
                        data=puntos_Training, sp=c(sp))
    summary(selected_model.sp.j)
    
    # Predict Model
    puntos_validacion<-data[groups == j,]
    
    selected_model.sp.j.pred <- predict(selected_model.sp.j, newdata=puntos_validacion, type="response")
    puntos_validacion$Pred <- selected_model.sp.j.pred
    
    # Confussion matrix and accuracy table for fold j
    obs <- puntos_validacion$occurrenceStatus
    predSCGAM <- puntos_validacion$Pred
    # print(confusion.matrix(obs, predSCGAM, threshold=myThreshold))
    # print(accuracy(obs, predSCGAM, threshold=myThreshold))
    myCM <- rbind(myCM, as.numeric(confusion.matrix(obs, predSCGAM, threshold=myThreshold))) # columns are: obs0pred0, obs0pred1, obs1pred0, obs1pred1
    myACC <- rbind(myACC, accuracy(obs, predSCGAM, threshold=myThreshold))
  } # end of loop for j
  
  # save threshold
  Threshold <- myThreshold
  
  # Mean values across k-folds
  mean_AUC <- mean(myACC$AUC)
  mean_Omision <- mean(myACC$omission.rate)
  mean_sensitivity <- mean(myACC$sensitivity)
  mean_specificity <- mean(myACC$specificity)
  mean_Prop.Corr <- mean(myACC$prop.correct)  
  
  # add mean values to the validation summary table for species
  validation_summary[1,]<-c(Threshold,
                            mean_AUC,
                            mean_Omision,
                            mean_sensitivity,
                            mean_specificity,
                            mean_Prop.Corr)
  
  validation_summary.df <- as.data.frame(validation_summary)

  names (validation_summary.df) <- c("Threshold", "mean_AUC", "mean_Omision", "mean_sensitivity","mean_specificity", "mean_Prop.Corr")

  validation_summary.df
  
  save(validation_summary.df, file = here::here("models/validation_summary.RData"))
```


<!--chapter:end:07-model_validation.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Prediction and maps

predict from fitted models and produce maps

Loading required libraries

```{r, eval=F}

library(rgdal)
library(ggplot2)
library(dplyr)
library(fields) #imageplot
library(maps) #map world
library(raster)
library(RColorBrewer)#color palette
library(tidyverse)
library(rstudioapi)

# general settings for ggplot (black-white background, larger base_size)
theme_set(theme_bw(base_size = 16))
```

## Projection

```{r, eval=F}

# location of script
setwd(dirname(getSourceEditorContext()$path))

#load PA data
load(file.path ("data", "outputs_for_modelling", "PAdata_with_env.Rdata"))

#Load raster stack containing all variables

load("data/env/env_dataframe.Rdata")

#remove positive bathy
env_dataframe_proj<-subset(env_dataframe,BO_bathymean<0)

#Load SC-GAM model

load(file.path("models", "model.Rdata"))

# predicting 
predict <- predict(selected_model,newdata=env_dataframe_proj,type ="response",se.fit=T)         
env_dataframe_proj$fit<-predict$fit
env_dataframe_proj$se.fit<-predict$se.fit


```


## Mapping

```{r, eval=F}
p<-ggplot()+geom_raster(data=subset(env_dataframe_proj),aes(x,y,fill=fit))+scale_fill_gradient2(low="blue", mid="orange",high="red",midpoint = 0.5) +ggtitle("with bathy concave")+ geom_point(data=subset(data,occurrenceStatus==1),aes(LON,LAT),col=1,size=0.3)     

print(p)

ggsave(file.path("plots","projections",paste0("proj_map.png")), p, device="png")

save(env_dataframe_proj, file="results/projection.Rdata")

```


<!--chapter:end:08-projections.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:09-references.Rmd-->

