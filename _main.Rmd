--- 
title: "Species distribution models (SDM)"
author: "AZTI"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography:
- references.bib
description: |
  This is a short tutorial about constructing species distribution models in R using shape-constrained generalized additive models.  
link-citations: yes
github-repo: Fundacion-AZTI/SDM
---

# About {-}

This is a short tutorial for constructing species distribution models in R using shape-constrained generalized additive models. 

The code is available in [AZTI's github repository](https://github.com/Fundacion-AZTI/SDM) repository and the book is readily available [here](https://fundacion-azti.github.io/SDM/).

To cite this book, please use: 

BLA BLA BLA



<!--chapter:end:index.Rmd-->

# Introduction

Species distribution models (SDMs) are numerical tools that combine observations of species occurrence or abundance at known locations with information on the environmental and/or spatial characteristics of those locations [@elith_etal_2009]. SDMs are widely used as a tool for understanding species spatial ecology and are also known as ecological niche models (ENM) or habitat suitability models.

According to ecological niche theory, species response curves are unimodal with respect to environmental gradients [@hutchinson_1957]. While a variety of statistical methods have been developed for species distribution modelling, a general problem with most of these habitat modelling approaches is that the estimated response curves can display biologically implausible shapes which do not respect ecological niche theory. This is because species response curves are fit statistically with any assumption or restriction, which sometimes do not respect the ecological niche theory. To better understand species response to environmental changes, SDMs should consider theoretical background such as the ecological niche theory and pursue the unimodality of the response curve with respect to environmental gradients.

This book provides a tutorial on how to use shape-constrained generalized additive models (SC-GAMs) to build SDMs under the ecological niche theory framework [@citores_etal_2020]. SC-GAMs impose monotonicity and concavity constraints in the linear predictor of the GAMs and avoid overfitting. SC-GAM is an effective alternative to fitting nonsymmetric parametric response curves, while retaining the unimodality constraint, required by ecological niche theory, for direct variables and limiting factors.

The book is organised following the key steps in good modelling practice of SDMs [@elith_etal_2009]. First, presence data of a selected species are downloaded from GBIF/OBIS global public datasets and pseudo-absence data are created. Then, environmental data are downloaded from public repositories and extracted at each of the presence/pseudo-absence data points. Based on this dataset, an exploratory analysis is conducted to help deciding on the best modelling approach. The model is fitted to the dataset and the quality of the fit and the realism of the fitted response function are evaluated. After selecting a threshold to transform the continuous probability predictions into binary responses, the model is validated using a k-fold approach. Finally, the predicted maps are generated for visualization.
   


<!--chapter:end:01-intro.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Libraries

Load libraries that will be used

```{r}
library("scam")
```

Note that all the libraries must be installed. If some library is not installed, run: 

```{r}
install.packages("scam")
```

#NOTA INTERNA: eta zerbait horrela idazten badugu: 

List of required packages to follow this tutorial
```{r}
requiredPackages <- c(
  #GENERAL USE LIBRARIES --------#
  "here", # Library for reproducible workflow
  "rstudioapi",  # Library for reproducible workflow
  "maptools", #plotting world map
  "ggplot2", #for plotting
  
  #Chapter 3 Presence-absence data --------#
  "robis", # Specific library to get the occurrence data
  "rgbif",# Specific library to get the occurrence data
  "CoordinateCleaner", #to remove outlier
  "rgdal", # to work with Spatial data
  "sf", # to work with spatial data (shapefiles)
  "data.table", #for reading data,
  "dplyr", #for reading data,
  "tidyr", #for reading data,
  
  #Chapter 4 Environmental data --------#
  "raster", #to work with spatial data
  
  #Chapter 5 Prepare final dataset --------#
  
  #Chapter 6 Shape Constrained-Generalized Additive Models --------#
  "scam" #to build SC-GAMs models
  )

```

Function to install packages in case is need and load all required packages
```{r}
#Function to install the required packages that are not in your system and load all the required packages
install_load_function <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg))
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}

install_load_function(requiredPackages)
```


<!--chapter:end:02-libraries.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Presence-absence data

In this chapter we first, download occurrence data from global open-access datasets such as Global Biodiversity Information Facility (GBIF, https://www.gbif.org/) and Ocean Biodiversity Information System (OBIS, https://obis.org/); second, clean downloaded data reformating, renaming fields and removing outliers data; and lastly, we generate a set of pseudoabsence points along our study area. 

First we load list required libraries.

```{r, eval=T,message=FALSE,warning=FALSE}
requiredPackages <- c(
  #GENERAL USE LIBRARIES --------#
  "here", # Library for reproducible workflow
  "rstudioapi",  # Library for reproducible workflow
  "maptools", #plotting world map
  "ggplot2", #for plotting
  
  #Presence-absence data chapter--------#
  "robis", # Specific library to get the occurrence data
  "rgbif",# Specific library to get the occurrence data
  "CoordinateCleaner", #to remove outlier
  "rgdal", # to work with Spatial data
  "sf", # to work with spatial data (shapefiles)
  "data.table", #for reading data,
  "dplyr", #for reading data,
  "tidyr" #for reading data
  )
```

Function to install the required packages that are not in your system and load all the required packages
```{r, eval=F}
install_load_function <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg))
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}

install_load_function(requiredPackages)

```

Define some settings
```{r, eval=F}
# location of script
setwd(dirname(getSourceEditorContext()$path))

# general settings for ggplot (black-white background, larger base_size)
theme_set(theme_bw(base_size = 16))
```

## Download presence data

First we need to define our study area, in this case we select the Atlantic ocean based on the The Food and Agriculture Organization (FAO) Major Fishing Areas for Statistical Purposes and we remove Black sea subarea.   
```{r, eval=F}
# url where FAO shapfile is stored
url<-"https://www.fao.org/fishery/geoserver/area/ows?service=WFS&version=1.0.0&request=GetFeature&typeName=area%3AFAO_AREAS&maxFeatures=50&outputFormat=SHAPE-ZIP"

#download file
download.file(url,"data/spatial/FAO_AREAS.zip",mode="wb")

#unzip downloaded file
unzip("data/spatial/FAO_AREAS.zip",exdir="data/spatial")

# Load FAO (spatial multipolygon)
FAO<- st_read(file.path("data", "spatial", "FAO_AREAS.shp"))

#Select Atlantic Ocean FAO Area 
FAO_Atl <- FAO[FAO$OCEAN=="Atlantic",]

#Select Black sea subarea 
Black_Sea <- FAO_Atl[FAO_Atl$ID=="20",]

# transform to sf objects
FAO_Atl.sf <- st_as_sf(FAO_Atl)
Black_Sea.sf <- st_as_sf(Black_Sea)

# Remove Black sea using st_difference (reverse of st_intersection)
FAO_Atl_no_black_sea <- st_difference(FAO_Atl.sf,Black_Sea.sf) %>%   dplyr::select (F_AREA)

#transform to spatial polygons dataframe
study_area<- sf:::as_Spatial(FAO_Atl_no_black_sea)

plot(study_area)
```

Download data from OBIS and GBIF using scientific name. In this case we select Albacore tuna species (<em>Thunnus alalunga</em>).
```{r, eval=F}
# Get data from OBIS
mydata.obis<-robis::occurrence(scientificname="Thunnus alalunga")

# Get data from GBIF
mydata.gbif<-occ_data(scientificName="Thunnus alalunga", hasCoordinate = TRUE, limit=100000)$data
```

We now check the downloaded data and select the fields of interest
```{r, eval=F}
# check names for GBIF data
names(mydata.gbif)
  
#select columns of interest
mydata.gbif <- mydata.gbif %>%
                dplyr::select("acceptedScientificName",
                  "decimalLongitude",
                  "decimalLatitude",
                  "year",
                  "month",
                  "day",
                  "eventDate",
                  "depth")

#check names in for OBIS data
names(mydata.obis)

#select columns of interest
mydata.obis <-  mydata.obis %>%
                dplyr::select("scientificName",
                  "decimalLongitude",
                  "decimalLatitude",
                  "date_year",
                  "month",
                  "day",
                  "eventDate",
                  "depth",
                  "bathymetry",
                  "occurrenceStatus",
                  "sst")
  
```

Reformat the data addin a new field and renaming some columns from mydata.gbif dataframe in order to have the same columns and be able to join both tables

```{r, eval=F}
mydata.gbif <- mydata.gbif %>% 
    dplyr::rename(scientificName= "acceptedScientificName") %>% 
    dplyr::rename(date_year = "year") %>% 
    dplyr::mutate(bathymetry= NA) %>% 
    dplyr::mutate(occurrenceStatus=1) %>% 
    dplyr::mutate(sst= NA)

#Join data from OBIS and GBIF 
mydata.fus<-rbind(mydata.obis,mydata.gbif)
  
#assign unique scientific name 
  mydata.fus <- mydata.fus %>% 
    dplyr::mutate(scientificName= paste(mydata.obis$scientificName[1]))
```

Clean raw data
```{r, eval=F}
#give date format to eventDate and fill out month and date_year columns
  mydata.fus$eventDate <- as.Date(mydata.fus$eventDate)
  mydata.fus$date_year <- as.numeric(mydata.fus$date_year)
  mydata.fus$month <- as.numeric(mydata.fus$month)

#mutate occurrenceStatus column giving value of 1 to presences and 0 to absences
  
unique (mydata.fus$occurrenceStatus)

mydata.fus <- mydata.fus %>% 
    mutate(occurrenceStatus = ifelse(occurrenceStatus== "NA", NA, occurrenceStatus)) %>%
    mutate(occurrenceStatus = ifelse(occurrenceStatus== "Present", 1, occurrenceStatus)) %>% 
    mutate(occurrenceStatus = ifelse(occurrenceStatus== "present", 1, occurrenceStatus)) %>% 
    mutate(occurrenceStatus = ifelse(occurrenceStatus== "Presente", 1, occurrenceStatus)) %>% 
    mutate(occurrenceStatus = ifelse(occurrenceStatus== "Presence", 1, occurrenceStatus)) %>% 
    mutate(occurrenceStatus = ifelse(occurrenceStatus== "P", 1, occurrenceStatus)) %>% 
    mutate(occurrenceStatus = ifelse(occurrenceStatus== "Q", 1, occurrenceStatus))
```

Assign 1 value to all occurrences
```{r, eval=F}
mydata.fus <- mydata.fus  %>% 
    dplyr::mutate(occurrenceStatus = 1)
```

Remove outliers based on distance method (total distance= 1000 km)
```{r, eval=F}

out.dist <- cc_outl(x=mydata.fus,
                lon = "decimalLongitude", lat = "decimalLatitude",
                species = "scientificName",
                method="distance", tdi=1000, # distance method with tdi=1000km
                thinning=T, thinning_res=0.5,
                value="flagged") 

#remove outliers from the data
mydata.fus <- mydata.fus[out.dist, ]
```

Remove duplicates based on the date
```{r, eval=F}

date <- cbind(mydata.fus$decimalLongitude,mydata.fus$decimalLatitude,mydata.fus$eventDate)

mydata.fus<-mydata.fus[!duplicated(date),]
```

Prepare the data to 
```{r, eval=F}
# PREPARE DATA TO USE FAO ATLANTIC REGION MASK
---------------------------------    
    
  # Prepare coordinate format and projection to be able to use FAO zone masks
  dat <- data.frame(cbind(mydata.fus$decimalLongitude,mydata.fus$decimalLatitude))
  ptos<-as.data.table(dat,keep.columnnames=TRUE)
  
  coordinates(ptos) <- ~ X1 + X2
  
  proj4string(ptos) <-proj4string(study_area)
  
  ## Select only occurrences from FAO Atlantic
  match2<-data.frame(subset(mydata.fus,!is.na(over(ptos, study_area)[,1])))
  
  ## Extract the FAO area of each point
  match3<-data.frame(subset(over(ptos, study_area), !is.na(over(ptos, study_area)[,1])))
  
  # create data frame with area, name, lon, lat and year 
  df0<-cbind(F_AREA=match3$F_AREA,match2)[,c("F_AREA","scientificName","decimalLongitude","decimalLatitude","date_year","occurrenceStatus")]
  
  #rename some columns
  names(df0)[3:5]<-c("LON","LAT","YEAR")
  
  # add bathymetry from NOAA
  library(marmap)

  bathy <- getNOAA.bathy(lon1=-100,lon2=30,lat1=-41,lat2=55, resolution = 1, 
                       keep=FALSE, antimeridian=FALSE)

  df0$bathymetry <- get.depth(bathy, df0[,c("LON","LAT")], locator=F)$depth
```


```{r, eval=F}
# SAVE OCCURRENCE DATA
---------------------------------    

#NOTA INTERNA: quiero guardar el RData en la carpeta data/occurrences con el nombre de la especies sin espacios para no machar los RData en caso de que hagamos más de una especie pero no lo consigo. 

# lo dejo como esta, asi luego al cargar los datos no hay que poner el nombre de la especie y es mas sencillo

save(df0, file = "data/occurrences/occ.RData")
save(study_area, file = "data/spatial/total_area.RData")
```


```{r, eval=F}
# PLOT OCCURRENCES MAP
---------------------------------   
ggplot() +
   geom_path(data = study_area, 
             aes(x = long, y = lat, group = group),
             color = 'gray', size = .2) +
   geom_point(data=df0, aes(x=decimalLongitude, y=decimalLatitude,colour= occurrenceStatus)) 
  
```

## Create pseudo-absence data

Prevalence 50%

See code from ANICHO (mantaining some space around presences). Leire C.

Ref [@barbetmassin_etal_2012]

Copio aqui el codigo de anicho tal cual, luego lo limpiaré para este caso:

```{r, eval=F}
# Script information ------------------------------------------------------

# Title: Generate pseudo-absences for IM-18-ANICHO
# Last modified by Leire Ibaibarriaga (libaibarriaga@azti.es) and Leire Citores (lcitores@azti.es)

# Load libraries ----------------------------------------------------------

library(tidyverse)
library(ggplot2)
library(scales)
library(here)
library(ggridges)

library(maps)        # some basic country maps
library(mapdata)     # higher resolution maps
library(mapproj)
library(marmap)      # access global topography data
library(mapplots)    # ices rectangles
library(sf)
library(gridExtra)
library(lubridate)
library(raster)

# load data-files
#occurrence data
load("data/occurrences/occ.RData")
#map Atlantic without black sea
load("data/spatial/total_area.RData")

#remove points in land
df0<-subset(df0,bathymetry<0)

#use years from 2000 to 2014
df0<- subset(df0, YEAR<=2014 & YEAR>=2000)

# Remove points in south hemisphere (optional)
#prop.table(table(df0$LAT<=0)) #only 22% of obs in the south
#df0<-subset(df0,LAT>0)

# Remove south hemisphere area
#area <- crop(study_area, extent(-100, 100, 0, 100))
area <- study_area

#convert to spatial point data frame
df<-df0 ; coordinates(df)<- ~LON+LAT
crs(df)<-crs(area)

#convert to sf
area.sf<-st_union(st_as_sf(area))
df.sf<-st_as_sf(df)

ggplot(area.sf) + geom_sf() + geom_sf(data=st_union(df.sf))


dim(df) # 15079    5
head(df)
tail(df)
summary(df)
```


```{r, eval=F}
# Maps --------------------------------------------------------------------

# basic ggplot

global <- map_data("worldHires")

p0 <- ggplot() + 
  annotation_map(map=global, fill="grey")+
  geom_sf(data=area.sf,fill=NA)
print(p0)

# EGSP: Transformation to UTM ---------------------------------------------

# function to find your UTM. Taken from Nerea Goikoetxea

lonlat2UTM = function(lonlat) {
  utm = (floor((lonlat[1] + 180) / 6) %% 60) + 1
  if(lonlat[2] > 0) {
    utm + 32600
  } else{
    utm + 32700
  }
}

EPSG_2_UTM <- lonlat2UTM(c(mean(df$LON), mean(df$LAT))) 
EPSG_2_UTM # 32623

# transform to UTMs (in m)
aux <- st_transform(area.sf, EPSG_2_UTM)
df.sf.utm <- st_transform(df.sf, EPSG_2_UTM)

# number of points within the study area

df.in <- st_intersects(df.sf.utm, aux, sparse=FALSE)
df$INSIDE <- df.in[,1]
mean(df$INSIDE) # 100% points inside the area of study

# create buffers of 10km (10000) around the points and join the resulting polygons

buffer <- st_buffer(df.sf.utm, dist=100000)
buffer <- st_union(buffer)

p0 + geom_sf(data=buffer)

p <- p0 +
  geom_sf(data=buffer, fill="blue")+
  coord_sf(xlim=c(-90,-82), ylim=c(25,31))
print(p)

# intersect the result with the buffers around the catch data points
aux0 <- st_difference(aux, buffer)

# ggplot for all data

p <- p0 +
  geom_sf(data=aux0, fill=2)+
  coord_sf(xlim=c(-95,-82), ylim=c(25,31))
print(p)

p<-p0 + geom_sf(data=aux0,fill=2)

ggsave(file.path("plots","pseudo",paste0("area_pseudo_all.png")), p, device="png")
```


```{r, eval=F}
# Generate pseudo-absences -------------------------------------------------


# Generate the pseudo-absence data frame

pseudo <- matrix(data=NA, nrow=dim(df0)[1], ncol=dim(df0)[2])
pseudo <- data.frame(pseudo)
names(pseudo) <- names(df0)

# set the seed

set.seed(1)

##no loop

buffer.sub <- st_buffer(df.sf.utm, dist=100000)
buffer.sub <- st_union(buffer.sub)
aux0.sub <- st_difference(aux, buffer.sub)
rp.sf <- st_sample(aux0.sub, size=dim(df.sf.utm)[1], type="random") # randomly sample points
rp.sf <- st_transform(rp.sf, 4326)
rp <- as.data.frame(st_coordinates(rp.sf)) # transform to lat&lon and extract coordinates as data.frame
pseudo$LON <- rp$X
pseudo$LAT <- rp$Y
p <- p0 +
  geom_sf(data=aux0.sub, fill=2, alpha=0.3)+
  geom_sf(data=rp.sf, col=1, shape=4,size=0.5)+
  geom_sf(data=df.sf.utm, col=4, alpha=0.3)+
  ggtitle(paste(unique(df$scientificName),"all"))
ggsave(file.path("plots","pseudo",paste0("pseudo_","all",".png")), p, device="png")


p <- p0 +
  geom_sf(data=aux0.sub, fill=NA, alpha=0.3)+
  geom_sf(data=df.sf.utm, col=4, alpha=0.3,size=0.5)+
  geom_sf(data=rp.sf, col=1, shape=4,size=0.5)+
  coord_sf(xlim=c(-98,-79), ylim=c(15,31))+
  ggtitle(paste(unique(df$scientificName),"all"))
ggsave(file.path("plots","pseudo",paste0("pseudo_","all_zoom",".png")), p, device="png")


# complete the rest of columns

pseudo$scientificName <- df0$scientificName
pseudo$occurrenceStatus  <- 0
```


```{r, eval=F}
# Save the final dataset including the pseudo-absences --------------------

head(df0)
head(pseudo)

# Join the two data sets and save the final dataset

dat <- rbind(df0, pseudo)
save(list=c("dat"),file=file.path("data","outputs_for_modelling",file="PAdata.RData"))
save(list=c("area"),file=file.path("data","spatial",file="area.RData"))
write.table(dat, file=file.path("data","outputs_for_modelling","PAdata.csv"), row.names=F, sep=";", dec=".")

```




<!--chapter:end:03-data_pa.Rmd-->

# Environmental data

Loading required libraries

```{r, eval=T, warning=F, message=F}
### Download Environmental data from Bio-oracle

# # for mapping ...
# library(maptools)
# library(rgrass7)
# library(raster)
# library(sp)
# library(ggplot2)
# library(maps)
# library(rgbif)
# library(dataverse)
# library(rstudioapi)

# libraries for bio-oracle
# library(rgdal)
#library(sdmpredictors) 
# library(leaflet)

library(knitr) # format tables
library(kableExtra) # format tables

# http://bio-oracle.org/code.php
#install.packages("sdmpredictors")
#install.packages("leaflet")

# location of script
#setwd(dirname(getSourceEditorContext()$path))
```

## Download from public repositories

Environmental data can be available from different sources. In this case, we used the Bio-ORACLE (ocean
rasters for analyses of climate and environment) database [@tyberghein_etal_2012; @assis_etal_2017]. These data are publicly available and are easily accessible from the `sdmpredictors` package [`sdmpredictors`](https://cran.r-project.org/web/packages/sdmpredictors/index.html). 

After loading the `sdmpredictors` package, we can check the list of available datasets with the function `list_datasets` as follows: 

```{r, warning=F, message=F}
library(sdmpredictors) 

mydat <- list_datasets()

kable(mydat)%>% 
  kable_styling("striped") %>% 
  scroll_box(height="600px", width = "100%")
```

By default this function returns all the supported datasets. To return only marine datasets we can set the `marine` argument equal to `TRUE` or equivalently we could set the `terrestrial` and `freshwater` arguments equal to `FALSE`:

```{r}
mydat <- list_datasets(marine=T)
# or equivalently: 
# mydat <- list_datasets(terrestrial=F, freshwater=F)
# mydat <- list_datasets(marine=T, terrestrial=F, freshwater=F)

kable(mydat)%>% 
  kable_styling("striped") %>% 
  scroll_box(height="600px", width = "100%")
```

There are two datasets (Bio-ORACLE and MARSOEC) that have marine data. The function `list_layers` returns information on the layers of one or more datasets. So, we can see the layers available in the Bio-ORACLE dataset as follows:

```{r}
mytab <- list_layers("Bio-ORACLE")

kable(mytab)%>% 
  kable_styling("striped") %>% 
  scroll_box(height="600px", width = "100%")
```

Once we identify the dataset and the layers we are interested on, we can download them using the function `load_layers`. In this case, we download data on chlorophyll, salinity, diffuse attenuation coefficient, surface temperature and bathymetry:  

```{r}
myBioracle.layers <- load_layers(c("BO2_chlomean_ss", "BO2_salinitymean_ss", "BO_damean" ,"BO_sstmean", "BO_bathymean")) 
```

Note that the resulting object is a `rasterStack`, where each variable is a layer

```{r}
class(myBioracle.layers)
myBioracle.layers
``` 

And we can plot it:

```{r, warning=F, message=F}
library(raster)
raster::plot(myBioracle.layers)
```

In case we are not interested on the whole area, we can crop the raster objects to the area of interest. 

For example, we can load the `area` object that is a SpatialPolygonsDataFrame that has been created previously and defines the extent of our spatial data and we can crop the rasterStack to the same extent:

CHANGE HERE THE tmp NAME TO SOMETHING BETTER:

```{r, warning=F, message=F}
load("data/spatial/area.RData")
tmp <- crop(myBioracle.layers, extent(area))
plot(tmp)
```

To facilitate subsequent access, the rasterStack with the downloaded data is saved in a local folder: 

```{r, eval=T}
save(myBioracle.layers, file="data/env/myBioracle.layers.Rdata")
```

Note that the functions `dataset_citations` and `layer_citations` provide the bibliographic entries of the datasets and layers for proper citation:

```{r}
print(dataset_citations("Bio-ORACLE"))
print(layer_citations("BO2_chlomean_ss"))
```




<!--chapter:end:04-data_env.Rmd-->

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE, fig.width=16, fig.height=12)
```

# Prepare final dataset

Loading required libraries

```{r, eval=T}
# library(rgdal) # to read shp
# library(raster) # to work with rasters
# library (here) # Libraries for reproducible workflow
# library (sp)     # to perform raster
# library(HH) #to calculate for VIF (Variance Inflation Factor)
# library(ggplot2)# plots


# library(gridExtra)# multiple plots
# library(dplyr)
# library(RColorBrewer)#plotting colors
# library(fields)#ploting images
# library(marmap)#ploting world
# library(maptools)#mapping
# library(rstudioapi)

# theme_set(theme_bw(base_size = 16))
```

## Extract environmental data associated to presence-absence data

Once the environmental rasters and the species abundance data (including pseudo-absences) have been prepared, we need to merge both sources of data. First, we load the objects created in previous sections:

```{r, eval=T}
#load presence-absence data
load("data/outputs_for_modelling/PAdata.RData")

# load environmental rasters
library(sp)
library(raster)
load(file="data/env/myBioracle.layers.Rdata") 
```

Now we can extract the environmental data associated to each of the species data points using the function `extract` from the `raster` package. The method employed is `bilinear` that returns the interpolated value from the four nearest raster cells.

CHECK THE NAMES THROUGHOUT!! dat, data... mydata1.env.bil TOO LONG... 
I'm HAVING ERRORS WITH THIS. ONLY WORKS EXECUTED AFTER THE OTEHR... WHY?

```{r, eval=F}
mydata1.env.bil <- raster::extract(x=myBioracle.layers, y=dat[,c("LON","LAT")], method="bilinear", na.rm=TRUE, df=T) 
head(mydata1.env.bil)
```

We merge the presence/absence data and the environmental data: 

```{r, eval=F}
data <- cbind(dat, mydata1.env.bil)
```

We can conduct some quick checks on the new dataset:  

```{r, eval=F}
dim(data)
str(data)
head(data)
summary(data)
```

ZER DA bathymetry ZUTABEA? AZALDU BEHAR DA??
```{r, eval=F}
# LI'k GEHITUTAKO KODIGOA. KENDU?
plot(data$BO_bathymean, data$bathymetry)
abline(0,1, col=2)
```

The new dataset has 30158 rows and 13 columns, and there are 162 NA's in the environmental dataset. Note also that there are some positive bathymetries that correspond to points located very close to the coast. We remove both the points with NA's and the points with positive bathymetries:

```{r, eval=F}
data <- subset(data, BO_bathymean<0)
```

We check again the dataset:

```{r, eval=F}
dim(data)
summary(data) # 29435 sites : 4 are NAs
```

The resulting dataset has 29435, from which 4 have NA's in BO2_chlomean_ss and BO2_salinitymean_ss. We save this dataset in a local file to work on it in subsequent steps. 

LI: REMOVE THE 4 NA's?? IT WILL FACILITATE MODEL COMPARISON

```{r, eval=F}
save(list="data", file="data/outputs_for_modelling/PAdata_with_env.RData")
```

## Exploratory plots of environmental variables

LI: This first chunk can be removed, but I'm having problems with the first subsection

```{r, eval=T}
load("data/outputs_for_modelling/PAdata_with_env.RData")
```

Before starting the modelling process, we are going to explore the individual variables in the dataset. 

The `YEAR` variable is only available for the presence data and the time coverage is quite different along time as shown by the following plot: 

```{r}

table(is.na(dat$YEAR), dat$occurrenceStatus) # NOTE THAT THE PSEUDO-ABSENCES DO NOT HAVE YEAR. So MAYBE BETTER TO REMOVE THIS PART.

library(dplyr)
library(tidyr)
library(ggplot2)
theme_set(theme_bw(base_size=16))

data %>% 
  # filter(!is.na(YEAR)) %>% 
  group_by(YEAR) %>% 
  summarise(N=n()) %>% 
  ggplot(aes(x=YEAR, y=N)) + 
  geom_bar(stat="identity", col="red", fill="red", alpha=0.5)
```

We can explore the distributions of each of the environmental variables by looking at the violin and boxplots and at the histograms and density plots as follows: 

```{r}
tmp <- data[, c("BO2_chlomean_ss","BO2_salinitymean_ss","BO_damean","BO_sstmean","BO_bathymean")]
tmp <- pivot_longer(data=tmp, cols=everything()) 

ggplot(data=tmp, aes(x=name, y=value)) + 
  geom_boxplot()+
  facet_wrap(~name, scales="free")

ggplot(data=tmp, aes(x=name, y=value)) + 
  geom_violin(fill="red", alpha=0.3)+
  geom_boxplot(width=0.1)+
  facet_wrap(~name, scales="free")

ggplot(data=tmp, aes(x=value)) + 
  geom_histogram(aes(y= ..density.. ), colour=1, fill="red", alpha=0.3)+
  geom_density(lwd=1)+
  facet_wrap(~name, scales="free")

```

## Exploratory plots of environmental variables depending on presence/absence data

To analyse if there are preferences for certain ranges of the environmental variables, we compare the distribution of the environmental variables for presence and absence data:   

```{r}

tmp <- data[, c("LON", "LAT", "BO2_chlomean_ss","BO2_salinitymean_ss","BO_damean","BO_sstmean","BO_bathymean","occurrenceStatus")]
tmp <- pivot_longer(data=tmp, cols=!occurrenceStatus) 

ggplot(data=tmp, aes(x=factor(occurrenceStatus), y=value, fill=factor(occurrenceStatus), group=factor(occurrenceStatus))) + 
  geom_violin(alpha=0.3)+
  geom_boxplot(fill="white", width=0.1)+
  facet_wrap(~name, scales="free")+
  theme(legend.position = "bottom",legend.background = element_rect(fill = "white", colour = NA))

ggplot(data=tmp, aes(x=value, fill=factor(occurrenceStatus), group=factor(occurrenceStatus))) + 
  geom_density(lwd=1, alpha=0.3)+
  facet_wrap(~name, scales="free")+
  theme(legend.position = "bottom",legend.background = element_rect(fill = "white", colour = NA))
```

## Correlation analysis

Some of the environmental variables can be correlated. The `GGally` package allows to easily produce pairplots of the variables and their correlation.  

```{r}
library(GGally)

tmp <- data[, c("LON","LAT","BO2_chlomean_ss","BO2_salinitymean_ss","BO_damean","BO_sstmean","BO_bathymean")]
ggpairs(tmp)

```

A more detailed analysis of the potential correlations can be conducted using the package `ggcorrplot`:

```{r}
library(ggcorrplot)

tmp <- data[, c("LON","LAT","BO2_chlomean_ss","BO2_salinitymean_ss","BO_damean","BO_sstmean","BO_bathymean")]

mat <- cor(tmp, use="complete.obs") 
p.mat <- cor_pmat(tmp)

ggcorrplot(mat, type = "lower", lab=T, p.mat = p.mat)
```

## Variance Inflation Factor (VIF) 

Furthermore, multicollinearity in regression analysis can be explored using the VIF (Variance Inflation Factor). The value of the VIF statistics indicate the level of multicollinearity with the rest of the variables:

* VIF equal to 1 = variables are not correlated
* VIF between 1 and 5 = variables are moderately correlated 
* VIF greater than 5 = variables are highly correlated

There are several packages in `R` that allows to calculate the VIF statistics. In this case we use the package `HH`: 

```{r, eval=T}

library(HH)

# select variables for vif calculation
v.table <- data %>% 
  dplyr::select (BO2_salinitymean_ss, BO_sstmean, BO2_chlomean_ss, BO_damean, BO_bathymean)

# get vif results

out.vif <- vif(v.table)
sort(out.vif)
```

We remove the variable that has the highest VIF value and we test again the multicollinearity: 

```{r}

v.table <- v.table %>% 
  dplyr::select (-BO_damean)

#  get new vif results

out.vif <- vif(v.table)
sort(out.vif)
```

Now all the variables have VIF values that are acceptable. So, we proceed to save these variables for the next modelling stages:

```{r}
data <- data %>% dplyr::select (-BO_damean)

# save(list="data", file="data/outputs_for_modelling/PAdata_with_env.RData")
```



<!--chapter:end:05-data_final.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Shape Constrained-Generalized Additive Models


In order to fit SDM in agreement with the ecological niche theory, the proposed shape-constrained GAMs in [@citores_etal_2020] are fitted in this section. SC-GAMs are based on generalized additive models, allowing us to impose shape-constraints to the linear predictor function. The R package SCAM implements the general framework developed by [@pya_etal_2015] using shape-constrained P-splines. Monotonicity and concavity/convexity constraints can be imposed on the sign of the first and/or the second derivatives of the smooth terms. For fitting species distribution models in agreement with the ecological niche theory, we imposed concavity  constraints ($f''(x) \le 0$), so that the response can presents at most a single mode.  

Alternatively, the R package `mboost` fits SC-GAMs using boosting methodos. This alternative won't be further developed here.

First we load all required libraries.

```{r, eval=T,message=FALSE,warning=FALSE}
library(scam)
library(plotmo)
library(rgdal)
library(ggplot2)
library(dplyr)
library(fields)
library(maps) 
library(raster)
library(RColorBrewer)
library(SDMTools)
library(dismo)
library(stringr)
library(rstudioapi)
```


## Model fit

We set the working directory to the folder where the current script is located and we load the dataset (PAdata_with_env.Rdata) containing the presence-absence data toghether with the environmental data.

```{r, eval=T}
setwd(dirname(getSourceEditorContext()$path))

load(file.path ("data", "outputs_for_modelling", "PAdata_with_env.Rdata"))
```

To fit a logistic regression model in the SC-GAMs framework, we use the scam function, where we set the binomial family with the logit link function. Our response variable is the presence-absence data and the the selected three explanatory variables are the SST, chlorophyll and salinity. Each variable is included in the model through an spline function where the concavity constraint is set using bs="cv". The details about this option can be found in the section "Constructor for concave P-splines in SCAMs" of the SCAM manual (https://cran.r-project.org/web/packages/scam/scam.pdf). The number of knots (k) is fixed at 8 in this example for a good balance between flexibility and computation time.

**UNIVARIATE MODELS**

Before fitting the model with the selected three environmental variables, we can fit univariate model as follows.

We fit the univariate model for SST, we print the summary of the model fit, and look at the fitted curve in the response scale.

```{r, eval=T,message=FALSE,warning=FALSE}
model_sst <- scam (occurrenceStatus ~  s(BO_sstmean, k=8,bs="cv"), family=binomial(link="logit"), data=data)
summary(model_sst)
plotmo(model_sst,level = 0.95, pt.col=8)
```

We repeat the same for the rest of the variables.

```{r, eval=T,message=FALSE,warning=FALSE}
model_chl <- scam (occurrenceStatus ~  s(BO2_chlomean_ss, k=8,bs="cv"), family=binomial(link="logit"), data=data)
summary(model_chl)
plotmo(model_chl,level = 0.95, pt.col=8)
```


Due to convergence issues, sometimes it is necessary to fix the smoothing parameter (sp) at $10^{-5}$, as here when introducing salinity as an explanatory variable. If no value is provided, the smoothing parameter is estimated within the model.

```{r, eval=T,message=FALSE,warning=FALSE}
model_sal <- scam (occurrenceStatus ~  s(BO2_salinitymean_ss, k=8,bs="cv"), family=binomial(link="logit"), data=data,sp=0.00001)
summary(model_sal)
plotmo(model_sal,level = 0.95, pt.col=8)
```


**MODEL WITH ALL VARIABLES**

Now we fit the model including the three selected variables.

```{r, eval=T,message=FALSE,warning=FALSE}
model <- scam (occurrenceStatus ~  s(BO_sstmean, k=8,bs="cv")+ s(BO2_salinitymean_ss, k=8,bs="cv")+s(BO2_chlomean_ss, k=8,bs="cv"), family=binomial(link="logit"), data=data,sp=rep(0.00001,3))
summary(model)
plotmo(model,level = 0.95, pt.col=8)
```

We can see in the summary of the fit, that all included variables are statistically significant (with p<0.05) and present a unimodal response curve.

For a more detailed check of the fitting, we can use the scam.check function:

```{r, eval=T,message=FALSE,warning=FALSE}
scam.check(model)
```

Finally, we save the model.

```{r, eval=T,message=FALSE,warning=FALSE}
selected_model<-model
save(list=c("selected_model"), file = ("models/model.RData"))
```


## Model selection

When several explanatory variables are available, a variable selection process can be carried out. Here we provide as an example, a function that performs forward variable selection (modsel.scam) based on the significance of variables and AIC values of the fits. 

```{r, eval=T,message=FALSE,warning=FALSE}
source("function/function_scam_selection_optimized.R")
```

We save the names of the variables we want to introduce for the variable selection process as a vector:

```{r, eval=T,message=FALSE,warning=FALSE}
vars <- c("BO_sstmean",
          "BO2_salinitymean_ss",
          "BO2_chlomean_ss")
```

The default AIC tolerance is 2 and there is not a limit on selected terms in this example. These options can be modified through aic.tol and vmax arguments in the function. The number of knots and the sp can be also modified.

```{r, eval=T,message=FALSE,warning=FALSE}
model_SCGAM <- try(modsel.scam(basef="occurrenceStatus ~ 1", vars=vars, dat=data,sp=0.00001), silent=T)  
```

We check results of the selected model, such as, selected variable names:
```{r, eval=T,message=FALSE,warning=FALSE}
model_SCGAM$svars
```

AICs of the fitted models:
```{r, eval=T,message=FALSE,warning=FALSE}
sapply(model_SCGAM$smod, AIC)
```

Explained deviance of fitted models:
```{r, eval=T,message=FALSE,warning=FALSE}
sapply(model_SCGAM$smod, function(x) summary(x)$dev.expl)
```

Formulas of the fitted models:
```{r, eval=T,message=FALSE,warning=FALSE}
lapply(model_SCGAM$smod, formula)
```

Summaries of the fitted models:
```{r, eval=T,message=FALSE,warning=FALSE}
lapply(model_SCGAM$smod, summary)
```

The last model of the list, is the selected one. Which in this case contains the considered three variables.

```{r, eval=T,message=FALSE,warning=FALSE}
selected_model <- model_SCGAM$smod[[length(model_SCGAM$smod)]]
```

Note that there are multiple options and criteria for model selection that are not reviewed here. Any model selection technique used for GAMs can be also for SC-GAMs.

<!--chapter:end:06-model_fit.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---


# Model validation

Bla bla 


Loading required libraries

```{r, eval=F}
library(scam)
#library(mgcv)
library(plotmo)
library(rgdal)
library(ggplot2)
library(dplyr)
library(fields) #imageplot
library(maps) #map world
library(raster)
library(RColorBrewer)#color palette
library(tidyverse)
library(R.utils) #loadToEnv

#SDMTools - install RTools and follow: https://cran.r-project.org/bin/windows/Rtools/
#install.packages("remotes")
#remotes::install_version("SDMTools", "1.1-221.2")
library(SDMTools)
library(dismo)

#plots
library(ggplot2)
library(ggpubr)
library(hrbrthemes) # theme_ipsum
library(rstudioapi)
```

## Optimum threshold

```{r, eval=F}

# location of script
setwd(dirname(getSourceEditorContext()$path))

#NOTA INTERNA: aquí hay que cargar el RData del modelo, cargo el que se crea con el 06-model_fit para poder correr el código

#Load SC-GAM model

load(file.path ("models", "model.Rdata"))

#Load species PA data

#load(here::here ("data", "outputs_for_modelling", "PAdata_with_env.Rdata"))
#data <- dat.all #NOTA INTERNA: así se llama el RData 
data<-selected_model$model

  # Predict 
  scgam.pred <- predict(selected_model, newdata=data, type="response")
  
  # Add the prediction to the table
  data$scgam.pred <- as.vector(scgam.pred)
  head(data)

  ## Optimizing the threshold probability
  obs <- data$occurrenceStatus
  predSCGAM_P <- data$scgam.pred
  
  # threshold optimizing
  myoptim <- optim.thresh (obs,predSCGAM_P)
  myoptim

```


## k-fold validation

VALIDATION: https://rpubs.com/mlibxmda/SDMPartOne

Create a df to save validation results
```{r, eval=F}
validation_summary <- matrix(NA, ncol = 6, nrow = 1)
```


```{r, eval=F}
  # select the threshold that maximizes the sum of sensitivity and specificity (Jimenez-Valverde and Lobo, 2017)
  myThreshold <- as.numeric((myoptim[["max.sensitivity+specificity"]])) 
  
  # In case, the result is a range (instead of a single value), we select the mean value of the range 

  if(length(myThreshold)>1){
    print(paste("Note that the threshold is the mean of the range:", myThreshold))
    myThreshold <- mean (myThreshold) 
  } 
  
  # accuracy values with all observations
  accuracy (obs, predSCGAM_P, threshold=myThreshold)
  
  # create confusion matrix with all observations
  confusion.matrix(obs, predSCGAM_P, threshold=myThreshold)
  
  # isolate the formula  
  v_summary <- summary(selected_model)
  
  formula <- v_summary[["formula"]]
  
  # Generating K Groups of presences for k-fold cross-validation
  
  # cross-validation using k-fold
  k <- 5 # Number of groups

  #generate groups
  groups<-kfold(data, k, by=data$status)
  
  
  # initialise the confusion matrix and the accuracy table: 
  myCM <- NULL 
  myACC <- NULL

  # loop for each group k
  
  for (j in 1:k) {
    
    print(paste("Results k-fold validation: group ", j))
    
    # Preparation of Training Sites

    puntos_Training <- data[groups != j,]
    
    # Model fit
    sp <- selected_model$sp
    
    selected_model.sp.j <- scam (formula, family=binomial(link="logit"), 
                        data=puntos_Training, sp=c(sp))
    summary(selected_model.sp.j)
    
    # Predict Model
    puntos_validacion<-data[groups == j,]
    
    selected_model.sp.j.pred <- predict(selected_model.sp.j, newdata=puntos_validacion, type="response")
    puntos_validacion$Pred <- selected_model.sp.j.pred
    
    # Confussion matrix and accuracy table for fold j
    obs <- puntos_validacion$occurrenceStatus
    predSCGAM <- puntos_validacion$Pred
    # print(confusion.matrix(obs, predSCGAM, threshold=myThreshold))
    # print(accuracy(obs, predSCGAM, threshold=myThreshold))
    myCM <- rbind(myCM, as.numeric(confusion.matrix(obs, predSCGAM, threshold=myThreshold))) # columns are: obs0pred0, obs0pred1, obs1pred0, obs1pred1
    myACC <- rbind(myACC, accuracy(obs, predSCGAM, threshold=myThreshold))
  } # end of loop for j
  
  # save threshold
  Threshold <- myThreshold
  
  # Mean values across k-folds
  mean_AUC <- mean(myACC$AUC)
  mean_Omision <- mean(myACC$omission.rate)
  mean_sensitivity <- mean(myACC$sensitivity)
  mean_specificity <- mean(myACC$specificity)
  mean_Prop.Corr <- mean(myACC$prop.correct)  
  
  # add mean values to the validation summary table for species
  validation_summary[1,]<-c(Threshold,
                            mean_AUC,
                            mean_Omision,
                            mean_sensitivity,
                            mean_specificity,
                            mean_Prop.Corr)
  
  validation_summary.df <- as.data.frame(validation_summary)

  names (validation_summary.df) <- c("Threshold", "mean_AUC", "mean_Omision", "mean_sensitivity","mean_specificity", "mean_Prop.Corr")

  validation_summary.df
  
  save(validation_summary.df, file = here::here("models/validation_summary.RData"))
```


<!--chapter:end:07-model_validation.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Prediction and maps

predict from fitted models and produce maps

Loading required libraries

```{r, eval=F}

library(rgdal)
library(ggplot2)
library(dplyr)
library(fields) #imageplot
library(maps) #map world
library(raster)
library(RColorBrewer)#color palette
library(tidyverse)
library(rstudioapi)

# general settings for ggplot (black-white background, larger base_size)
theme_set(theme_bw(base_size = 16))
```

## Prepare environmental data. 04 ATALETIK HONA MUGITUTA

In previous steps (see section XX), we have defined the polygon that defines the extent of our spatial data. We load the `area` object that is a SpatialPolygonsDataFrame class:

```{r, eval=T, warning=F, message=F}
load("data/spatial/area.RData")
```

And we load the rasterStack with the downloaded environmental data

```{r}
load("data/env/myBioracle.layers.Rdata")
```

We transform the environmental data set first into a data frame, and then into a SpatialDataFrame   

```{r, eval=F}
env_dataframe <- raster::as.data.frame(myBioracle.layers, xy=TRUE)
pts <- env_dataframe[,c("x","y")]

coordinates(pts) <- ~ x + y
proj4string(pts) <-proj4string(area)
```

```{r, eval=F}
  
## Select only occurrences in the defined previously selected area
env_dataframe <- data.frame(subset(env_dataframe, !is.na(over(pts, area)[,1])))
```

```{r, eval=F}

#save the data frame
save(env_dataframe, file="data/env/env_dataframe.Rdata")
```



## Projection

```{r, eval=F}

# location of script
setwd(dirname(getSourceEditorContext()$path))

#load PA data
load(file.path ("data", "outputs_for_modelling", "PAdata_with_env.Rdata"))

#Load raster stack containing all variables

load("data/env/env_dataframe.Rdata")

#remove positive bathy
env_dataframe_proj<-subset(env_dataframe,BO_bathymean<0)

#Load SC-GAM model

load(file.path("models", "model.Rdata"))

# predicting 
predict <- predict(selected_model,newdata=env_dataframe_proj,type ="response",se.fit=T)         
env_dataframe_proj$fit<-predict$fit
env_dataframe_proj$se.fit<-predict$se.fit


```


## Mapping

```{r, eval=F}
p<-ggplot()+geom_raster(data=subset(env_dataframe_proj),aes(x,y,fill=fit))+scale_fill_gradient2(low="blue", mid="orange",high="red",midpoint = 0.5) +ggtitle("with bathy concave")+ geom_point(data=subset(data,occurrenceStatus==1),aes(LON,LAT),col=1,size=0.3)     

print(p)

ggsave(file.path("plots","projections",paste0("proj_map.png")), p, device="png")

save(env_dataframe_proj, file="results/projection.Rdata")

```


<!--chapter:end:08-projections.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:09-references.Rmd-->

