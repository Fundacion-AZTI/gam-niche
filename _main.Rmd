--- 
title: "GAM-NICHE: Shape-Constrained GAMs to build Species Distribution Models under the ecological niche theory"
author: "AZTI"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography:
- references.bib
description: |
  This is a short tutorial for constructing species distribution models in R using shape-constrained generalized additive models, based on the development and application to marine fish by @citores_etal_2020.  
link-citations: yes
github-repo: Fundacion-AZTI/gam-niche
---


# About {-}

This is a short tutorial for constructing Species Distribution Models in R using Shape-Constrained Generalized Additive Models, based on the development and application to marine fish by @citores_etal_2020.

The code is available in [AZTI's github repository](https://github.com/Fundacion-AZTI/gam-niche) and the book is readily available [here](https://fundacion-azti.github.io/gam-niche/).This work is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/)

<img src="images/by-nc-sa.eu.png" width="200">

<!-- ![](images/by-nc-sa.eu.png) -->

To cite this book, please use:

Valle, M., Citores, L., Ibaibarriaga, L., Chust, C. (2023) GAM-NICHE: Shape-Constrained GAMs to build Species Distribution Models under the ecological niche theory. AZTI. https://doi.org/10.57762/fzpy-6w51

<!-- the code below is to have an image over the book title -->
```{js, echo = FALSE}
title=document.getElementById('header');
title.innerHTML = '<img src="images/grafica_GAM_NICHE.png" alt="Test Image">' + title.innerHTML
```

<!--chapter:end:index.Rmd-->

<!-- 
This file is part of a gitbook that should be cited as: 

Valle, M., Citores, L., Ibaibarriaga, L., Chust, C. (2023) GAM-NICHE: Shape-Constrained GAMs to build Species Distribution Models under the ecological niche theory. AZTI. https://doi.org/10.57762/fzpy-6w51 

This tutorial has been supported by the European Union’s Horizon 2020 research and innovation programme under grant agreements No 862428 MISSION ATLANTIC project
-->

# Introduction

Species Distribution Models (SDMs) are numerical tools that combine observations of species occurrence or abundance at known locations with information on the environmental and/or spatial characteristics of those locations [@elith_etal_2009]. SDMs are widely used as a tool for understanding species spatial ecology and are also known as ecological niche models (ENM) or habitat suitability models.

According to ecological niche theory, species response curves are unimodal with respect to environmental gradients [@hutchinson_1957]. While a variety of statistical methods have been developed for species distribution modelling, a general problem with most of these habitat modelling approaches is that the estimated response curves can display biologically implausible shapes which do not respect ecological niche theory. This is because species response curves are fit statistically with any assumption or restriction, which sometimes do not respect the ecological niche theory. To better understand species response to environmental changes, SDMs should consider theoretical background such as the ecological niche theory and pursue the unimodality of the response curves with respect to environmental gradients.

This book provides a tutorial on how to use Shape-Constrained Generalized Additive Models (SC-GAMs) to build SDMs under the ecological niche theory framework [@citores_etal_2020]. SC-GAMs impose monotonicity and concavity constraints in the linear predictor of the GAMs and avoid overfitting. SC-GAM is an effective alternative to fitting nonsymmetric parametric response curves, while retaining the unimodality constraint, required by ecological niche theory, for direct variables and limiting factors.

The book is organised following the key steps in good modelling practice of SDMs [@elith_etal_2009]. First, presence data of a selected species are downloaded from GBIF/OBIS global public datasets and pseudo-absence data are created. Then, environmental data are downloaded from public repositories and extracted at each of the presence/pseudo-absence data points. Based on this dataset, an exploratory analysis is conducted to help deciding on the best modelling approach. The model is fitted to the dataset and the quality of the fit and the realism of the fitted response function are evaluated. After selecting a threshold to transform the continuous probability predictions into binary responses, the model is validated using a k-fold approach. Finally, the predicted maps are generated for visualization.
   


<!--chapter:end:01-intro.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---

<!-- 
This file is part of a gitbook that should be cited as: 

Valle, M., Citores, L., Ibaibarriaga, L., Chust, C. (2023) GAM-NICHE: Shape-Constrained GAMs to build Species Distribution Models under the ecological niche theory. AZTI. https://doi.org/10.57762/fzpy-6w51 

This tutorial has been supported by the European Union’s Horizon 2020 research and innovation programme under grant agreements No 862428 MISSION ATLANTIC project
-->

# Presence-absence data

In this chapter we first, download occurrence data from global open-access datasets such as Global Biodiversity Information Facility (GBIF, https://www.gbif.org/) and Ocean Biodiversity Information System (OBIS, https://obis.org/); second, clean downloaded data reformating, renaming fields and removing outliers data; and lastly, we generate a set of pseudoabsence points along the defined study area. 

First we load a list of required libraries.
```{r, eval=T,message=FALSE,warning=FALSE}
requiredPackages <- c(
  #GENERAL USE LIBRARIES --------#
  "here", # Library for reproducible workflow
  "rstudioapi",  # Library for reproducible workflow
  "maptools", #plotting world map
  "ggplot2", #for plotting
  
  #Download presence data--------#
  "robis", # Specific library to get the occurrence data
  "rgbif",# Specific library to get the occurrence data
  "CoordinateCleaner", #to remove outlier
  "rgdal", # to work with Spatial data
  "sf", # to work with spatial data (shapefiles)
  "data.table", #for reading data,
  "dplyr", #for reading data,
  "tidyr", #for reading data
  "marmap", #bathymetry getNOAA.bathy remotes::install_github("ericpante/marmap")
  
  #Create pseudo-absence data--------#
  "tidyverse", 
  "scales",
  "ggridges",
  "maps",     # some basic country maps
  "mapdata",   # higher resolution maps
  "mapproj",
  "mapplots",   # ICES rectangles
  "gridExtra",
  "lubridate",
  "raster" # to work with Spatial data
    )
```

We run a function to install the required packages that are not in our system and load all the required packages.
```{r, eval=T,message=FALSE,warning=FALSE}
install_load_function <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg))
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}

install_load_function(requiredPackages)

```

We define some overall settings.
```{r, eval=T,message=FALSE,warning=FALSE}
# General settings for ggplot (black-white background, larger base_size)
theme_set(theme_bw(base_size = 16))
```

## Download presence data

In this section we download presence data from global public datasets. 

To do so, we first define a study area, in this case we select the Atlantic Ocean based on the The Food and Agriculture Organization (FAO) Major Fishing Areas for Statistical Purposes and we remove Black Sea subarea.   
```{r, eval=T,message=FALSE,warning=FALSE}
# url where FAO shapfile is stored
url<-"https://www.fao.org/fishery/geoserver/area/ows?service=WFS&version=1.0.0&request=GetFeature&typeName=area%3AFAO_AREAS&maxFeatures=50&outputFormat=SHAPE-ZIP"

# Download file
download.file(url,"data/spatial/FAO_AREAS.zip",mode="wb")

# Unzip downloaded file
unzip(here::here ("data", "spatial", "FAO_AREAS.zip"),exdir="data/spatial")

# Load FAO (spatial multipolygon)
FAO<- st_read(file.path("data", "spatial", "FAO_AREAS.shp"))

# Select Atlantic Ocean FAO Area 
FAO_Atl <- FAO[FAO$OCEAN=="Atlantic",]

# Select Black Sea subarea 
Black_Sea <- FAO_Atl[FAO_Atl$ID=="20",]

# Transform to sf objects
FAO_Atl.sf <- st_as_sf(FAO_Atl)
Black_Sea.sf <- st_as_sf(Black_Sea)

# Remove Black sea using st_difference (reverse of st_intersection)
FAO_Atl_no_black_sea <- st_difference(FAO_Atl.sf,Black_Sea.sf) %>%   dplyr::select (F_AREA)

# Transform to spatial polygons dataframe
study_area<- sf:::as_Spatial(FAO_Atl_no_black_sea)

plot(study_area)

# Remove unused files
rm(FAO, FAO_Atl, FAO_Atl.sf, FAO_Atl_no_black_sea, Black_Sea, Black_Sea.sf)
```

Download occurrence data from OBIS and GBIF using scientific name. 

In this case we select Albacore tuna species (<em>Thunnus alalunga</em>).
```{r, eval=T,message=FALSE,warning=FALSE}
# Get data from OBIS
#mydata.obis<-robis::occurrence(scientificname="Thunnus alalunga")
#save(mydata.obis, file=file.path("data","occurrences",file="mydata.obis.RData"))
load(here::here ("data", "occurrences", "mydata.obis.RData"))

# Get data from GBIF
#mydata.gbif<-occ_data(scientificName="Thunnus alalunga", hasCoordinate = TRUE, limit=100000)$data
#save(mydata.gbif,file=file.path("data","occurrences",file="mydata.gbif.RData"))
load(here::here ("data", "occurrences", "mydata.gbif.RData"))
```

We now check the downloaded data and select the fields of interest.
```{r, eval=T,message=FALSE,warning=FALSE}
# Check names for GBIF data
names(mydata.gbif)
  
# Select columns of interest
mydata.gbif <- mydata.gbif %>%
                dplyr::select("acceptedScientificName",
                  "decimalLongitude",
                  "decimalLatitude",
                  "year",
                  "month",
                  "day",
                  "eventDate",
                  "depth")

# Check names in for OBIS data
names(mydata.obis)

# Select columns of interest
mydata.obis <-  mydata.obis %>%
                dplyr::select("scientificName",
                  "decimalLongitude",
                  "decimalLatitude",
                  "date_year",
                  "month",
                  "day",
                  "eventDate",
                  "depth",
                  "bathymetry",
                  "occurrenceStatus",
                  "sst")

```

Reformat the data adding a new field and renaming some columns from mydata.gbif dataframe in order to have the same columns and be able to join both downloaded datasets.
```{r, eval=T,message=FALSE,warning=FALSE}
mydata.gbif <- mydata.gbif %>% 
    dplyr::rename(scientificName= "acceptedScientificName") %>% 
    dplyr::rename(date_year = "year") %>% 
    dplyr::mutate(bathymetry= NA) %>% 
    dplyr::mutate(occurrenceStatus=1) %>% 
    dplyr::mutate(sst= NA)

# Join data from OBIS and GBIF 
mydata.fus<-rbind(mydata.obis,mydata.gbif)
  
# Assign unique scientific name 
mydata.fus <- mydata.fus %>% 
    dplyr::mutate(scientificName= paste(mydata.obis$scientificName[1]))

# Remove unused files
rm(mydata.gbif, mydata.obis)
```

We now clean downloaded raw data.
```{r, eval=T,message=FALSE,warning=FALSE}
# Give date format to eventDate and fill out month and date_year columns
  mydata.fus$eventDate <- as.Date(mydata.fus$eventDate)
  mydata.fus$date_year <- as.numeric(mydata.fus$date_year)
  mydata.fus$month <- as.numeric(mydata.fus$month)

# Assign 1 value to occurrenceStatus
mydata.fus <- mydata.fus  %>% 
    dplyr::mutate(occurrenceStatus = 1)
```

We remove outliers based on distance method (total distance= 1000 km).
```{r, eval=T,message=FALSE,warning=FALSE}

out.dist <- cc_outl(x=mydata.fus,
                lon = "decimalLongitude", lat = "decimalLatitude",
                species = "scientificName",
                method="distance", tdi=1000, # distance method with tdi=1000km
                thinning=T, thinning_res=0.5,
                value="flagged") 

# Remove outliers from the data
mydata.fus <- mydata.fus[out.dist, ]
```

Remove duplicates.
```{r, eval=T,message=FALSE,warning=FALSE}
# First create a vector containing longitude, latitude and event date information
date <- cbind(mydata.fus$decimalLongitude,mydata.fus$decimalLatitude,mydata.fus$eventDate)

# Remove the duplicated records
mydata.fus<-mydata.fus[!duplicated(date),]

# Remove unused files
rm(date)
```

Mask retrieved occurrence data to our study area and add bathymetry value to each occurrence point.
```{r, eval=T,message=FALSE,warning=FALSE}
# Assign coordinate format and projection to be able to use FAO Atlantic as a mask
dat <- data.frame(cbind(mydata.fus$decimalLongitude,mydata.fus$decimalLatitude))
ptos<-as.data.table(dat,keep.columnnames=TRUE)
  
coordinates(ptos) <- ~ X1 + X2

# Assign projection
proj4string(ptos) <-proj4string(study_area)
  
# Select only occurrences from FAO Atlantic
match2<-data.frame(subset(mydata.fus,!is.na(over(ptos, study_area)[,1])))
  
# Extract the FAO area of each point
match3<-data.frame(subset(over(ptos, study_area), !is.na(over(ptos, study_area)[,1])))
  
# Create data frame with area, name, long, lat and year 
df0<-cbind(F_AREA=match3$F_AREA,match2)[,c("F_AREA","scientificName","decimalLongitude","decimalLatitude","date_year","occurrenceStatus")]

# Rename some columns
names(df0)[3:5]<-c("LON","LAT","YEAR")

# Add bathymetry from NOAA
#bathy <- marmap::getNOAA.bathy(lon1=-100,lon2=30,lat1=-41,lat2=55, resolution = 1,               keep=TRUE, antimeridian=FALSE, path= here::here("data", "spatial"))
load(here::here ("data", "spatial", "bathy.RData"))

df0$bathymetry <- get.depth(bathy, df0[,c("LON","LAT")], locator=F)$depth

# Remove unused files
rm(mydata.fus, match2, match3, dat, ptos)
```

We plot the occurrence data.
```{r, eval=T,message=FALSE,warning=FALSE}
ggplot() +
   geom_path(data = study_area, 
             aes(x = long, y = lat, group = group),
             color = 'gray', linewidth = .2) +
   geom_point(data=df0, aes(x=LON, y=LAT)) 

```

And, we save the data.
```{r, eval=T,message=FALSE,warning=FALSE}
save(df0, file = "data/occurrences/occ.RData")
save(study_area, file = "data/spatial/study_area.RData")
```

## Create pseudo-absence data

After saving presence data for the species of interest we need to generate absence information in order to work with logistic regression SDM afterwards. In this case we create pseudo-absence data with a constant prevalence of 50% [@mcpherson_2004]; [@barbetmassin_etal_2012].

For that, we generate a buffer around each presence data point, with a radius of 100km, where no points can be generated. 

Before starting the pseudo-absence generation process, we delete observations in land (positive bathymetry) and select data between 2000 and 2014 (same temporal range as the environmental data that we will download later).
```{r,  eval=T,message=FALSE,warning=FALSE}
# Remove points in land
df0<-subset(df0,bathymetry<0)

# Select only years from 2000 to 2014
df0<- subset(df0, YEAR<=2014 & YEAR>=2000)

```

Then we transform the presence data frame to "SpatialPointsDataFrame" class object and to "sf" class object so we can operate and plot easily with spatial data tools. We can see the characteristics of the object through its summary.
```{r,  eval=T,message=FALSE,warning=FALSE}
# Convert to spatial point data frame
df<-df0 ; coordinates(df)<- ~LON+LAT
crs(df)<-crs(study_area)

# Convert to sf
df.sf<-st_as_sf(df)
study_area.sf<-st_union(st_as_sf(study_area))


summary(df)
```

We can easily plot sf objects using ggplot, here we plot the area of study and the presence data points.
```{r,  eval=T,message=FALSE,warning=FALSE}
ggplot(study_area.sf) + 
  geom_sf() + 
  geom_sf(data=st_union(df.sf),
          size=1,
          alpha=0.5)
```

We save a base plot (p0) with the world map and our area of study in blue.
```{r, eval=T,message=FALSE,warning=FALSE}
# Basic ggplot
global <- map_data("worldHires")

p0 <- ggplot() + 
  annotation_map(map=global, fill="grey")+
  geom_sf(data=study_area.sf,fill=5)

print(p0)
```

In order to generate a buffer around each occurrence data point, we need to work with euclidean distances, so first, we need to transform the decimal latitude and longitude values to UTM.
```{r, eval=T,message=FALSE,warning=FALSE}
# Function to find your UTM. 
lonlat2UTM = function(lonlat) {
  utm = (floor((lonlat[1] + 180) / 6) %% 60) + 1
  if(lonlat[2] > 0) {
    utm + 32600
  } else{
    utm + 32700
  }
}

(EPSG_2_UTM <- lonlat2UTM(c(mean(df$LON), mean(df$LAT))))

# Transform study_area and data points to UTMs (in m)
aux <- st_transform(study_area.sf, EPSG_2_UTM)
df.sf.utm <- st_transform(df.sf, EPSG_2_UTM)
```

Now, we can create buffers of 100 km around the points and join the resulting polygons. Then this buffer is intersected with the area of study defining the area where the pseudo-absences can be generated. To visualize the defined areas, we plot the buffers in red and the area that we will use to generate pseudo-absences in green.
```{r, eval=T,message=FALSE,warning=FALSE}
# Create buffers of 100000m
buffer <- st_buffer(df.sf.utm, dist=100000)
buffer <- st_union(buffer)

# Intersect the are with the buffer
aux0 <- st_difference(aux, buffer)

# ggplot for all data
p0 +
  geom_sf(data=aux0,fill=3) +
  geom_sf(data=buffer,fill=2)

#zoom
p0 +
  geom_sf(data=aux0,fill=3) +
  geom_sf(data=buffer,fill=2)+
  coord_sf(xlim=c(-95,-82), ylim=c(22,31))
```

We create a data frame for pseudo-absences with the same dimensions as the presences data frame.
```{r, eval=T,message=FALSE,warning=FALSE}
# Generate the pseudo-absence data frame
pseudo <- matrix(data=NA, nrow=dim(df0)[1], ncol=dim(df0)[2])
pseudo <- data.frame(pseudo)
names(pseudo) <- names(df0)
```

To generate the pseudo-absence data points, we sample randomly from the defined area and we extract their latitude and longitude to incorporate them in the created data frame. We set the occurrenceStatus equal to 0 as they are absences.
```{r, eval=T,message=FALSE,warning=FALSE}
# Set the seed
set.seed(1)

# Sample from the defined area
rp.sf <- st_sample(aux0, size=dim(df.sf.utm)[1], type="random") # randomly sample points

# Transform to lat and lon and extract coordinates as data.frame
rp.sf <- st_transform(rp.sf, 4326)
rp <- as.data.frame(st_coordinates(rp.sf)) 
pseudo$LON <- rp$X
pseudo$LAT <- rp$Y

# Complete the rest of columns
pseudo$scientificName <- df0$scientificName
pseudo$occurrenceStatus  <- 0
```

We can plot the generated pseudo-absence data (in pink) in the map, together with the presence data points (in black).
```{r, eval=T,message=FALSE,warning=FALSE}
p0 +
  geom_sf(data=rp.sf, col=6, shape=4,size=0.5)+
  geom_sf(data=df.sf.utm, col=1, alpha=0.8,size=0.5)+
  ggtitle(unique(df$scientificName))

# Zoom
p0 +
  geom_sf(data=rp.sf, col=6, shape=4,size=1)+
  geom_sf(data=df.sf.utm, col=1, alpha=0.8,size=0.5)+
  coord_sf(xlim=c(-95,-82), ylim=c(23,31))+
  ggtitle(unique(df$scientificName))
```

Finally we join the presence and pseudo-absence data frames selecting the columns of interest and save the new data frame.
```{r, eval=T,message=FALSE,warning=FALSE}
# Join the two data sets 
PAdata <- rbind(df0, pseudo)[,c("scientificName","LON","LAT","YEAR","occurrenceStatus")]

# Save the final dataset of occurrence and pseudo-absence points
save(list=c("PAdata"),file=file.path("data","outputs_for_modelling",file="PAdata.RData"))
```




<!--chapter:end:02-data_pa.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---

<!-- 
This file is part of a gitbook that should be cited as: 

Valle, M., Citores, L., Ibaibarriaga, L., Chust, C. (2023) GAM-NICHE: Shape-Constrained GAMs to build Species Distribution Models under the ecological niche theory. AZTI. https://doi.org/10.57762/fzpy-6w51 

This tutorial has been supported by the European Union’s Horizon 2020 research and innovation programme under grant agreements No 862428 MISSION ATLANTIC project
-->

# Environmental data

In this chapter we first, download environmental data from a public repository; second, we crop the data to our area of interest, and we save it as raster stack. 

As in Chapter 2, first, we load a list of required libraries.
```{r, eval=T,message=FALSE,warning=FALSE}
requiredPackages <- c(
  #GENERAL USE LIBRARIES --------#
  "here", # Library for reproducible workflow
  "rstudioapi",  # Library for reproducible workflow
  "maptools", #plotting world map
  "ggplot2", #for plotting
  "knitr",  # format tables
  "kableExtra", # format tables
  "raster", # to work with spatial data
  "dplyr",
  
  #DOWNLOAD FROM PUBLIC REPOSITORIES --------#
  "sdmpredictors" #to access Bio-ORACLE dataset
  
    )
```

We run a function to install the required packages that are not in our system and load all the required packages.
```{r, eval=T,message=FALSE,warning=FALSE}
install_load_function <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg))
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}

install_load_function(requiredPackages)
```

We define some overall settings.
```{r, eval=T,message=FALSE,warning=FALSE}
# General settings for ggplot (black-white background, larger base_size)
theme_set(theme_bw(base_size = 16))
```

## Download from public repositories

Environmental data can be available from different sources. In this case, we used the Bio-ORACLE (ocean
rasters for analyses of climate and environment) database [@tyberghein_etal_2012; @assis_etal_2017]. These data are publicly available and are easily accessible from the [`sdmpredictors` package](https://cran.r-project.org/web/packages/sdmpredictors/index.html). 

We can check the list of available datasets with the function `list_datasets` from `sdmpredictors` package, as follows: 
```{r, eval=T,message=FALSE,warning=FALSE}
mydat <- list_datasets()

kable(mydat)%>% 
  kable_styling("striped") %>% 
  scroll_box(height="600px", width = "100%")
```


By default this function returns all the supported datasets. To return only marine datasets we can set the `marine` argument equal to `TRUE` or equivalently we could set the `terrestrial` and `freshwater` arguments equal to `FALSE`:
```{r, eval=T,message=FALSE,warning=FALSE}
mydat <- list_datasets(marine=T)
# or equivalently: 
# mydat <- list_datasets(terrestrial=F, freshwater=F)
# mydat <- list_datasets(marine=T, terrestrial=F, freshwater=F)
```

There are two datasets (Bio-ORACLE and MARSPEC) that have marine data. The function `list_layers` returns information on the layers of one or more datasets. So, we can see the layers available in the Bio-ORACLE dataset as follows:
```{r, eval=T,message=FALSE,warning=FALSE}
mytab <- list_layers("Bio-ORACLE")

kable(mytab)%>% 
  kable_styling("striped") %>% 
  scroll_box(height="600px", width = "100%")
```


Once we identify the dataset and the layers we are interested on, we can extract their details from the list. In this case, we download data on chlorophyll, salinity, diffuse attenuation coefficient and sea surface temperature.
```{r, eval=T,message=FALSE,warning=FALSE}
target <- c("BO2_chlomean_ss",
            "BO2_salinitymean_ss",
            "BO_damean",
            "BO_sstmean")

# Extrat details from the list
myvars <- mytab %>% 
  dplyr::filter (mytab$layer_code %in% target)

myvars$name
```

And we can download them using the function `load_layers` 
```{r, eval=T,message=FALSE,warning=FALSE}
# Download layers
myBioracle.layers <- load_layers(c("BO2_chlomean_ss", "BO2_salinitymean_ss", "BO_damean" ,"BO_sstmean")) 

save (list = "myBioracle.layers", file = "data/env/myBioracle.layers.RData")
```

Note that the resulting object is a `rasterStack`, where each variable is a layer
```{r, eval=T,message=FALSE,warning=FALSE}
class(myBioracle.layers)
myBioracle.layers
``` 

And we can plot it:
```{r, eval=T,message=FALSE,warning=FALSE}
raster::plot(myBioracle.layers)
```

Note that the functions `dataset_citations` and `layer_citations` provide the bibliographic entries of the datasets and layers for proper citation:
```{r}
print(dataset_citations("Bio-ORACLE"))
print(layer_citations("BO2_chlomean_ss"))
```

In case we are not interested on the whole area, we can crop the raster objects to the area of interest. 

For example, we can load the `study_area` object that is a SpatialPolygonsDataFrame that has been created previously and defines the extent of our spatial data and we can crop the rasterStack to the same extent:
```{r, warning=F, message=F}
load(here::here ("data", "spatial", "study_area.RData"))

mylayers <- crop(myBioracle.layers, extent(study_area))

plot(mylayers)
```

To facilitate subsequent access, the rasterStack with the downloaded data is saved in a local folder: 
```{r, eval=T}
writeRaster(mylayers, filename="data/env/mylayers.tif", options="INTERLEAVE=BAND", overwrite=TRUE)
```






<!--chapter:end:03-data_env.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---

<!-- 
This file is part of a gitbook that should be cited as: 

Valle, M., Citores, L., Ibaibarriaga, L., Chust, C. (2023) GAM-NICHE: Shape-Constrained GAMs to build Species Distribution Models under the ecological niche theory. AZTI. https://doi.org/10.57762/fzpy-6w51 

This tutorial has been supported by the European Union’s Horizon 2020 research and innovation programme under grant agreements No 862428 MISSION ATLANTIC project
-->

# Prepare the final dataset

In this chapter we first, extract environmental data associated to the presence/pseudo-absence data, we explore the data we got, we check correlation between variables and we calculate the Variance Inflation Factor (VIF) to make a selection of the variables we are going to use in the model. 

First, we load a list of required libraries.
```{r, eval=T,message=FALSE,warning=FALSE}
requiredPackages <- c(
  #GENERAL USE LIBRARIES --------#
  "here", # Library for reproducible workflow
  "rstudioapi",  # Library for reproducible workflow
  
  #EXTRACT ENVIRONMENTAL DATA AND PLOTS
  "sp", # spatial data
  "raster", #spatial data
  "dplyr",
  "tidyr",
  "ggplot2",
  "ggcorrplot",
  
  #CORRELATION ANALYSIS
  "GGally", #correlation analysis
  "HH" #calculate VIF
    )
```

We run a function to install the required packages that are not in our system and load all the required packages.
```{r, eval=T,message=FALSE,warning=FALSE}
install_load_function <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg))
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}

install_load_function(requiredPackages)

```

We define some overall settings.
```{r, eval=T,message=FALSE,warning=FALSE}
# General settings for ggplot (black-white background, larger base_size)
theme_set(theme_bw(base_size = 16))
```

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE, fig.width=16, fig.height=12)
```

## Extract environmental data associated to species distribution data

Once we have prepared our species distribution data (occurrences and pseudo-absences) and the environmental rasters, we need to merge both sources of data. First, we load the objects created in previous sections:
```{r, eval=T,message=FALSE,warning=FALSE}
# Load presence-absence data
load(here::here ("data", "outputs_for_modelling", "PAdata.RData"))

# Load environmental rasters
mylayers<-stack(here::here ("data", "env", "mylayers.tif"))
```

Now we can extract the environmental data associated to each of the species data points using the function `extract` from the `raster` package. The method employed is `bilinear` that returns the interpolated value from the four nearest raster cells.
```{r, eval=T,message=FALSE,warning=FALSE}
raster_ex <- raster::extract(x=mylayers, y=PAdata[,c("LON","LAT")], method="bilinear", na.rm=TRUE, df=T) 

colnames(raster_ex)[-1]<-c("BO2_chlomean_ss", "BO2_salinitymean_ss", "BO_damean" ,"BO_sstmean")

head(raster_ex)
```

We merge the presence/pseudo-absence data and the environmental data: 
```{r, eval=T,message=FALSE,warning=FALSE}
data <- cbind(PAdata, raster_ex)
```

We can conduct some quick checks on the new dataset:  
```{r, eval=T,message=FALSE,warning=FALSE}
dim(data)
str(data)
head(data)
summary(data)
```

The new dataset has 29806 rows and 10 columns, and there are 139 NA's in the environmental dataset. We remove the points with NA's:
```{r, eval=T,message=FALSE,warning=FALSE}
data <- data %>% 
  dplyr::select (-YEAR) %>% #we remove year column because pseudoabsences miss this info
  na.omit()
```

We check again the dataset:
```{r, eval=T,message=FALSE,warning=FALSE}
dim(data)
summary(data) 
```

The resulting dataset has 29661. We save this dataset in a local file to work on it in subsequent steps. 
```{r, eval=T,message=FALSE,warning=FALSE}
save(list="data", file="data/outputs_for_modelling/PAdata_with_env.RData")
```

## Exploratory plots of environmental variables

Before starting the modelling process, we are going to explore the individual variables in the dataset. 

We can explore the distributions of each of the environmental variables by looking at the violin and boxplots and at the histograms and density plots as follows: 

```{r, eval=T,message=FALSE,warning=FALSE}
tmp <- data[, c("BO2_chlomean_ss","BO2_salinitymean_ss","BO_damean","BO_sstmean")]
tmp <- pivot_longer(data=tmp, cols=everything()) 

ggplot(data=tmp, aes(x=name, y=value)) + 
  geom_boxplot()+
  facet_wrap(~name, scales="free")

ggplot(data=tmp, aes(x=name, y=value)) + 
  geom_violin(fill="red", alpha=0.3)+
  geom_boxplot(width=0.1)+
  facet_wrap(~name, scales="free")

ggplot(data=tmp, aes(x=value)) + 
  geom_histogram(aes(y= after_stat(density)), colour=1, fill="red", alpha=0.3)+
  geom_density(lwd=1)+
  facet_wrap(~name, scales="free")
```

## Exploratory plots of environmental variables depending on species distribution data

To analyse if there are preferences for certain ranges of the environmental variables, we compare the distribution of the environmental variables for presence and pseudo-absence data:   
```{r, eval=T,message=FALSE,warning=FALSE}
tmp <- data[, c("LON", "LAT", "BO2_chlomean_ss","BO2_salinitymean_ss","BO_damean","BO_sstmean","occurrenceStatus")]
tmp <- pivot_longer(data=tmp, cols=!occurrenceStatus) 

ggplot(data=tmp, aes(x=factor(occurrenceStatus), y=value, fill=factor(occurrenceStatus), group=factor(occurrenceStatus))) + 
  geom_violin(alpha=0.3)+
  geom_boxplot(fill="white", width=0.1)+
  facet_wrap(~name, scales="free")+
  theme(legend.position = "bottom",legend.background = element_rect(fill = "white", colour = NA))

ggplot(data=tmp, aes(x=value, fill=factor(occurrenceStatus), group=factor(occurrenceStatus))) + 
  geom_density(lwd=1, alpha=0.3)+
  facet_wrap(~name, scales="free")+
  theme(legend.position = "bottom",legend.background = element_rect(fill = "white", colour = NA))
```

## Correlation analysis

Some of the environmental variables can be correlated. The `GGally` package allows to easily produce pairplots of the variables and their correlation.  
```{r, eval=T,message=FALSE,warning=FALSE}
tmp <- data[, c("LON","LAT","BO2_chlomean_ss","BO2_salinitymean_ss","BO_damean","BO_sstmean")]

ggpairs(tmp) #this takes some minutes
```

A more detailed analysis of the potential correlations can be conducted using the package `ggcorrplot`:
```{r, eval=T,message=FALSE,warning=FALSE}
mat <- cor(tmp, use="complete.obs") 
p.mat <- cor_pmat(tmp)

ggcorrplot(mat, type = "lower", lab=T, p.mat = p.mat)
```

## Variance Inflation Factor (VIF) 

Furthermore, multicollinearity in regression analysis can be explored using the VIF (Variance Inflation Factor). The value of the VIF statistics indicate the level of multicollinearity with the rest of the variables:

* VIF equal to 1 = variables are not correlated
* VIF between 1 and 5 = variables are moderately correlated 
* VIF greater than 5 = variables are highly correlated

There are several packages in `R` that allows to calculate the VIF statistics. In this case we use the package `HH`: 
```{r, eval=T,message=FALSE,warning=FALSE}
# Select variables for VIF calculation
v.table <- data %>% 
  dplyr::select (BO2_salinitymean_ss, BO_sstmean, BO2_chlomean_ss, BO_damean)

# Get VIF results
out.vif <- vif(v.table)
sort(out.vif)
```

We remove the variable that has the highest VIF value and we test again the multicollinearity: 
```{r, eval=T,message=FALSE,warning=FALSE}
v.table <- v.table %>% 
  dplyr::select (-BO_damean)

# Get new VIF results
out.vif <- vif(v.table)
sort(out.vif)
```

Now all the variables have VIF values that are acceptable. So, we proceed to remove BO_damean (Diffuse attenuation coefficient at 490 nm). And save the selected variables for the next modelling stages:
```{r, eval=T,message=FALSE,warning=FALSE}
data <- data %>% dplyr::select (-BO_damean)
```

We save the dataset as our output for modelling.
```{r, eval=T,message=FALSE,warning=FALSE}
save(list="data", file="data/outputs_for_modelling/PAdata_with_env.RData")
```



<!--chapter:end:04-data_final.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---

<!-- 
This file is part of a gitbook that should be cited as: 

Valle, M., Citores, L., Ibaibarriaga, L., Chust, C. (2023) GAM-NICHE: Shape-Constrained GAMs to build Species Distribution Models under the ecological niche theory. AZTI. https://doi.org/10.57762/fzpy-6w51 

This tutorial has been supported by the European Union’s Horizon 2020 research and innovation programme under grant agreements No 862428 MISSION ATLANTIC project
-->

# Shape Constrained-Generalized Additive Models

In order to fit SDM in agreement with the ecological niche theory, the proposed Shape-Constrained Generalized Additive Models (SC-GAMs) in [@citores_etal_2020] are fitted in this section. SC-GAMs are based on Generalized Additive Models, allowing us to impose shape-constraints to the linear predictor function. The R package SCAM implements the general framework developed by [@pya_etal_2015] using shape-constrained P-splines. Monotonicity and concavity/convexity constraints can be imposed on the sign of the first and/or the second derivatives of the smooth terms. For fitting Species Distribution Models in agreement with the ecological niche theory, we imposed concavity  constraints ($f''(x) \le 0$), so that the response can presents at most a single mode.  

Alternatively, the R package `mboost` fits SC-GAMs using boosting methods. We are not going to develop this alternative here.

First, we load the list of required libraries.
```{r, eval=T,message=FALSE,warning=FALSE}
requiredPackages <- c(
  #GENERAL USE LIBRARIES --------#
  "here", # Library for reproducible workflow
  "rstudioapi",  # Library for reproducible workflow
  "stringr",
  "RColorBrewer",  
  "ggplot2",
  "dplyr",
  
  #SPATIAL DATA --------#
  "rgdal",
  "fields",
  "maps" ,
  "raster",

  #MODEL FIT --------#
  "scam",
  "plotmo",
  "SDMTools",
  "pkgbuild",
  "dismo"
  )
```

We run a function to install the required packages that are not in our system and load all the required packages.
```{r, eval=T,message=FALSE,warning=FALSE}
install_load_function <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg))
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}

install_load_function(requiredPackages)

```

Install and load SDMTools. You need to manually install RTools: https://cran.r-project.org/bin/windows/Rtools/history.html 
```{r, eval=T,message=FALSE,warning=FALSE}
# find_rtools()
# 
# install.packages("remotes")
# remotes::install_version("SDMTools", version = "1.1-221.2")
```

We define some overall settings.
```{r, eval=T,message=FALSE,warning=FALSE}
# General settings for ggplot (black-white background, larger base_size)
theme_set(theme_bw(base_size = 16))
```

## Model fit

We set the working directory to the folder where the current script is located and we load the dataset (PAdata_with_env.Rdata) containing the presence-absence data together with the environmental data.
```{r, eval=T,message=FALSE,warning=FALSE}
load(here::here ("data", "outputs_for_modelling", "PAdata_with_env.Rdata"))
```

To fit a logistic regression model in the SC-GAMs framework, we use the scam function, where we set the binomial family with the logit link function. Our response variable is the presence-absence data and the selected three explanatory variables are the SST, chlorophyll and salinity. Each variable is included in the model through an spline function where the concavity constraint is set using bs="cv". The details about this option can be found in the section "Constructor for concave P-splines in SCAMs" of the SCAM manual (https://cran.r-project.org/web/packages/scam/scam.pdf). The number of knots (k) is fixed at 8 in this example for a good balance between flexibility and computation time.

**UNIVARIATE MODELS**

Before fitting the model with the selected three environmental variables, we can fit univariate model as follows.

We fit the univariate model for SST, we print the summary of the model fit, and look at the fitted curve in the response scale.
```{r, eval=T,message=FALSE,warning=FALSE}
model_sst <- scam (occurrenceStatus ~  s(BO_sstmean, k=8,bs="cv"), family=binomial(link="logit"), data=data)
summary(model_sst)
plotmo(model_sst,level = 0.95, pt.col=8)
```

We repeat the same steps for the rest of the variables.
```{r, eval=T,message=FALSE,warning=FALSE}
model_chl <- scam (occurrenceStatus ~  s(BO2_chlomean_ss, k=8,bs="cv"), family=binomial(link="logit"), data=data)
summary(model_chl)
plotmo(model_chl,level = 0.95, pt.col=8)
```

Due to convergence issues, sometimes it is necessary to fix the smoothing parameter (sp) at small value, i.e. $10^{-5}$, as here when introducing salinity as an explanatory variable. If no value is provided, the smoothing parameter is estimated within the model.
```{r, eval=T,message=FALSE,warning=FALSE}
model_sal <- scam (occurrenceStatus ~  s(BO2_salinitymean_ss, k=8,bs="cv"), family=binomial(link="logit"), data=data,sp=0.00001)
summary(model_sal)
plotmo(model_sal,level = 0.95, pt.col=8)
```

**MODEL WITH ALL VARIABLES**

Now we fit the model including the three selected variables.
```{r, eval=T,message=FALSE,warning=FALSE}
model <- scam (occurrenceStatus ~  s(BO_sstmean, k=8,bs="cv")+ s(BO2_salinitymean_ss, k=8,bs="cv")+s(BO2_chlomean_ss, k=8,bs="cv"), family=binomial(link="logit"), data=data,sp=rep(0.00001,3))
summary(model)
plotmo(model,level = 0.95, pt.col=8)
```

We can see in the summary of the fit, that all included variables are statistically significant (with p<0.05) and present a unimodal response curve.

For a more detailed check of the fitting, we can use the scam.check function:
```{r, eval=T,message=FALSE,warning=FALSE}
scam.check(model)
```

## Model selection

When several explanatory variables are available, a variable selection process can be carried out. Here we provide as an example, a function that performs forward variable selection (modsel.scam) based on the significance of variables and AIC values of the fits. 
```{r, eval=T,message=FALSE,warning=FALSE}
source("function/function_scam_selection_optimized.R")
```

We save the names of the variables we want to introduce for the variable selection process as a vector:
```{r, eval=T,message=FALSE,warning=FALSE}
vars <- c("BO_sstmean",
          "BO2_salinitymean_ss",
          "BO2_chlomean_ss")
```

The default AIC tolerance is 2 and there is not a limit on selected terms in this example. These options can be modified through aic.tol and vmax arguments in the function. The number of knots and the sp can be also modified.
```{r, eval=T,message=FALSE,warning=FALSE}
model_SCGAM <- try(modsel.scam(basef="occurrenceStatus ~ 1", vars=vars, dat=data,sp=0.00001), silent=T)  
```

We check results of the selected model, such as, selected variable names:
```{r, eval=T,message=FALSE,warning=FALSE}
model_SCGAM$svars
```

AICs of the fitted models:
```{r, eval=T,message=FALSE,warning=FALSE}
sapply(model_SCGAM$smod, AIC)
```

Explained deviance of fitted models:
```{r, eval=T,message=FALSE,warning=FALSE}
sapply(model_SCGAM$smod, function(x) summary(x)$dev.expl)
```

Formulas of the fitted models:
```{r, eval=T,message=FALSE,warning=FALSE}
lapply(model_SCGAM$smod, formula)
```

Summaries of the fitted models:
```{r, eval=T,message=FALSE,warning=FALSE}
lapply(model_SCGAM$smod, summary)
```

The last model of the list, is the selected one. Which in this case contains the considered three variables.
```{r, eval=T,message=FALSE,warning=FALSE}
selected_model <- model_SCGAM$smod[[length(model_SCGAM$smod)]]
```

We save the selected model
```{r}
save(list="selected_model", file="models/selected_model.RData")
```

Note that there are multiple options and criteria for model selection that are not reviewed here. Any model selection technique used for GAMs can be used also for SC-GAMs.

<!--chapter:end:05-model_fit.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---

<!-- 
This file is part of a gitbook that should be cited as: 

Valle, M., Citores, L., Ibaibarriaga, L., Chust, C. (2023) GAM-NICHE: Shape-Constrained GAMs to build Species Distribution Models under the ecological niche theory. AZTI. https://doi.org/10.57762/fzpy-6w51 

This tutorial has been supported by the European Union’s Horizon 2020 research and innovation programme under grant agreements No 862428 MISSION ATLANTIC project
-->

# Model validation

In this section, model validation is performed in order to assess the predictive performance of the selected model. This validation is conducted via k-fold cross-validation. The data set is divided into k equally sized groups [@hijmans_2012], using a percentage of randomly selected observations to run the model and the remaining for validation, iteratively for each fold. 

First, we load the list of required libraries.
```{r, eval=T,message=FALSE,warning=FALSE}
requiredPackages <- c(
  #GENERAL USE LIBRARIES --------#
  "here", # Library for reproducible workflow
  "rstudioapi",  # Library for reproducible workflow
  "stringr",
  "RColorBrewer",  
  "ggplot2",
  "dplyr",
  "tidyverse",
  "R.utils",
  "ggpubr",
  "hrbrthemes",
  
  #SPATIAL DATA --------#
  "rgdal",
  "fields",
  "maps" ,
  "raster",

  #MODEL FIT --------#
  "scam",
  "plotmo",
  "SDMTools",
  "pkgbuild",
  "dismo"
  )
```

We run a function to install the required packages that are not in our system and load all the required packages.
```{r, eval=T,message=FALSE,warning=FALSE}
install_load_function <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg))
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}

install_load_function(requiredPackages)

```

We define some overall settings.
```{r, eval=T,message=FALSE,warning=FALSE}
# General settings for ggplot (black-white background, larger base_size)
theme_set(theme_bw(base_size = 16))
```

We load output from the selected model saved in the previous step.
```{r, eval=T,message=FALSE,warning=FALSE}
load(here::here ("models", "selected_model.Rdata"))
```

## Optimum threshold

We generate a data frame with the data used in the selected model and we add the predicted values.
```{r, eval=T,message=FALSE,warning=FALSE}
#PAdata_enviroment used in the selected model
data<-selected_model$model

# Predict 
scgam.pred <- predict(selected_model, newdata=data, type="response")
  
# Add the prediction to the data object
data$scgam.pred <- as.vector(scgam.pred)
head(data)
``` 

The threshold for presence-absence classification for each species is obtained as the values maximizing sensitivity plus specificity [@jimenez_etal_2007]. If the result was a range (instead of a single value), we would select the mean value of the range. 
```{r, eval=T,message=FALSE,warning=FALSE}
# Optimizing the threshold probability
obs <- data$occurrenceStatus
predSCGAM_P <- data$scgam.pred
  
# Threshold optimizing
myoptim <- optim.thresh (obs,predSCGAM_P)
myoptim
  
# Select the threshold that maximizes the sum of sensitivity and specificity 
myThreshold <- as.numeric((myoptim[["max.sensitivity+specificity"]]))
```

Accuracy indicators, such as AUC (Area Under the Receiver Operating Characteristic—ROC—curve), sensitivity (true predicted presences) and specificity (true predicted absences) are first computed for the all observations.
```{r, eval=T,message=FALSE,warning=FALSE}
# Accuracy values with all observations
accuracy (obs, predSCGAM_P, threshold=myThreshold)
  
# Create confusion matrix with all observations
confusion.matrix(obs, predSCGAM_P, threshold=myThreshold)
```

## k-fold validation

In this case we use a 5-fold cross-validation. 
```{r, eval=T,message=FALSE,warning=FALSE}
# Number of groups
k <- 5 

# Generate groups
groups<-kfold(data, k, by=data$occurrencestatus)
```


The model is run for each of the 5 random subset (with a 20% of the observations) and indicators are then computed using the remaining 80% of the observations. Indicators are the averaged across folds. 
```{r, eval=T,message=FALSE,warning=FALSE}
# Initialise the confusion matrix and the accuracy table: 
myCM <- NULL 
myACC <- NULL
  
# Get the formula of the selected model  
formula <- summary(selected_model)[["formula"]]
  
# Get the smoothing parameters of the selected model  
sp <- selected_model$sp

# Loop for each group k
for (j in 1:k) {
  # Preparation of Training Sites
  p_Training <- data[groups != j,]
  
  # Model fit
  selected_model.sp.j <- scam (formula, family=binomial(link="logit"),data=p_Training, sp=c(sp))
  
  # Predict Model
  p_validacion<-data[groups == j,]
  
  selected_model.sp.j.pred <- predict(selected_model.sp.j, newdata=p_validacion, type="response")
  p_validacion$Pred <- selected_model.sp.j.pred
    
  # Confussion matrix and accuracy table for fold j
  obs <- p_validacion$occurrenceStatus
  predSCGAM <- p_validacion$Pred
  myCM <- rbind(myCM, as.numeric(confusion.matrix(obs, predSCGAM, threshold=myThreshold)))
  myACC <- rbind(myACC, accuracy(obs, predSCGAM, threshold=myThreshold))
  } 
  
# Mean values across k-folds
validation_summary<-cbind(Threshold=myThreshold,
                            mean_AUC=mean(myACC$AUC),
                            mean_Omision=mean(myACC$omission.rate),
                            mean_sensitivity=mean(myACC$sensitivity),
                            mean_specificity=mean(myACC$specificity),
                            mean_Prop.Corr=mean(myACC$prop.correct))

validation_summary
```

We save the validation summary object.
```{r, eval=F}
save(validation_summary, file = here::here("models/validation_summary.RData"))
```


<!--chapter:end:06-model_validation.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---

<!-- 
This file is part of a gitbook that should be cited as: 

Valle, M., Citores, L., Ibaibarriaga, L., Chust, C. (2023) GAM-NICHE: Shape-Constrained GAMs to build Species Distribution Models under the ecological niche theory. AZTI. https://doi.org/10.57762/fzpy-6w51 

This tutorial has been supported by the European Union’s Horizon 2020 research and innovation programme under grant agreements No 862428 MISSION ATLANTIC project
-->

# Prediction and maps

In this chapter we predict from the fitted model and produce final SDMs maps.

First, we load a list of required libraries.
```{r, eval=T,message=FALSE,warning=FALSE}
requiredPackages <- c(
  #GENERAL USE LIBRARIES --------#
  "here", # Library for reproducible workflow
  "rstudioapi",  # Library for reproducible workflow
  "ggplot2", #for plotting
  "tidyverse", 
  "rgdal", # to work with Spatial data
  "raster", #spatial 
  "maps", #world map
  "maptools", #plotting world map
  "RColorBrewer", #color palette
  "scam", #sdm models under the ecological niche theory framework
  "ggpubr"
  )
```

We run a function to install the required packages that are not in our system and load all the required packages.
```{r, eval=T,message=FALSE,warning=FALSE}
install_load_function <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg))
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}

install_load_function(requiredPackages)

```

We define some overall settings.
```{r, eval=T,message=FALSE,warning=FALSE}
# General settings for ggplot (black-white background, larger base_size)
theme_set(theme_bw(base_size = 16))
```

## Prepare environmental data

In previous steps (see Chapter 2), we have defined the study area that defines the extent of our spatial data. We load the `study_area` object that is a SpatialPolygonsDataFrame class:
```{r, eval=T, warning=F, message=F}
load(here::here ("data", "spatial", "study_area.RData"))
```

And we load the rasterStack with the downloaded environmental data.
```{r, eval=T, warning=F, message=F}
mylayers<-stack("data/env/mylayers.tif")
```

We transform the environmental data set first into a data frame, and then into a SpatialDataFrame.
```{r, eval=T, warning=F, message=F}
env_dataframe <- raster::as.data.frame(mylayers, xy=TRUE)

summary(env_dataframe)

names(env_dataframe) <- c("x", "y", "BO2_chlomean_ss", "BO2_salinitymean_ss", "BO_damean", "BO_sstmean")
```

## Projection

We load the selected model and predict into the whole environmental data. 
```{r, eval=T, warning=F, message=F}
# Load SC-GAM model
load(here::here("models", "selected_model.Rdata"))

# Predicting 
predict <- predict(selected_model,newdata=env_dataframe,type ="response",se.fit=T)         

env_dataframe$fit<-predict$fit
env_dataframe$se.fit<-predict$se.fit

save(env_dataframe, file="results/projection.Rdata")
```

## Mapping

```{r, eval=T, warning=F, message=F}
# Load PA data
load(here::here ("data", "outputs_for_modelling", "PAdata_with_env.Rdata"))


proj_map <-ggplot()+
  geom_raster(data=subset(env_dataframe),
              aes(x,y,fill=fit)) +
  scale_fill_gradient2(low="blue", 
                       mid="orange",
                       high="red",
                       midpoint = 0.5,
                       limits = c(0,1)) +
  ggtitle("Occurrence probabilty Thunnus alalunga")+ 
  geom_point(data=subset(data,occurrenceStatus==1),
             aes(LON,LAT),
             col=1,
             size=0.3) +
  theme_pubclean(base_size = 14)+
  theme(panel.background = element_blank(),
        plot.title = element_text(face = "italic"), 
        #text = element_text(size = 14), 
        axis.text.x = element_text(size = 10),
        axis.text.y = element_text(size = 10),
        legend.position="right") +
  labs(y="latitude", x = "longitude")
  
print(proj_map)
```

We finally save the projection map.
```{r, eval=T, warning=F, message=F}
ggsave(filename= "Thunnus_alalunga_proj_map.tif", 
        plot=proj_map, 
        device="tiff",
        path=here::here ("plots", "projections"), 
        height=22, width=30,
        units="cm", dpi=300)
```


<!--chapter:end:07-projections.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---

<!-- 
This file is part of a gitbook that should be cited as: 

Valle, M., Citores, L., Ibaibarriaga, L., Chust, C. (2023) GAM-NICHE: Shape-Constrained GAMs to build Species Distribution Models under the ecological niche theory. AZTI. https://doi.org/10.57762/fzpy-6w51 

This tutorial has been supported by the European Union’s Horizon 2020 research and innovation programme under grant agreements No 862428 MISSION ATLANTIC project
-->

# Acknowledgements

This tutorial has been supported by the European Union’s Horizon 2020 research and innovation programme under grant agreements No 862428 [MISSION ATLANTIC project](https://missionatlantic.eu/).

<!--chapter:end:08_acknowledgements.Rmd-->

<!-- 
This file is part of a gitbook that should be cited as: 

Valle, M., Citores, L., Ibaibarriaga, L., Chust, C. (2023) GAM-NICHE: Shape-Constrained GAMs to build Species Distribution Models under the ecological niche theory. AZTI. https://doi.org/10.57762/fzpy-6w51 

This tutorial has been supported by the European Union’s Horizon 2020 research and innovation programme under grant agreements No 862428 MISSION ATLANTIC project
-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:09-references.Rmd-->

