[["model-validation.html", "Chapter 7 Model validation 7.1 Optimum threshold 7.2 k-fold validation", " Chapter 7 Model validation In this section, model validation is performed in order to assess the predictive performance of the selected model. This validation is conducted via k-fold cross-validation. The data set is divided into k equally sized groups (Hijmans 2012), using a percentage of randomly selected observations to run the model and the remaining for validation, iteratively for each fold. First we load all required libraries. library(scam) library(plotmo) library(rgdal) library(ggplot2) library(dplyr) library(fields) library(maps) library(raster) library(RColorBrewer) library(tidyverse) library(R.utils) library(SDMTools) library(dismo) library(ggplot2) library(ggpubr) library(hrbrthemes) library(rstudioapi) We load output from the selected model saved in the previous step. setwd(dirname(getSourceEditorContext()$path)) load(file.path (&quot;models&quot;, &quot;model.Rdata&quot;)) 7.1 Optimum threshold We generate a data frame with the data used in the selected model and we add the predicted values. #data used in the selected model data&lt;-selected_model$model # Predict scgam.pred &lt;- predict(selected_model, newdata=data, type=&quot;response&quot;) # Add the prediction to the data object data$scgam.pred &lt;- as.vector(scgam.pred) head(data) ## occurrenceStatus BO_sstmean BO2_salinitymean_ss BO2_chlomean_ss scgam.pred ## 1 1 16.99439 35.42666 1.20410306 0.9833270 ## 4 1 16.99439 35.42666 1.20410306 0.9833270 ## 7 1 25.52963 36.43070 0.08455528 0.4364165 ## 9 1 16.99439 35.42666 1.20410306 0.9833270 ## 11 1 15.21486 34.20426 0.72283227 0.8450083 ## 12 1 16.99439 35.42666 1.20410306 0.9833270 The threshold for presence-absence classification for each species is obtained as the values maximizing sensitivity plus specificity (Jim√©nez-Valverde and Lobo 2007). If the result was a range (instead of a single value), we would select the mean value of the range. # Optimizing the threshold probability obs &lt;- data$occurrenceStatus predSCGAM_P &lt;- data$scgam.pred # threshold optimizing myoptim &lt;- optim.thresh (obs,predSCGAM_P) myoptim ## $min.occurence.prediction ## [1] 0.006721373 ## ## $mean.occurence.prediction ## [1] 0.6611373 ## ## $`10.percent.omission` ## [1] 0.37 ## ## $`sensitivity=specificity` ## [1] 0.44 ## ## $`max.sensitivity+specificity` ## [1] 0.42 ## ## $maxKappa ## [1] 0.42 ## ## $max.prop.correct ## [1] 0.42 ## ## $min.ROC.plot.distance ## [1] 0.43 # select the threshold that maximizes the sum of sensitivity and specificity myThreshold &lt;- as.numeric((myoptim[[&quot;max.sensitivity+specificity&quot;]])) Accuracy indicators, such as AUC (Area Under the Receiver Operating CharacteristicROCcurve), sensitivity (true predicted presences) and specificity (true predicted absences) are first computed for the all observations. # Accuracy values with all observations accuracy (obs, predSCGAM_P, threshold=myThreshold) ## threshold AUC omission.rate sensitivity specificity prop.correct ## 1 0.42 0.7415126 0.2016207 0.7983793 0.6846459 0.7409194 ## Kappa ## 1 0.4824257 # Create confusion matrix with all observations confusion.matrix(obs, predSCGAM_P, threshold=myThreshold) ## obs ## pred 0 1 ## 0 10180 2936 ## 1 4689 11626 ## attr(,&quot;class&quot;) ## [1] &quot;confusion.matrix&quot; 7.2 k-fold validation In this case we use a 5-fold cross-validation. # Number of groups k &lt;- 5 # Generate groups groups&lt;-kfold(data, k, by=data$status) The model is run for each of the 5 random subset (with a 20% of the observations) and indicators are then computed using the remaining 80% of the observations. Indicators are the averaged across folds. # Initialise the confusion matrix and the accuracy table: myCM &lt;- NULL myACC &lt;- NULL # get the formula of the selected model formula &lt;- summary(selected_model)[[&quot;formula&quot;]] # get the smoothing parameters of the selected model sp &lt;- selected_model$sp # loop for each group k for (j in 1:k) { # Preparation of Training Sites p_Training &lt;- data[groups != j,] # Model fit selected_model.sp.j &lt;- scam (formula, family=binomial(link=&quot;logit&quot;),data=p_Training, sp=c(sp)) # Predict Model p_validacion&lt;-data[groups == j,] selected_model.sp.j.pred &lt;- predict(selected_model.sp.j, newdata=p_validacion, type=&quot;response&quot;) p_validacion$Pred &lt;- selected_model.sp.j.pred # Confussion matrix and accuracy table for fold j obs &lt;- p_validacion$occurrenceStatus predSCGAM &lt;- p_validacion$Pred myCM &lt;- rbind(myCM, as.numeric(confusion.matrix(obs, predSCGAM, threshold=myThreshold))) myACC &lt;- rbind(myACC, accuracy(obs, predSCGAM, threshold=myThreshold)) } # Mean values across k-folds validation_summary&lt;-cbind(Threshold=myThreshold, mean_AUC=mean(myACC$AUC), mean_Omision=mean(myACC$omission.rate), mean_sensitivity=mean(myACC$sensitivity), mean_specificity=mean(myACC$specificity), mean_Prop.Corr=mean(myACC$prop.correct)) validation_summary ## Threshold mean_AUC mean_Omision mean_sensitivity mean_specificity ## [1,] 0.42 0.7380621 0.209646 0.790354 0.6857702 ## mean_Prop.Corr ## [1,] 0.7375217 We save the validation summary object. save(validation_summary, file = here::here(&quot;models/validation_summary.RData&quot;)) References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
