[["index.html", "Species distribution models (SDM) About", " Species distribution models (SDM) AZTI 2023-01-18 About This is a short tutorial for constructing species distribution models in R using shape-constrained generalized additive models. The code is available in AZTIs github repository repository and the book is readily available here. To cite this book, please use: BLA BLA BLA "],["introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction Species distribution models (SDMs) are numerical tools that combine observations of species occurrence or abundance at known locations with information on the environmental and/or spatial characteristics of those locations (Elith and Leathwick 2009). They are also known as ecological niche models (ENM) or habitat suitability models or A wide variety of methods have been used  Reviews of SDM literature include  One of the common problems is that, the fitted models do not agree with the ecological niche theory This book provides a tutorial on how to use shape-constrained generalized additive models to build SDMs. It is organised following the key steps in good modeling practice of SDMs (Elith and Leathwick 2009). First, presence data of a selected species are downloaded from GBIF/OBIS datasets and pseudo-absence data are created. Then, environmental data are downloaded from public repositories and extracted at each of the presence/pseudo-absence data points. Based on this dataset, an exploratory analysis is conducted to help deciding on the best modelling approach. The model is fitted to the dataset and the quality of the fit and the realism of the fitted response function are evaluated. After selecting a threshold to transform the continuous probability predictions into binary responses, the model is validated using a k-fold approach. Finally, the predicted maps are generated for visualization. References "],["libraries.html", "Chapter 2 Libraries", " Chapter 2 Libraries Load libraries that will be used library(&quot;scam&quot;) ## Loading required package: mgcv ## Loading required package: nlme ## This is mgcv 1.8-39. For overview type &#39;help(&quot;mgcv-package&quot;)&#39;. ## This is scam 1.2-12. Note that all the libraries must be installed. If some library is not installed, run: install.packages(&quot;scam&quot;) ## Warning: package &#39;scam&#39; is in use and will not be installed "],["presence-absence-data.html", "Chapter 3 Presence-absence data 3.1 Download presence data 3.2 Create pseudo-absence data", " Chapter 3 Presence-absence data Bla bla bla 3.1 Download presence data Download from GBIF OBIS. Mireia 3.2 Create pseudo-absence data Prevalence 50% See code from ANICHO (mantaining some space around presences). Leire C. Ref (Barbet-Massin et al. 2012) Copio aqui el codigo de anicho tal cual, luego lo limpiaré para este caso: # Script information ------------------------------------------------------ # Title: Generate pseudo-absences for IM-18-ANICHO # Last modified by Leire Ibaibarriaga (libaibarriaga@azti.es) and Leire Citores (lcitores@azti.es) # Load libraries ---------------------------------------------------------- library(tidyverse) library(ggplot2) library(scales) library(here) library(ggridges) library(maps) # some basic country maps library(mapdata) # higher resolution maps library(mapproj) library(marmap) # access global topography data library(mapplots) # ices rectangles library(sf) library(gridExtra) library(lubridate) # general settings for ggplot (black-white background, larger base_size) theme_set(theme_bw(base_size = 16)) # Set directories --------------------------------------------------------- # final data set created by Nerea Goikoetxea after combining logboook and VMS data are in: # \\\\dok\\nas\\K\\AZTIMAR\\PROYECTOS\\Funcionamiento de los ecosistemas marinos\\IM-18-ANICHO\\Cruce logbooks vs VMS\\Data\\df_2010_2019_DEF.csv # note that these data do not have yet the environmental variables. We have to repeat the process including the pseudo-absences # because some dates were wrong in the previous file (anicho_landings_final_FECHASMAL.csv) data.dir &lt;- here(&quot;data&quot;) # Data frame with environmental variables --------------------------------- # read data-file df &lt;- read.csv(file.path(data.dir,&quot;df_2010_2019_DEF.csv&quot;), header=T, sep=&quot;;&quot;, dec=&quot;,&quot;) dim(df) # 26336 rows and 11 columns head(df) tail(df) summary(df) # change the names names(df) &lt;- c(&quot;CODEUEBUQUE&quot;,&quot;FECHA_CAPT&quot;,&quot;DOY&quot;,&quot;WEEK&quot;,&quot;MONTH&quot;,&quot;YEAR&quot;,&quot;LAND&quot;,&quot;OK&quot;,&quot;PESO_ANE&quot;,&quot;LAT&quot;,&quot;LON&quot;) # format df$FECHA_CAPT &lt;- as.Date(df$FECHA_CAPT, format=&quot;%Y-%m-%d&quot;) # check duplicates in all the columns dupli &lt;- duplicated(df) table(dupli) # there are 13 duplicated rows!! df[dupli, ] # this shows only the rows that are duplicated (ie the second/third/... time they appear) dupli &lt;- duplicated(df) | duplicated(df, fromLast=T) # to see the duplicates and the first time they appear table(dupli) # 26, ie each duplicated row appears twice df[dupli, ] # LIC: tras hablar con Lucia parece que aunque parezcan replicados, son datos oficiales de SGPM y # tendrian que ser correctos. Asi que los agrupo y sumo la variable PESO df &lt;- df %&gt;% group_by_at(vars(-PESO_ANE)) %&gt;% summarise(PESO_ANE=sum(PESO_ANE)) dim(df) # 26085 observations # select only points from March to July df &lt;- subset(df, MONTH %in% c(3:7)) # 25036 observations dim(df) # 25036 observations # Depth, ICES statistical rectangles etc ---------------------------------- # read shapefile with ices divisions ices.areas.shp &lt;- st_read(&quot;C:/use/proyectos/IM-18-ANICHO/datos/ICES_shapefiles/ICES_areas&quot;) st_crs(ices.areas.shp) wgs&lt;-&quot;+proj=longlat +datum=WGS84 +ellps=WGS84&quot; ices.areas.shp &lt;- st_transform(ices.areas.shp, wgs) ices.rect.shp &lt;- st_read(&quot;C:/use/proyectos/IM-18-ANICHO/datos/ICES_shapefiles/ICES_rectangles&quot;) st_crs(ices.rect.shp) wgs&lt;-&quot;+proj=longlat +datum=WGS84 +ellps=WGS84&quot; ices.rect.shp &lt;- st_transform(ices.rect.shp, wgs) ices.rectareas.shp &lt;- st_read(&quot;C:/use/proyectos/IM-18-ANICHO/datos/ICES_shapefiles/ICES_StatRec_mapto_ICES_Areas&quot;) st_crs(ices.rectareas.shp) wgs&lt;-&quot;+proj=longlat +datum=WGS84 +ellps=WGS84&quot; ices.rectareas.shp &lt;- st_transform(ices.rectareas.shp, wgs) # get bathymetry data bathy &lt;- getNOAA.bathy(lon1=-18,lon2=0,lat1=41,lat2=51, resolution = 1, keep=FALSE, antimeridian=FALSE) class(bathy) autoplot(bathy) bathy.df &lt;- fortify(bathy) class(bathy.df) str(bathy.df) # add Depth from marmap according to Lon and Lat idx &lt;- which(!is.na(df$LON) &amp; !is.na(df$LAT) &amp; df$LON &gt; -18 &amp; df$LON &lt; 0 &amp; df$LAT &gt;41 &amp; df$LAT &lt; 51) df$DEPTH &lt;- NA df$DEPTH[idx] &lt;- get.depth(bathy, df[idx,c(&quot;LON&quot;,&quot;LAT&quot;)], locator=F)$depth # Maps -------------------------------------------------------------------- # basic map data global &lt;- map_data(&quot;worldHires&quot;) # basic ggplot p0 &lt;- ggplot() + geom_contour(data=bathy.df, aes(x,y,z=z), breaks=c(-100, -200), col=&quot;grey&quot;)+ annotation_map(map=global, fill=&quot;grey&quot;)+ geom_sf(data=fortify(ices.areas.shp[1]), fill=NA)+ scale_x_continuous(minor_breaks = seq(-10, 0, 1), breaks = seq(-10, 0, 1))+ # ices rectangles scale_y_continuous(minor_breaks = seq(42, 50, 0.5), breaks=seq(42, 50, 1))+ # ices rectangles coord_sf(xlim=c(-10,0), ylim=c(42,50))+ xlab(&quot;&quot;)+ ylab(&quot;&quot;) print(p0) # EGSP: Transformation to UTM --------------------------------------------- # function to find your UTM. Taken from Nerea Goikoetxea lonlat2UTM = function(lonlat) { utm = (floor((lonlat[1] + 180) / 6) %% 60) + 1 if(lonlat[2] &gt; 0) { utm + 32600 } else{ utm + 32700 } } EPSG_2_UTM &lt;- lonlat2UTM(c(mean(df$LON), mean(df$LAT))) EPSG_2_UTM # 32630 --&gt; UTM zone 30N; # WGS84 Bounds: -6.0000, 0.0000, 0.0000, 84.0000 # Projected Bounds: 166021.4431, 0.0000, 833978.5569, 9329005.1825 # Visualize areas for generating pseudo-absences --------------------------- # check class class(ices.rectareas.shp) # select multipolygon object from the shapefile aux1 &lt;- ices.areas.shp[ices.areas.shp$Area_27 %in% c(&quot;8.b&quot;, &quot;8.c&quot;), ] # create a polygon for intersection with ices areas, so that we can select from 6ÂºW to the east aux2 &lt;- st_sfc(st_polygon( list(rbind(c(-6, 40), c(-6, 50), c(1, 50), c(1,40), c(-6, 40))))) aux2 &lt;- st_set_crs(aux2, &quot;+proj=longlat +datum=WGS84 +ellps=WGS84&quot;) wgs&lt;-&quot;+proj=longlat +datum=WGS84 +ellps=WGS84&quot; aux2 &lt;- st_transform(aux2, wgs) # create a polygon for union with ices areas, so that we can add two rectangles from 4ÂºW to the east aux3 &lt;- st_sfc(st_polygon( list(rbind(c(-4, 44), c(-4, 46), c(-2, 46), c(-2,44), c(-4, 44))))) # aux3 &lt;- st_sfc(st_polygon( list(rbind(c(-4, 44.5), c(-4, 45.5), c(-2, 45.5), c(-2,44.5), c(-4, 44.5))))) aux3 &lt;- st_set_crs(aux3, &quot;+proj=longlat +datum=WGS84 +ellps=WGS84&quot;) wgs&lt;-&quot;+proj=longlat +datum=WGS84 +ellps=WGS84&quot; aux3 &lt;- st_transform(aux3, wgs) # transform the catch data points into sf and add the CRS df.sf &lt;- st_as_sf(df, coords=c(&quot;LON&quot;,&quot;LAT&quot;)) df.sf &lt;- st_set_crs(df.sf, &quot;+proj=longlat +datum=WGS84 +ellps=WGS84&quot;) wgs&lt;-&quot;+proj=longlat +datum=WGS84 +ellps=WGS84&quot; df.sf &lt;- st_transform(df.sf, wgs) # transform to UTMs (in m) aux1.utm &lt;- st_transform(aux1, EPSG_2_UTM) aux2.utm &lt;- st_transform(aux2, EPSG_2_UTM) aux3.utm &lt;- st_transform(aux3, EPSG_2_UTM) df.sf.utm &lt;- st_transform(df.sf, EPSG_2_UTM) # convex hull of presence data # df_buff &lt;- st_convex_hull(st_union(df.sf.utm)) # plot(df_buff) # create buffers of 10km (10000) around the points and join the resulting polygons buffer &lt;- st_buffer(df.sf.utm, dist=10000) buffer &lt;- st_union(buffer) plot(buffer) # intersect the ices divisions and the squares aux &lt;- st_intersection(x=st_union(st_union(aux1.utm), aux3.utm), y=aux2.utm) plot(aux, col=2) plot(buffer, add=T) # intersect the result with the buffers around the catch data points aux0 &lt;- st_difference(aux, buffer) plot(aux0, col=2) # ggplot for all data p &lt;- p0 + geom_sf(data=aux0, fill=&quot;red&quot;, alpha=0.3)+ coord_sf(xlim=c(-10,0), ylim=c(42,50)) ggsave(file.path(&quot;plots&quot;,&quot;pseudo&quot;,paste0(&quot;area_pseudo_all.png&quot;)), p, device=&quot;png&quot;) # # randomly sample inside the new polygon # # kk &lt;- st_sample(aux0, size=100, type=&quot;random&quot;) # # # plot # # p &lt;- p0 + # geom_sf(data=aux0, col=&quot;red&quot;, alpha=0.3)+ # geom_sf(data=fortify(kk))+ # coord_sf(xlim=c(-10,0), ylim=c(42,50)) # print(p) # # # extract coordinates as data.frame # # st_coordinates(kk) # loop to calculate area to generate pseudo-absences by year for (yy in sort(unique(df$YEAR))){ df.sub &lt;- subset(df.sf.utm, YEAR==yy) if (nrow(df.sub)&gt;0){ buffer.sub &lt;- st_buffer(df.sub, dist=10000) buffer.sub &lt;- st_union(buffer.sub) aux0.sub &lt;- st_difference(aux, buffer.sub) p &lt;- p0 + geom_sf(data=aux0.sub, fill=&quot;red&quot;, alpha=0.3)+ #geom_sf(data=df.sub)+ coord_sf(xlim=c(-6,0), ylim=c(43,46)) ggsave(file.path(&quot;plots&quot;,&quot;pseudo&quot;,paste0(&quot;area_pseudo_&quot;,yy,&quot;.png&quot;)), p, device=&quot;png&quot;) } } # loop to calculate area to generate pseudo-absences by month and year for (yy in sort(unique(df$YEAR))){ for (mm in seq(3,7,by=1)){ df.sub &lt;- subset(df.sf.utm, YEAR==yy &amp; MONTH==mm) if (nrow(df.sub)&gt;0){ buffer.sub &lt;- st_buffer(df.sub, dist=10000) buffer.sub &lt;- st_union(buffer.sub) aux0.sub &lt;- st_difference(aux, buffer.sub) p &lt;- p0 + geom_sf(data=aux0.sub, fill=&quot;red&quot;, alpha=0.3)+ #geom_sf(data=df.sub)+ coord_sf(xlim=c(-6,0), ylim=c(43,46)) ggsave(file.path(&quot;plots&quot;,&quot;pseudo&quot;,paste0(&quot;area_pseudo_&quot;,yy,&quot;_&quot;,mm,&quot;.png&quot;)), p, device=&quot;png&quot;) } } } # Remove points outside ices 8b, 8c or in land ---------------------------- # number of points within the study area df.in &lt;- st_intersects(df.sf.utm, aux, sparse=FALSE) df$INSIDE &lt;- df.in[,1] mean(df$INSIDE) # 0.9743969 points inside the area of study table(df$INSIDE) table(df$DEPTH&gt;0) table(df$INSIDE, df$DEPTH&gt;0) p &lt;- p0 + geom_point(data=subset(df, INSIDE==1 &amp; DEPTH &lt;=0), aes(x=LON, y=LAT), col=&quot;red&quot;, alpha=0.3)+ coord_sf(xlim=c(-10,0), ylim=c(43,46))+ ggtitle(&quot;Points inside area with Depth&lt;=0&quot;) ggsave(file.path(&quot;plots&quot;,&quot;pseudo&quot;,paste0(&quot;points_inside_depthneg.png&quot;)), p, device=&quot;png&quot;) p &lt;- p0 + geom_point(data=subset(df, INSIDE==1 &amp; DEPTH &gt;0), aes(x=LON, y=LAT), col=&quot;red&quot;, alpha=0.3)+ coord_sf(xlim=c(-10,0), ylim=c(43,46))+ ggtitle(&quot;Points inside area with Depth&gt;0&quot;) ggsave(file.path(&quot;plots&quot;,&quot;pseudo&quot;,paste0(&quot;points_inside_depthpos.png&quot;)), p, device=&quot;png&quot;) p &lt;- p0 + geom_point(data=subset(df, INSIDE==0 &amp; DEPTH &lt;= 0), aes(x=LON, y=LAT), col=&quot;red&quot;, alpha=0.3)+ coord_sf(xlim=c(-10,0), ylim=c(43,46))+ ggtitle(&quot;Points outside area with Depth&lt;=0&quot;) ggsave(file.path(&quot;plots&quot;,&quot;pseudo&quot;,paste0(&quot;points_outside_depthneg.png&quot;)), p, device=&quot;png&quot;) p &lt;- p0 + geom_point(data=subset(df, INSIDE==0 &amp; DEPTH &gt; 0), aes(x=LON, y=LAT), col=&quot;red&quot;, alpha=0.3)+ coord_sf(xlim=c(-10,0), ylim=c(43,46))+ # coord_sf(xlim=c(-10,0), ylim=c(42,50))+ ggtitle(&quot;Points outside area with Depth&gt;0&quot;) ggsave(file.path(&quot;plots&quot;,&quot;pseudo&quot;,paste0(&quot;points_outside_depthpos.png&quot;)), p, device=&quot;png&quot;) # So, we keep only the points INSIDE the area with DEPTH&lt;0 df &lt;- subset(df, INSIDE==1 &amp; DEPTH &lt;= 0) dim(df) # 24147 observations # remove the columns that are not going to be used df$LAND &lt;- df$OK &lt;- df$INSIDE &lt;- NULL # Generate pseudo-absences ------------------------------------------------- # we will use the positive database to generate the pseudo-absences dfpos &lt;- subset(df, PESO_ANE&gt;0) nbpoints &lt;- nrow(dfpos) # 23678 out of 24147 are positive observations (98%) # we compute again the buffer but only for the positive data points # transform the catch data points into sf and add the CRS dfpos.sf &lt;- st_as_sf(dfpos, coords=c(&quot;LON&quot;,&quot;LAT&quot;)) dfpos.sf &lt;- st_set_crs(dfpos.sf, &quot;+proj=longlat +datum=WGS84 +ellps=WGS84&quot;) wgs&lt;-&quot;+proj=longlat +datum=WGS84 +ellps=WGS84&quot; dfpos.sf &lt;- st_transform(dfpos.sf, wgs) dfpos.sf.utm &lt;- st_transform(dfpos.sf, EPSG_2_UTM) # transform to UTMs (in m) # Generate the pseudo-absence data frame pseudo &lt;- matrix(data=NA, nrow=nbpoints, ncol=10) pseudo &lt;- data.frame(pseudo) names(pseudo) &lt;- c(&quot;CODEUEBUQUE&quot;,&quot;FECHA_CAPT&quot;,&quot;DOY&quot;,&quot;WEEK&quot;,&quot;MONTH&quot;,&quot;YEAR&quot;,&quot;LAT&quot;,&quot;LON&quot;,&quot;PESO_ANE&quot;,&quot;DEPTH&quot;) # set the seed set.seed(1) # sample the date pseudo$FECHA_CAPT &lt;- sample(x=dfpos$FECHA_CAPT, size=nbpoints, replace = TRUE) pseudo$FECHA_CAPT &lt;- as.Date(pseudo$FECHA_CAPT, format=&quot;%Y-%m-%d&quot;) pseudo$DOY &lt;- yday(pseudo$FECHA_CAPT) pseudo$WEEK &lt;- week(pseudo$FECHA_CAPT) pseudo$MONTH &lt;- month(pseudo$FECHA_CAPT) pseudo$YEAR &lt;- year(pseudo$FECHA_CAPT) # loop by month and year for (yy in sort(unique(dfpos$YEAR))){ for (mm in sort(unique(dfpos$MONTH))){ idx &lt;- which(pseudo$YEAR==yy &amp; pseudo$MONTH==mm) if (length(idx)&gt;0){ df.sub &lt;- subset(dfpos.sf.utm, YEAR==yy &amp; MONTH==mm) buffer.sub &lt;- st_buffer(df.sub, dist=10000) buffer.sub &lt;- st_union(buffer.sub) aux0.sub &lt;- st_difference(aux, buffer.sub) rp.sf &lt;- st_sample(aux0.sub, size=length(idx), type=&quot;random&quot;) # randomly sample points rp.sf &lt;- st_transform(rp.sf, 4326) rp &lt;- as.data.frame(st_coordinates(rp.sf)) # transform to lat&amp;lon and extract coordinates as data.frame pseudo$LON[idx] &lt;- rp$X pseudo$LAT[idx] &lt;- rp$Y p &lt;- p0 + geom_sf(data=aux0.sub, fill=2, alpha=0.3)+ geom_sf(data=df.sub, col=4, alpha=0.3)+ geom_sf(data=rp.sf, col=1, shape=4)+ coord_sf(xlim=c(-6,0), ylim=c(43,46))+ ggtitle(paste(&quot;ANE&quot;,yy,mm)) ggsave(file.path(&quot;plots&quot;,&quot;pseudo&quot;,paste0(&quot;pseudo_&quot;,yy,&quot;_&quot;,mm,&quot;.png&quot;)), p, device=&quot;png&quot;) } } } # complete the rest of columns pseudo$CODEUEBUQUE &lt;- &quot;ESPxxxxxxxxx&quot; # generic name to distinguish the pseudo-absence data pseudo$PESO_ANE &lt;- 0 pseudo$DEPTH &lt;- get.depth(bathy, pseudo[ ,c(&quot;LON&quot;,&quot;LAT&quot;)], locator=F)$depth # there might be still some locations with Depth&gt;0 summary(pseudo$DEPTH) sum(pseudo$DEPTH&gt;0) # 197 obs mean(pseudo$DEPTH&gt;0) # 0.008316447 very small proportion # transform to sf object (lat&amp;lon) to plot pseudo.sf &lt;- st_as_sf(pseudo, coords=c(&quot;LON&quot;,&quot;LAT&quot;)) pseudo.sf &lt;- st_set_crs(pseudo.sf, &quot;+proj=longlat +datum=WGS84 +ellps=WGS84&quot;) wgs&lt;-&quot;+proj=longlat +datum=WGS84 +ellps=WGS84&quot; pseudo.sf &lt;- st_transform(pseudo.sf, wgs) p &lt;- p0 + geom_sf(data=pseudo.sf, aes(col=(DEPTH&gt;0)), col=&quot;red&quot;, alpha=0.3)+ coord_sf(xlim=c(-6,0), ylim=c(42,46)) # coord_sf(xlim=c(-10,0), ylim=c(42,50))+ print(p) # plot all pseudo-absences p &lt;- p0 + geom_sf(data=aux0, fill=2, alpha=0.3)+ geom_sf(data=dfpos.sf, col=4, alpha=0.3)+ geom_sf(data=pseudo.sf, col=1, shape=4, alpha=0.3)+ coord_sf(xlim=c(-6,0), ylim=c(43,46)) print(p) ggsave(file.path(&quot;plots&quot;,&quot;pseudo&quot;,&quot;pseudo_all.png&quot;), p, device=&quot;png&quot;) # plot pseudo-absences by year p &lt;- p0 + geom_sf(data=aux0, fill=2, alpha=0.3)+ geom_sf(data=dfpos.sf, col=4, alpha=0.3)+ geom_sf(data=pseudo.sf, col=1, shape=4, alpha=0.3)+ coord_sf(xlim=c(-6,0), ylim=c(43,46))+ facet_wrap(~YEAR) print(p) ggsave(file.path(&quot;plots&quot;,&quot;pseudo&quot;,&quot;pseudo_all_by_year.png&quot;), p, device=&quot;png&quot;) # Save the final dataset including the pseudo-absences -------------------- head(df) head(pseudo) # Join the two data sets and save the final dataset dat &lt;- rbind(df, pseudo) write.table(dat, file=file.path(&quot;data&quot;,&quot;dfpseudo_ane_2010_2019.csv&quot;), row.names=F, sep=&quot;;&quot;, dec=&quot;.&quot;) # End of script ----------------------------------------------------------- References "],["environmental-data.html", "Chapter 4 Environmental data 4.1 Download from public repositories 4.2 Operations with rasters (maybe not needed)", " Chapter 4 Environmental data Bla bla bla 4.1 Download from public repositories Download from Bio-oracle. ### Download Environmental data from Bio-oracle # for mapping ... library(maptools) library(rgrass7) library(raster) library(sp) library(ggplot2) library(maps) # libraries for bio-oracle library(rgdal) library(sdmpredictors) library(leaflet) # http://bio-oracle.org/code.php install.packages(&quot;sdmpredictors&quot;) #install.packages(&quot;leaflet&quot;) # Load package library(sdmpredictors) # Species Data example from GBIF mydata &lt;- occ_data(scientificName = &quot;Anisakis&quot;, hasCoordinate = TRUE)$data # 02/10/2021 416 obs x 150 var mydata.gbif &lt;- subset(mydata, select=c(acceptedScientificName,genus,specificEpithet,decimalLatitude,decimalLongitude,year,month,day)) mydata.gbif.ll &lt;- cbind(mydata.gbif$decimalLongitude, mydata.gbif$decimalLatitude) mydata.gbif.ll &lt;- as.data.frame(mydata.gbif.ll) names(mydata.gbif.ll) &lt;- c(&quot;Lon&quot;, &quot;Lat&quot;) # Explore datasets in the package list_datasets() list_layers(&quot;Bio-ORACLE&quot;) mytab &lt;- list_layers(&quot;Bio-ORACLE&quot;) ## Download specific layers to the current directory myBioracle.layers &lt;- load_layers(c(&quot;BO_chlomean&quot;, &quot;BO_damean&quot;, &quot;BO_salinity&quot;, &quot;BO_sstmean&quot;, &quot;BO_sstrange&quot;, &quot;BO_bathymean&quot;)) #save(myBioracle.layers, file=&quot;myBioracle.layers.Rdata&quot;) #load(file=&quot;myBioracle.layers.Rdata&quot;) # creo que no funciona myBioracle.layers # Check layer statistics layer_stats(&quot;myBioracle.layers&quot;) # Crop raster to fit the North East Atlantic window for estimating SST range # lat: 36.49 to 67.00 # lon: -16.000 to 9.000 my.NEatlantic.ext &lt;- extent(-100, 45, -90, 90) # xmin, xmax, ymin, ymax myBioracle.layers.cropNEatlantic &lt;- crop(myBioracle.layers, my.NEatlantic.ext) my.colors = colorRampPalette(c(&quot;#5E85B8&quot;,&quot;#EDF0C0&quot;,&quot;#C13127&quot;)) plot(myBioracle.layers.cropNEatlantic,col=my.colors(1000),axes=F, box=F) summary(myBioracle.layers.cropNEatlantic) # Generate a nice color ramp and plot the map my.colors = colorRampPalette(c(&quot;#5E85B8&quot;,&quot;#EDF0C0&quot;,&quot;#C13127&quot;)) plot(myBioracle.layers.cropNEatlantic,col=my.colors(1000),axes=F, box=F) image(log(myBioracle.layers.cropNEatlantic$BO_chlomean),col=my.colors(1000),axes=T, ylab=NA,xlab=NA, main=&quot;Chl (log scale)&quot;, cex.main=2) map(&quot;world&quot;,add=T, fill=T, col=&quot;white&quot;,lwd=0.1); box() image(log(myBioracle.layers.cropNEatlantic$BO_damean),col=my.colors(1000),axes=T, ylab=NA,xlab=NA,main=&quot;Diffuse Attenuation&quot;, cex.main=2) map(&quot;world&quot;,add=T, fill=T, col=&quot;white&quot;,lwd=0.1); box() image(myBioracle.layers.cropNEatlantic$BO_salinity,col=my.colors(1000),axes=T, ylab=NA,xlab=NA,main=&quot;Salinity&quot;, cex.main=2) map(&quot;world&quot;,add=T, fill=T, col=&quot;white&quot;,lwd=0.1); box() image(myBioracle.layers.cropNEatlantic$BO_sstmean,col=my.colors(1000),axes=T, ylab=NA,xlab=NA,main=&quot;SST mean&quot;, cex.main=2) map(&quot;world&quot;,add=T, fill=T, col=&quot;white&quot;,lwd=0.1); box() image(myBioracle.layers.cropNEatlantic$BO_sstrange,col=my.colors(1000),axes=T, ylab=NA,xlab=NA,main=&quot;SST range&quot;, cex.main=2) map(&quot;world&quot;,add=T, fill=T, col=&quot;white&quot;,lwd=0.1); box() image(myBioracle.layers.cropNEatlantic$BO_bathymean,col=my.colors(1000),axes=T, ylab=NA,xlab=NA,main=&quot;Depth&quot;, cex.main=2) map(&quot;world&quot;,add=T, fill=T, col=&quot;white&quot;,lwd=0.1); box() ### Extract environmental values from layers mydata1.env &lt;- raster::extract(x=myBioracle.layers,y=mydata.gbif.ll, df=T) # sin buffer (se extrae solo los puntos) # con Bilinear extraigo los sites de los 4 nearest cells mydata1.env.bil &lt;- raster::extract(x=myBioracle.layers,y=mydata.gbif.ll, method=&quot;bilinear&quot;, na.rm=TRUE, df=T) # 4 nearest cells, se consigue valor 29 sites más (se siguen perdiendo 64, de los cuales algunos son errores, es decir, no costeros) mydata.all &lt;- cbind(mydata.gbif.ll, mydata1.env.bil) summary(mydata.all) # 448 sites : 84 are NAs We can also download the bathymetry from the marmap library library(marmap) bathy &lt;- getNOAA.bathy(lon1=-11,lon2=0,lat1=41,lat2=51, resolution = 1, keep=FALSE, antimeridian=FALSE) bathy.df &lt;- fortify(bathy) # LI: maybe this is not needed here df$Depth &lt;- get.depth(bathy, df[,c(&quot;Lon&quot;,&quot;Lat&quot;)], locator=F)$depth 4.2 Operations with rasters (maybe not needed) We can complete this a bit more later on, though not necessary right now for example, given a raster, we can calculate gradients in the vertical or depth at which the max is found # Auxiliary functions ------------------------------------------------------ # Taken from: # https://gis.stackexchange.com/questions/114723/subset-netcdf-based-on-last-valid-variable-by-level # deepestValid: function to that, for each cell, # - returns NA in case all depth level values are NA # - returns the value of the last available depth level in case no NA occur # - else returns the value of the last non-NA depth level deepestValid &lt;- function(x) { na &lt;- is.na(x) if (all(na)) { return(NA) } else if (all(!na)) { return(x[length(x)]) } else { first_na &lt;- which(na)[1] last_valid &lt;- first_na - 1 return(x[last_valid]) } } # read the data files using the library raster b1 &lt;- brick(file.path(hdata.dir, paste0(&quot;thetao_&quot;,dd, &quot; 12:00:00.nc&quot;))) b2 &lt;- brick(file.path(hdata.dir, paste0(&quot;so_&quot;,dd, &quot; 12:00:00.nc&quot;))) b3 &lt;- brick(file.path(hdata.dir, paste0(&quot;mlotst_&quot;,dd, &quot; 12:00:00.nc&quot;))) b4 &lt;- brick(file.path(hdata.dir, paste0(&quot;chl_&quot;,dd, &quot; 12:00:00.nc&quot;))) b5 &lt;- brick(file.path(hdata.dir, paste0(&quot;o2_&quot;,dd, &quot; 12:00:00.nc&quot;))) # temperature at surface (exactly at 0.49m) # see names(b1) bb1a &lt;- b1[[1]] names(bb1a) &lt;- &quot;TEMP0m&quot; # temperature at 100m (layer 22, which is at 92.33m) or at the deepest available # see names(b1) bb1b &lt;- calc(subset(b1, 1:22), fun=deepestValid) names(bb1b) &lt;- &quot;TEMP100m&quot; # salinity at surface (exactly at 0.49m) # see names(b2) bb2 &lt;- b2[[1]] names(bb2) &lt;- &quot;so1&quot; # logarithm of ocean mixed layer thickness (m) bb3 &lt;- b3[[1]] bb3 &lt;- log(bb3) # or equivalently: # bb3 &lt;- calc(bb3, fun=function(x) {log(x)}) names(bb3) &lt;- &quot;logmlotst1&quot; # logarithm of the chlorophyll integrated until 100m (layer 22, which is at 92.33m) or at the deepest available # see names(b4) b4 &lt;- subset(b4, 1:22) # take only values in the first 22 layers corresponding up to depth 100m depth.lev &lt;- as.numeric(as.character(sapply(names(b4), substring, first=2))) # extract depth values from layer names depth.lev &lt;- c(0, depth.lev) w &lt;- diff(depth.lev) bb4 &lt;- calc(b4, function(x){sum(x*w, na.rm=T)}) # compute integrated value bb4 &lt;- log(bb4) names(bb4) &lt;- &quot;logCHL100m&quot; # oxygen integrated until 100m (layer 22, which is at 92.33m) or at the deepest available # see names(b5) b5 &lt;- subset(b5, 1:22) # take only values in the first 22 layers corresponding up to depth 100m depth.lev &lt;- as.numeric(as.character(sapply(names(b5), substring, first=2))) # extract depth values from layer names depth.lev &lt;- c(0, depth.lev) w &lt;- diff(depth.lev) bb5 &lt;- calc(b5, function(x){sum(x*w, na.rm=T)}) # compute integrated value names(bb5) &lt;- &quot;O100m&quot; # unique stack (because they all have the same extent and resolution) b &lt;- stack(bb1a, bb1b, bb2, bb3, bb4, bb5) # Step 2: Add bathymetry -------------------------------------------------- # get the extent of the raster to extract the corresponding bathymetry e &lt;- extent(b) # load bathymetry of the area (get wider limits) # resolution = 5 (resolucion en minutos, corresponde a 1/12 º) # mybathy &lt;- getNOAA.bathy(lon1=floor(e@xmin), lon2=ceiling(e@xmax), lat1=floor(e@ymin), lat2=ceiling(e@ymax), # resolution=1, keep=F) # save(mybathy, file=&quot;mybathy.RData&quot;) load(file.path(&quot;mybathy.RData&quot;)) # transform to data frame pred.dat &lt;- raster::as.data.frame(b, xy=T) # include bathymetry into the prediction data frame pred.dat$DEPTH &lt;- get.depth(mybathy, pred.dat[ ,c(&quot;x&quot;,&quot;y&quot;)], locator=F)$depth pred.dat$DEPTH[pred.dat$DEPTH&gt;=0] &lt;- NA pred.dat$logDEPTH&lt;-log(-pred.dat$DEPTH) # transform the bathymetry to raster format b7 &lt;- rasterFromXYZ(pred.dat[,c(&quot;x&quot;,&quot;y&quot;,&quot;logDEPTH&quot;)]) names(b7) &lt;- &quot;logDEPTH&quot; # include the bathymetry raster into the prediction stack b &lt;- stack(b, b7) # Step 3: Create latitude, longitude and doy rasters ---------------------- # these are needed to predict according to the spatio-temporal model b8 &lt;- b9 &lt;- b10 &lt;- b[[1]] # create rasters with same structure b8[] &lt;- coordinates(b8)[,1] #longitude names(b8) &lt;- &quot;LON&quot; b9[] &lt;- coordinates(b8)[,2] #latitude names(b9) &lt;- &quot;LAT&quot; b10[] &lt;- doy # day of the year names(b10) &lt;- &quot;DOY&quot; # include longitude, latitude and day of the year into the prediction stack, # they are needed for the spatio-temporal model prediction b &lt;- stack(b, b8, b9, b10) "],["prepare-final-dataset.html", "Chapter 5 Prepare final dataset 5.1 Extract environmental data associated to presence-absence data 5.2 Exploratory plots", " Chapter 5 Prepare final dataset Bla bla bla 5.1 Extract environmental data associated to presence-absence data 5.2 Exploratory plots "],["shape-constrained-generalized-additive-models.html", "Chapter 6 Shape Constrained-Generalized Additive Models 6.1 Model fit 6.2 Model selection", " Chapter 6 Shape Constrained-Generalized Additive Models One citation is (Citores et al. 2020) Mention there is an alternative using mboost that wont be further developed here. 6.1 Model fit 6.2 Model selection References "],["model-validation.html", "Chapter 7 Model validation 7.1 Optimum threshold 7.2 k-fold validation", " Chapter 7 Model validation Bla bla 7.1 Optimum threshold 7.2 k-fold validation "],["prediction-and-maps.html", "Chapter 8 Prediction and maps", " Chapter 8 Prediction and maps predict from fitted models and produce maps "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
